{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LaprM2Zgm0RL"
   },
   "source": [
    "# The Adaptive Learner - Google Colab Version\n",
    "\n",
    "This notebook implements the core functionality of The Adaptive Learner, a biologically-inspired, adaptive learning system for LLMs that prevents catastrophic forgetting through neuromodulated PEFT mechanisms and generative replay.\n",
    "\n",
    "## Features\n",
    "- Neuromodulation (Î³-Gain): A dynamic task-conditioned signal that modulates LoRA plasticity\n",
    "- AM-Managed PEFT: Task-specific LoRA modules managed by a router\n",
    "- LoRA Consolidation: Techniques to prevent catastrophic forgetting\n",
    "- Generative Replay: Data-level memory replay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "S6N14UY_QuQA"
   },
   "outputs": [],
   "source": [
    "# Cell 1: Environment Variable Setup\n",
    "\n",
    "import os\n",
    "\n",
    "# Configure PyTorch CUDA memory allocator to allow segments to expand.\n",
    "# This can help prevent out-of-memory errors in some situations by allowing\n",
    "# PyTorch to request more memory from the OS for CUDA allocations if needed,\n",
    "# rather than being strictly limited by initially reserved blocks.\n",
    "os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'\n",
    "\n",
    "# The following line for CUDA_VERSION was for older bitsandbytes compatibility issues.\n",
    "# Since we are installing PyTorch for CUDA 12.1 and recent bitsandbytes,\n",
    "# this is likely not needed and should remain commented out or removed\n",
    "# to avoid potential conflicts if the system/driver has a different CUDA version\n",
    "# that PyTorch's CUDA 12.1 components can still work with.\n",
    "# os.environ['CUDA_VERSION'] = '11.8' # Keep commented out\n",
    "\n",
    "# --- Suppress Hugging Face Tokenizers Parallelism Warning ---\n",
    "# When using PyTorch DataLoaders with num_workers > 0, the tokenizers library\n",
    "# can detect that it's being used in forked child processes after being used\n",
    "# in the main process. It then disables its own internal parallelism to prevent\n",
    "# potential deadlocks and issues a warning.\n",
    "# Setting this environment variable explicitly to 'true' or 'false' suppresses that warning.\n",
    "# Setting to 'false' is generally recommended and safer when DataLoader handles multiprocessing.\n",
    "os.environ['TOKENIZERS_PARALLELISM'] = 'false'\n",
    "# --- End of Tokenizers Parallelism Setting ---\n",
    "\n",
    "# You can add other global environment variable settings here if needed in the future.\n",
    "# For example, if you weren't using RunPods secrets for W&B and wanted to set them here:\n",
    "# os.environ['WANDB_API_KEY'] = 'your_wandb_api_key_here' # Not recommended if using secrets\n",
    "# os.environ['WANDB_PROJECT'] = 'your_wandb_project_name'\n",
    "# os.environ['WANDB_ENTITY'] = 'your_wandb_entity'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6iaw2q-em0RM"
   },
   "source": [
    "## Setup\n",
    "\n",
    "First, we'll install the required dependencies:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oP1olsPom0RM",
    "outputId": "732c756a-07aa-476f-bad6-ae4bd0227107"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing installation: torch 2.5.1+cu121\n",
      "Uninstalling torch-2.5.1+cu121:\n",
      "  Successfully uninstalled torch-2.5.1+cu121\n",
      "Found existing installation: torchvision 0.20.1+cu121\n",
      "Uninstalling torchvision-0.20.1+cu121:\n",
      "  Successfully uninstalled torchvision-0.20.1+cu121\n",
      "Found existing installation: torchaudio 2.5.1+cu121\n",
      "Uninstalling torchaudio-2.5.1+cu121:\n",
      "  Successfully uninstalled torchaudio-2.5.1+cu121\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mFound existing installation: transformers 4.51.3\n",
      "Uninstalling transformers-4.51.3:\n",
      "  Successfully uninstalled transformers-4.51.3\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mLooking in indexes: https://download.pytorch.org/whl/cu121\n",
      "Collecting torch\n",
      "  Using cached https://download.pytorch.org/whl/cu121/torch-2.5.1%2Bcu121-cp310-cp310-linux_x86_64.whl (780.4 MB)\n",
      "Collecting torchvision\n",
      "  Using cached https://download.pytorch.org/whl/cu121/torchvision-0.20.1%2Bcu121-cp310-cp310-linux_x86_64.whl (7.3 MB)\n",
      "Collecting torchaudio\n",
      "  Using cached https://download.pytorch.org/whl/cu121/torchaudio-2.5.1%2Bcu121-cp310-cp310-linux_x86_64.whl (3.4 MB)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.9.0)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.0)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.2)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2025.3.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.10/dist-packages (from torch) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.10/dist-packages (from torch) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch) (12.1.105)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision) (1.24.1)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision) (9.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.2)\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "peft 0.15.2 requires transformers, which is not installed.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed torch-2.5.1+cu121 torchaudio-2.5.1+cu121 torchvision-0.20.1+cu121\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
      "Requirement already satisfied: flash-attn in /usr/local/lib/python3.10/dist-packages (2.7.4.post1)\n",
      "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from flash-attn) (2.5.1+cu121)\n",
      "Requirement already satisfied: einops in /usr/local/lib/python3.10/dist-packages (from flash-attn) (0.8.1)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->flash-attn) (3.9.0)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch->flash-attn) (4.12.2)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->flash-attn) (3.0)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->flash-attn) (3.1.2)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->flash-attn) (2025.3.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->flash-attn) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->flash-attn) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->flash-attn) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.10/dist-packages (from torch->flash-attn) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch->flash-attn) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch->flash-attn) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch->flash-attn) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch->flash-attn) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch->flash-attn) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.10/dist-packages (from torch->flash-attn) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->flash-attn) (12.1.105)\n",
      "Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.10/dist-packages (from torch->flash-attn) (3.1.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch->flash-attn) (1.13.1)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch->flash-attn) (12.1.105)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch->flash-attn) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->flash-attn) (2.1.2)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
      "Collecting transformers\n",
      "  Using cached transformers-4.51.3-py3-none-any.whl.metadata (38 kB)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.9.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.31.4)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.24.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.21.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.5.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (2025.3.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (1.26.13)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2022.12.7)\n",
      "Using cached transformers-4.51.3-py3-none-any.whl (10.4 MB)\n",
      "Installing collected packages: transformers\n",
      "Successfully installed transformers-4.51.3\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
      "Requirement already satisfied: peft in /usr/local/lib/python3.10/dist-packages (0.15.2)\n",
      "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (3.6.0)\n",
      "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.6.1)\n",
      "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (3.10.3)\n",
      "Requirement already satisfied: wandb in /usr/local/lib/python3.10/dist-packages (0.19.11)\n",
      "Requirement already satisfied: python-dotenv in /usr/local/lib/python3.10/dist-packages (1.1.0)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (4.67.1)\n",
      "Requirement already satisfied: rouge-score in /usr/local/lib/python3.10/dist-packages (0.1.2)\n",
      "Requirement already satisfied: accelerate in /usr/local/lib/python3.10/dist-packages (1.7.0)\n",
      "Requirement already satisfied: bitsandbytes in /usr/local/lib/python3.10/dist-packages (0.45.5)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from peft) (1.24.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from peft) (23.2)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from peft) (5.9.6)\n",
      "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from peft) (6.0.1)\n",
      "Requirement already satisfied: torch>=1.13.0 in /usr/local/lib/python3.10/dist-packages (from peft) (2.5.1+cu121)\n",
      "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (from peft) (4.51.3)\n",
      "Requirement already satisfied: safetensors in /usr/local/lib/python3.10/dist-packages (from peft) (0.5.3)\n",
      "Requirement already satisfied: huggingface_hub>=0.25.0 in /usr/local/lib/python3.10/dist-packages (from peft) (0.31.4)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.9.0)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (20.0.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.2.3)\n",
      "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.32.3)\n",
      "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2025.3.0)\n",
      "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.15.3)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.5.0)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.6.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.3.2)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (4.58.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.4.8)\n",
      "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (9.3.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /usr/lib/python3/dist-packages (from matplotlib) (2.4.7)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (2.8.2)\n",
      "Requirement already satisfied: click!=8.0.0,>=7.1 in /usr/local/lib/python3.10/dist-packages (from wandb) (8.2.0)\n",
      "Requirement already satisfied: docker-pycreds>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (0.4.0)\n",
      "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (3.1.44)\n",
      "Requirement already satisfied: platformdirs in /usr/local/lib/python3.10/dist-packages (from wandb) (3.11.0)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<7,>=3.19.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (6.31.0)\n",
      "Requirement already satisfied: pydantic<3 in /usr/local/lib/python3.10/dist-packages (from wandb) (2.11.4)\n",
      "Requirement already satisfied: sentry-sdk>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (2.29.1)\n",
      "Requirement already satisfied: setproctitle in /usr/local/lib/python3.10/dist-packages (from wandb) (1.3.6)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from wandb) (68.2.2)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.4 in /usr/local/lib/python3.10/dist-packages (from wandb) (4.12.2)\n",
      "Requirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (from rouge-score) (2.2.2)\n",
      "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from rouge-score) (3.9.1)\n",
      "Requirement already satisfied: six>=1.14.0 in /usr/lib/python3/dist-packages (from rouge-score) (1.16.0)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.11.18)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.10/dist-packages (from gitpython!=3.1.29,>=1.0.0->wandb) (4.0.12)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3->wandb) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.10/dist-packages (from pydantic<3->wandb) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3->wandb) (0.4.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (1.26.13)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2022.12.7)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (3.0)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (3.1.2)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (12.1.105)\n",
      "Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (3.1.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (1.13.1)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.13.0->peft) (12.1.105)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch>=1.13.0->peft) (1.3.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk->rouge-score) (2024.11.6)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.10/dist-packages (from transformers->peft) (0.21.1)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.3.2)\n",
      "Requirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (5.0.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (23.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.6.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.4.4)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.3.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.20.0)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.10/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb) (5.0.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.13.0->peft) (2.1.2)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Cell 2: Setup - Installing Dependencies (Corrected and Enforced Order)\n",
    "\n",
    "# Uninstall potentially conflicting versions first for a cleaner state.\n",
    "!pip uninstall torch torchvision torchaudio -y\n",
    "!pip uninstall transformers -y\n",
    "\n",
    "# Install PyTorch and Torchvision first, from the specific CUDA 12.1 wheel.\n",
    "!pip install -U torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121\n",
    "\n",
    "# Install flash-attn\n",
    "!pip install -U flash-attn --no-build-isolation\n",
    "\n",
    "# Now, install transformers and the rest of your dependencies.\n",
    "!pip install -U transformers\n",
    "!pip install -U peft datasets scikit-learn matplotlib wandb python-dotenv tqdm rouge-score accelerate bitsandbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-20 16:15:09,233 - __main___setup_verification - INFO - --- Verifying Hugging Face and W&B Logins ---\n",
      "2025-05-20 16:15:09,234 - __main___setup_verification - INFO - --- Hugging Face Verification ---\n",
      "2025-05-20 16:15:09,235 - __main___setup_verification - INFO - HF_TOKEN/HUGGINGFACE_HUB_TOKEN found in environment variables.\n",
      "2025-05-20 16:15:09,440 - __main___setup_verification - INFO - Hugging Face CLI whoami successful:\n",
      "Galileo82\n",
      "2025-05-20 16:15:09,640 - __main___setup_verification - INFO - Hugging Face programmatic whoami successful: User 'Galileo82'\n",
      "2025-05-20 16:15:09,641 - __main___setup_verification - INFO - --- Weights & Biases Verification ---\n",
      "2025-05-20 16:15:09,641 - __main___setup_verification - INFO - WANDB_API_KEY found in environment variables.\n",
      "2025-05-20 16:15:09,642 - __main___setup_verification - INFO - WANDB_PROJECT found: RESTART01\n",
      "2025-05-20 16:15:09,642 - __main___setup_verification - INFO - WANDB_ENTITY found: doingmyownthing82-none\n",
      "2025-05-20 16:15:09,642 - __main___setup_verification - INFO - --- Verification Cell Complete ---\n"
     ]
    }
   ],
   "source": [
    "# Cell 2.5: Login & Verification (Run after Cell 2 - Pip Installs)\n",
    "\n",
    "import os\n",
    "import logging # Use standard logging for this setup cell\n",
    "import subprocess\n",
    "\n",
    "# Setup a basic logger for this cell if main logger isn't configured yet\n",
    "setup_logger = logging.getLogger(__name__ + \"_setup_verification\")\n",
    "if not setup_logger.handlers:\n",
    "    handler = logging.StreamHandler()\n",
    "    formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "    handler.setFormatter(formatter)\n",
    "    setup_logger.addHandler(handler)\n",
    "    setup_logger.setLevel(logging.INFO)\n",
    "\n",
    "setup_logger.info(\"--- Verifying Hugging Face and W&B Logins ---\")\n",
    "\n",
    "# 1. Hugging Face Login Verification\n",
    "setup_logger.info(\"--- Hugging Face Verification ---\")\n",
    "hf_token = os.environ.get(\"HF_TOKEN\") or os.environ.get(\"HUGGINGFACE_HUB_TOKEN\")\n",
    "if hf_token:\n",
    "    setup_logger.info(\"HF_TOKEN/HUGGINGFACE_HUB_TOKEN found in environment variables.\")\n",
    "    try:\n",
    "        # Use huggingface-cli whoami to check token validity without explicit login call in Python\n",
    "        # The `login` function in huggingface_hub can also be used but `whoami` is a good CLI check\n",
    "        # Make sure the CLI is accessible; it should be if transformers is installed.\n",
    "        result = subprocess.run(['huggingface-cli', 'whoami'], capture_output=True, text=True, check=False)\n",
    "        if result.returncode == 0 and \"ERROR\" not in result.stdout.upper() and \"Traceback\" not in result.stdout:\n",
    "            setup_logger.info(f\"Hugging Face CLI whoami successful:\\n{result.stdout.strip()}\")\n",
    "            # Additionally, try a programmatic check if desired, though CLI is often sufficient\n",
    "            from huggingface_hub import HfApi\n",
    "            try:\n",
    "                api = HfApi() # Uses token from env\n",
    "                user_info = api.whoami()\n",
    "                setup_logger.info(f\"Hugging Face programmatic whoami successful: User '{user_info.get('name')}'\")\n",
    "            except Exception as e_api:\n",
    "                setup_logger.warning(f\"Hugging Face programmatic whoami failed (token might be invalid for API access or other issue): {e_api}\")\n",
    "        else:\n",
    "            setup_logger.warning(f\"Hugging Face CLI whoami failed or indicated an issue. stdout: {result.stdout.strip()}, stderr: {result.stderr.strip()}\")\n",
    "            setup_logger.warning(\"Ensure your HF_TOKEN is valid and has permissions for the models you intend to use (e.g., Gemma).\")\n",
    "    except FileNotFoundError:\n",
    "        setup_logger.error(\"huggingface-cli not found. Ensure transformers/huggingface_hub are correctly installed and in PATH.\")\n",
    "    except Exception as e:\n",
    "        setup_logger.error(f\"An error occurred during Hugging Face CLI whoami check: {e}\")\n",
    "else:\n",
    "    setup_logger.warning(\"HF_TOKEN/HUGGINGFACE_HUB_TOKEN NOT found in environment variables. Model downloads for gated repos will likely fail.\")\n",
    "\n",
    "# 2. W&B Login Verification\n",
    "setup_logger.info(\"--- Weights & Biases Verification ---\")\n",
    "wandb_api_key = os.environ.get(\"WANDB_API_KEY\")\n",
    "wandb_project = os.environ.get(\"WANDB_PROJECT\")\n",
    "wandb_entity = os.environ.get(\"WANDB_ENTITY\")\n",
    "\n",
    "if wandb_api_key:\n",
    "    setup_logger.info(\"WANDB_API_KEY found in environment variables.\")\n",
    "    # The W&B library will automatically use this.\n",
    "    # We can try a dry-run init to verify, but this might create an empty run.\n",
    "    # A simpler check is just to confirm the key, project, and entity are present.\n",
    "    if wandb_project:\n",
    "        setup_logger.info(f\"WANDB_PROJECT found: {wandb_project}\")\n",
    "    else:\n",
    "        setup_logger.warning(\"WANDB_PROJECT NOT found in environment variables. Will need to be set in config or W&B UI.\")\n",
    "    \n",
    "    if wandb_entity:\n",
    "        setup_logger.info(f\"WANDB_ENTITY found: {wandb_entity}\")\n",
    "    else:\n",
    "        setup_logger.warning(\"WANDB_ENTITY NOT found in environment variables. Will use default or needs to be set in config/W&B UI.\")\n",
    "    \n",
    "    # You could try a programmatic login test if you want to be very thorough,\n",
    "    # but it might be overkill if the env var is present.\n",
    "    # Example (optional, might create a .netrc entry or require user interaction if key is wrong):\n",
    "    # try:\n",
    "    #     import wandb\n",
    "    #     wandb.login() # This will use the env var if available, or prompt if not\n",
    "    #     setup_logger.info(\"wandb.login() command executed (relies on env var or prompts if needed).\")\n",
    "    # except Exception as e:\n",
    "    #     setup_logger.error(f\"Error during wandb.login() test: {e}\")\n",
    "\n",
    "else:\n",
    "    setup_logger.warning(\"WANDB_API_KEY NOT found in environment variables. W&B logging will be disabled or require manual login/key entry.\")\n",
    "\n",
    "setup_logger.info(\"--- Verification Cell Complete ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zIGQxONxe9hI",
    "outputId": "a719634a-da89-4423-e84d-89829fbe5c25"
   },
   "outputs": [],
   "source": [
    "# Cell X (or start of Cell 7) on RunPods\n",
    "import os\n",
    "# No more google.colab.drive\n",
    "\n",
    "# Define your project's base path on the RunPods instance\n",
    "# This could be in /workspace (cleared when pod stops unless it's a persistent volume pod type)\n",
    "# or on a /mnt/volume/ if you have persistent storage attached.\n",
    "gdrive_project_base_path = \"/workspace/MyAdaptiveLearnerProject\" # ADJUST THIS PATH\n",
    "outputs_on_gdrive_path = os.path.join(gdrive_project_base_path, \"outputs\") # Name is just historical\n",
    "os.makedirs(outputs_on_gdrive_path, exist_ok=True)\n",
    "# The logger will be defined in Cell 3, so logging this path will happen later"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k-bhcEYjm0RN"
   },
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "d80-FXYqm0RN",
    "outputId": "8d599cb5-860c-420b-d759-1b775191d17b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-05-20 16:15:14,066 - __main__ - INFO - --- TEST: Adaptive Learner Logging Initialized ---\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import logging\n",
    "import sys # Import sys for stdout\n",
    "import time\n",
    "import io\n",
    "import base64\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Any, Tuple, Optional, Union\n",
    "from dataclasses import dataclass, field\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader # DataLoader was used here\n",
    "\n",
    "import transformers\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    PreTrainedModel,\n",
    "    PreTrainedTokenizer\n",
    ")\n",
    "\n",
    "# Ensure these are present for datasets library\n",
    "from datasets import load_dataset, DownloadMode, concatenate_datasets # << MAKE SURE THIS IS PRESENT AND UNCOMMENTED\n",
    "\n",
    "from peft import (\n",
    "    PeftModel,\n",
    "    PeftConfig,\n",
    "    PeftModelForCausalLM,\n",
    "    get_peft_model,\n",
    "    LoraConfig,\n",
    "    TaskType\n",
    ")\n",
    "\n",
    "import wandb\n",
    "from dotenv import load_dotenv\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# Try to import Colab specific userdata\n",
    "try:\n",
    "    from google.colab import userdata\n",
    "    WANDB_API_KEY_SECRET_NAME = \"WANDB_API_KEY\"\n",
    "    logger_colab_secrets = logging.getLogger(__name__ + \".colab_secrets\")\n",
    "except ImportError:\n",
    "    userdata = None\n",
    "    WANDB_API_KEY_SECRET_NAME = None\n",
    "    logger_colab_secrets = None\n",
    "\n",
    "\n",
    "# Configure logging - MORE EXPLICIT HANDLER\n",
    "# Remove all handlers associated with the root logger.\n",
    "for handler in logging.root.handlers[:]:\n",
    "    logging.root.removeHandler(handler)\n",
    "\n",
    "logging.basicConfig(level=logging.INFO,\n",
    "                  format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',\n",
    "                  stream=sys.stdout) # Force output to stdout\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "# Ensure our logger also propagates to the root and thus to the new basicConfig handler\n",
    "logger.propagate = True\n",
    "# Optionally set our specific logger level if needed, though INFO from basicConfig should cover it\n",
    "# logger.setLevel(logging.INFO)\n",
    "\n",
    "# Test if logger is working now\n",
    "logger.info(\"--- TEST: Adaptive Learner Logging Initialized ---\")\n",
    "\n",
    "\n",
    "# Load environment variables if .env file exists\n",
    "load_dotenv()\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "def set_seed(seed=123):\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "set_seed()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J2Adwfa3m0RN"
   },
   "source": [
    "## Config (Cell 7):\n",
    "Add feature_replay_alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iCN1IOTqm0RN",
    "outputId": "52672289-4c2e-4598-bc8e-c5a2958cd818"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO (AdaptiveLearnerConfig): Found WANDB_API_KEY in environment variables.\n",
      "INFO (AdaptiveLearnerConfig): Hugging Face token (HF_TOKEN/HUGGINGFACE_HUB_TOKEN) found in environment variables.\n",
      "INFO (AdaptiveLearnerConfig): Found WANDB_PROJECT in environment variables: RESTART01\n",
      "INFO (AdaptiveLearnerConfig): Found WANDB_ENTITY in environment variables: doingmyownthing82-none\n"
     ]
    }
   ],
   "source": [
    "# Cell 7: AdaptiveLearnerConfig (Modified for RunPods, Exp Tagging, SharedLoRA, etc.)\n",
    "import os\n",
    "import torch\n",
    "from dataclasses import dataclass, field\n",
    "from typing import List, Dict, Any, Tuple, Optional, Union\n",
    "\n",
    "# --- Define your RunPods project base path here ---\n",
    "RUNPODS_PROJECT_BASE_PATH = \"/workspace/MyAdaptiveLearnerProject\" # <<< ENSURE THIS IS YOUR RUNPODS PATH\n",
    "# --- End of RunPods path definition ---\n",
    "\n",
    "@dataclass\n",
    "class AdaptiveLearnerConfig:\n",
    "    \"\"\"Configuration class for The Adaptive Learner\"\"\"\n",
    "    model_name: str = \"google/gemma-2b\"\n",
    "    max_seq_length: int = 512\n",
    "    device: str = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "    lora_r: int = 8\n",
    "    lora_alpha: int = 16\n",
    "    lora_dropout: float = 0.05\n",
    "    target_modules: List[str] = field(default_factory=lambda: [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"])\n",
    "\n",
    "    batch_size: int = 2\n",
    "    learning_rate: float = 5e-5\n",
    "    weight_decay: float = 0.01\n",
    "    num_lora_train_epochs: int = 3\n",
    "\n",
    "    gamma_gain_lambda: float = 0.5\n",
    "    gamma_gain_metrics: List[str] = field(default_factory=lambda: [\"nll\", \"gradient_norm\", \"entropy\", \"accuracy\"])\n",
    "    gamma_gain_weights: Dict[str, float] = field(default_factory=lambda: {\"nll\": 0.3, \"gradient_norm\": 0.2, \"entropy\": 0.2, \"accuracy\": 0.3})\n",
    "\n",
    "    router_type: str = \"linear\"\n",
    "    router_hidden_size: int = 64\n",
    "    router_confidence_threshold: float = 0.7\n",
    "    router_learning_rate: float = 1e-4\n",
    "    router_training_epochs_per_call: int = 1\n",
    "\n",
    "    consolidation_method: str = \"ewc\"\n",
    "    aflora_importance_threshold: float = 0.01\n",
    "    ewc_lambda: float = 100.0\n",
    "    ewc_data_loader_num_samples: int = 5\n",
    "    ewc_batch_size: int = 1\n",
    "    ewc_fixed_lambda_bypass_gamma: bool = False\n",
    "\n",
    "    replay_method: str = \"cmt\"\n",
    "    replay_buffer_size: int = 500\n",
    "    replay_alpha: float = 0.5\n",
    "    feature_replay_alpha: float = 0.3\n",
    "\n",
    "    replay_model_learning_rate: float = 5e-5\n",
    "    replay_model_weight_decay: float = 1e-5\n",
    "    replay_model_grad_clip_norm: float = 1.0\n",
    "    replay_model_internal_batch_size: int = 2\n",
    "    replay_backbone_encoding_batch_size: int = 8\n",
    "\n",
    "    cmt_memory_size: int = 128\n",
    "    cmt_compressor_layers: int = 2\n",
    "    cmt_uniformity_weight: float = 0.1\n",
    "\n",
    "    pcgr_latent_dim: int = 256\n",
    "    pcgr_kl_weight: float = 0.1\n",
    "    pcgr_prototype_update_freq: int = 100\n",
    "\n",
    "    run_sft_baseline: bool = False\n",
    "    sft_lora_id: str = \"sft_baseline_adapter\"\n",
    "    benchmark_tasks_to_run: List[Dict[str, Any]] = field(default_factory=list)\n",
    "    num_train_samples_benchmark: int = 250\n",
    "    num_val_samples_benchmark: int = 50\n",
    "\n",
    "    output_dir: str = os.path.join(RUNPODS_PROJECT_BASE_PATH, \"outputs\")\n",
    "\n",
    "    experiment_tag: Optional[str] = None\n",
    "    load_router_state_from_tag: Optional[str] = None\n",
    "\n",
    "    k_examples_for_prototype: int = 3\n",
    "    num_tasks_to_share_lora: int = 0 # New field for Shared LoRA experiments\n",
    "\n",
    "    use_advanced_embeddings: bool = True\n",
    "    k_for_grad_sketch: int = 3\n",
    "    grad_sketch_layer_names: List[str] = field(default_factory=lambda: [\n",
    "        \"model.layers.2.self_attn.q_proj\",\n",
    "        \"model.layers.4.mlp.gate_proj\",\n",
    "    ])\n",
    "    grad_sketch_max_elements: int = 2048\n",
    "    use_context_stats_avg_token_entropy: bool = True\n",
    "    use_context_stats_avg_seq_length: bool = True\n",
    "\n",
    "    use_wandb: bool = True\n",
    "    wandb_project: str = \"adaptive-learner\"\n",
    "    wandb_entity: Optional[str] = None\n",
    "    wandb_api_key: Optional[str] = None\n",
    "\n",
    "    num_workers: int = 0\n",
    "\n",
    "    # Early Stopping Parameters for LoRA Training\n",
    "    use_lora_early_stopping: bool = False\n",
    "    lora_early_stopping_patience: int = 3\n",
    "    lora_early_stopping_metric: str = \"accuracy\" # Metric to monitor on val set\n",
    "    min_lora_epochs_before_early_stop: int = 1\n",
    "    lora_early_stopping_delta: float = 0.001\n",
    "\n",
    "\n",
    "    def __post_init__(self):\n",
    "        base_output_dir_to_check = self.output_dir\n",
    "        if 'RUNPODS_PROJECT_BASE_PATH' in globals() and not self.output_dir.startswith(globals()['RUNPODS_PROJECT_BASE_PATH']):\n",
    "            self.output_dir = os.path.join(globals()['RUNPODS_PROJECT_BASE_PATH'], \"outputs\")\n",
    "            base_output_dir_to_check = self.output_dir\n",
    "            print(f\"INFO (AdaptiveLearnerConfig.__post_init__): Corrected output_dir to: {self.output_dir}\")\n",
    "\n",
    "        os.makedirs(base_output_dir_to_check, exist_ok=True)\n",
    "\n",
    "        if self.experiment_tag:\n",
    "            exp_general_output_dir = os.path.join(base_output_dir_to_check, self.experiment_tag)\n",
    "            os.makedirs(exp_general_output_dir, exist_ok=True)\n",
    "\n",
    "        env_wandb_api_key = os.environ.get(\"WANDB_API_KEY\")\n",
    "        if env_wandb_api_key:\n",
    "            self.wandb_api_key = env_wandb_api_key\n",
    "            print(f\"INFO (AdaptiveLearnerConfig): Found WANDB_API_KEY in environment variables.\")\n",
    "        elif self.wandb_api_key:\n",
    "            print(f\"INFO (AdaptiveLearnerConfig): Using WANDB_API_KEY provided in config object.\")\n",
    "        elif 'userdata' in globals() and userdata is not None and 'WANDB_API_KEY_SECRET_NAME' in globals() and WANDB_API_KEY_SECRET_NAME is not None:\n",
    "            try:\n",
    "                self.wandb_api_key = userdata.get(WANDB_API_KEY_SECRET_NAME)\n",
    "                print(f\"INFO (AdaptiveLearnerConfig): Fetched WANDB_API_KEY from Colab userdata.\")\n",
    "            except Exception as e:\n",
    "                print(f\"WARNING (AdaptiveLearnerConfig): Failed to get WANDB_API_KEY from Colab userdata: {e}\")\n",
    "\n",
    "        if os.environ.get(\"HF_TOKEN\") or os.environ.get(\"HUGGINGFACE_HUB_TOKEN\"):\n",
    "            print(f\"INFO (AdaptiveLearnerConfig): Hugging Face token (HF_TOKEN/HUGGINGFACE_HUB_TOKEN) found in environment variables.\")\n",
    "\n",
    "        env_wandb_project = os.environ.get(\"WANDB_PROJECT\")\n",
    "        if env_wandb_project:\n",
    "            self.wandb_project = env_wandb_project\n",
    "            print(f\"INFO (AdaptiveLearnerConfig): Found WANDB_PROJECT in environment variables: {self.wandb_project}\")\n",
    "\n",
    "        env_wandb_entity = os.environ.get(\"WANDB_ENTITY\")\n",
    "        if env_wandb_entity:\n",
    "            self.wandb_entity = env_wandb_entity\n",
    "            print(f\"INFO (AdaptiveLearnerConfig): Found WANDB_ENTITY in environment variables: {self.wandb_entity}\")\n",
    "\n",
    "    def to_dict(self) -> Dict[str, Any]:\n",
    "        return {k: v for k, v in self.__dict__.items() if not k.startswith('_')}\n",
    "\n",
    "default_config = AdaptiveLearnerConfig()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V_2l9A-pm0RN"
   },
   "source": [
    "## Backbone Model\n",
    "\n",
    "The backbone module handles the base LLM and efficient processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "S0uOPbkNm0RN"
   },
   "outputs": [],
   "source": [
    "class AdaptiveLearnerBackbone:\n",
    "    \"\"\"\n",
    "    LLM Backbone module that handles the base LLM model and efficient input processing\n",
    "    \"\"\"\n",
    "    def __init__(self, config: AdaptiveLearnerConfig):\n",
    "        \"\"\"\n",
    "        Initialize the LLM backbone\n",
    "\n",
    "        Args:\n",
    "            config (AdaptiveLearnerConfig): Configuration object\n",
    "        \"\"\"\n",
    "        self.config = config\n",
    "        self.device = torch.device(config.device)\n",
    "\n",
    "        logger.info(f\"Loading model: {config.model_name}\")\n",
    "        self.model = self._load_model()\n",
    "        self.tokenizer = self._load_tokenizer()\n",
    "\n",
    "        # Count parameters\n",
    "        num_params = self._count_parameters()\n",
    "        logger.info(f\"Model loaded with {num_params:,} parameters\")\n",
    "\n",
    "        # Freeze base model parameters\n",
    "        self._freeze_model_parameters()\n",
    "        logger.info(\"Base model parameters frozen\")\n",
    "\n",
    "    def _load_model(self) -> PreTrainedModel:\n",
    "        \"\"\"Load the LLM model\"\"\"\n",
    "        model_name = self.config.model_name\n",
    "        custom_model_path = os.environ.get(\"CUSTOM_MODEL_PATH\")\n",
    "        if custom_model_path and os.path.exists(custom_model_path):\n",
    "            model_name = custom_model_path\n",
    "            logger.info(f\"Using custom model path: {model_name}\")\n",
    "\n",
    "        model_kwargs = {\n",
    "            \"torch_dtype\": torch.float16,\n",
    "            \"device_map\": \"auto\",\n",
    "            \"load_in_8bit\": True # Maintained for memory efficiency\n",
    "        }\n",
    "\n",
    "        # Attempt to use Flash Attention 2 if available and configured\n",
    "        use_flash_attn = getattr(self.config, 'use_flash_attention_2', True) # Default to True if not in config\n",
    "                                                                            # Or, you can add use_flash_attention_2 to AdaptiveLearnerConfig\n",
    "        if use_flash_attn:\n",
    "            logger.info(\"Attempting to load model with Flash Attention 2.\")\n",
    "            try:\n",
    "                model = AutoModelForCausalLM.from_pretrained(\n",
    "                    model_name,\n",
    "                    **model_kwargs,\n",
    "                    attn_implementation=\"flash_attention_2\"\n",
    "                )\n",
    "                logger.info(\"Model loaded successfully with Flash Attention 2 and 8-bit quantization.\")\n",
    "                return model # Return early if successful\n",
    "            except Exception as e_fa:\n",
    "                logger.warning(f\"Failed to load with Flash Attention 2 (Error: {e_fa}). Falling back.\")\n",
    "\n",
    "        # Fallback if Flash Attention 2 fails or is not requested\n",
    "        try:\n",
    "            logger.info(\"Attempting to load model with 8-bit quantization (Flash Attention not explicitly enabled or auto-detected).\")\n",
    "            model = AutoModelForCausalLM.from_pretrained(\n",
    "                model_name,\n",
    "                **model_kwargs # attn_implementation will not be in model_kwargs here\n",
    "            )\n",
    "            logger.info(\"Model loaded in 8-bit quantization.\")\n",
    "        except Exception as e_8bit:\n",
    "            logger.warning(f\"8-bit loading also failed (Error: {e_8bit}). Falling back to fp16/fp32 without 8-bit.\")\n",
    "            # Remove 8-bit specific kwarg for final fallback\n",
    "            del model_kwargs[\"load_in_8bit\"]\n",
    "            model = AutoModelForCausalLM.from_pretrained(\n",
    "                model_name,\n",
    "                torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,\n",
    "                device_map=\"auto\" if torch.cuda.is_available() else None\n",
    "                # attn_implementation=\"eager\" # or let transformers decide default\n",
    "            )\n",
    "            if not torch.cuda.is_available():\n",
    "                model = model.to(self.device)\n",
    "            logger.info(\"Model loaded in standard fp16/fp32.\")\n",
    "        return model\n",
    "\n",
    "    def _load_tokenizer(self) -> PreTrainedTokenizer:\n",
    "        \"\"\"Load the tokenizer\"\"\"\n",
    "        model_name = self.config.model_name\n",
    "        custom_model_path = os.environ.get(\"CUSTOM_MODEL_PATH\")\n",
    "        if custom_model_path and os.path.exists(custom_model_path):\n",
    "            model_name = custom_model_path\n",
    "\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "        if tokenizer.pad_token is None:\n",
    "            if tokenizer.eos_token is not None:\n",
    "                tokenizer.pad_token = tokenizer.eos_token\n",
    "            else:\n",
    "                # Fallback for models like gemma that might not have eos_token set by default in some versions/configs\n",
    "                # and tokenizer.unk_token might also be problematic if not present.\n",
    "                # A common practice is to add a special pad token if truly missing.\n",
    "                # However, for now, let's stick to eos or unk if eos is missing.\n",
    "                # If unk is also missing, this would be an issue with the base tokenizer.\n",
    "                if tokenizer.unk_token is not None:\n",
    "                    tokenizer.pad_token = tokenizer.unk_token\n",
    "                else: # Add a new pad token if unk is also None\n",
    "                    logger.warning(f\"Tokenizer for {model_name} has no pad_token, eos_token, or unk_token. Adding a new pad_token '<|pad|>'.\")\n",
    "                    tokenizer.add_special_tokens({'pad_token': '<|pad|>'})\n",
    "                    # If we add a token, the model's embedding matrix needs to be resized.\n",
    "                    # This is usually handled if you train the whole model, but for PEFT,\n",
    "                    # it's safer if the base tokenizer has it.\n",
    "                    # self.model.resize_token_embeddings(len(tokenizer)) # This would make base model params trainable.\n",
    "                    # For now, rely on pre-existing tokens.\n",
    "        return tokenizer\n",
    "\n",
    "    def _freeze_model_parameters(self):\n",
    "        \"\"\"Freeze all model parameters\"\"\"\n",
    "        for param in self.model.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "    def _count_parameters(self) -> int:\n",
    "        \"\"\"Count the number of parameters in the model\"\"\"\n",
    "        return sum(p.numel() for p in self.model.parameters())\n",
    "\n",
    "    def chunk_and_process(self, texts: List[str], max_chunk_size: Optional[int] = None) -> List[Dict[str, torch.Tensor]]:\n",
    "        \"\"\"\n",
    "        Process text inputs by chunking them into manageable pieces\n",
    "\n",
    "        Args:\n",
    "            texts (List[str]): List of input texts\n",
    "            max_chunk_size (int, optional): Maximum chunk size (tokens). Defaults to config.max_seq_length.\n",
    "\n",
    "        Returns:\n",
    "            List[Dict[str, torch.Tensor]]: List of processed chunks\n",
    "        \"\"\"\n",
    "        if max_chunk_size is None:\n",
    "            max_chunk_size = self.config.max_seq_length\n",
    "\n",
    "        tokenized_texts = [self.tokenizer(text, return_tensors=\"pt\", truncation=False) for text in texts]\n",
    "        chunks = []\n",
    "        for i, tokenized in enumerate(tokenized_texts):\n",
    "            input_ids = tokenized['input_ids'][0]\n",
    "            attention_mask = tokenized['attention_mask'][0]\n",
    "            if len(input_ids) <= max_chunk_size:\n",
    "                chunks.append({\n",
    "                    'input_ids': input_ids.to(self.device),\n",
    "                    'attention_mask': attention_mask.to(self.device),\n",
    "                    'text_idx': i, 'chunk_idx': 0\n",
    "                })\n",
    "            else:\n",
    "                for j in range(0, len(input_ids), max_chunk_size):\n",
    "                    end_idx = min(j + max_chunk_size, len(input_ids))\n",
    "                    chunks.append({\n",
    "                        'input_ids': input_ids[j:end_idx].to(self.device),\n",
    "                        'attention_mask': attention_mask[j:end_idx].to(self.device),\n",
    "                        'text_idx': i, 'chunk_idx': j // max_chunk_size\n",
    "                    })\n",
    "        return chunks\n",
    "\n",
    "    def generate(self,\n",
    "                prompt: str,\n",
    "                max_new_tokens: int = 100,\n",
    "                temperature: float = 0.7,\n",
    "                top_p: float = 0.9,\n",
    "                do_sample: bool = True) -> str:\n",
    "        \"\"\"Generate text from a prompt\"\"\"\n",
    "        inputs = self.tokenizer(prompt, return_tensors=\"pt\").to(self.model.device) # Ensure inputs are on same device as model\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model.generate(\n",
    "                **inputs, max_new_tokens=max_new_tokens, temperature=temperature,\n",
    "                top_p=top_p, do_sample=do_sample, pad_token_id=self.tokenizer.pad_token_id\n",
    "            )\n",
    "        generated_text = self.tokenizer.decode(outputs[0][inputs['input_ids'].shape[1]:], skip_special_tokens=True)\n",
    "        return generated_text\n",
    "\n",
    "    def get_embedding(self, text: str) -> torch.Tensor:\n",
    "        \"\"\"Get the embedding for a text input (last hidden state of last token)\"\"\"\n",
    "        inputs = self.tokenizer(text, return_tensors=\"pt\").to(self.model.device) # Ensure inputs are on same device as model\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model(**inputs, output_hidden_states=True)\n",
    "            last_hidden_state = outputs.hidden_states[-1][0][-1] # [batch_size=0 (implicit), seq_pos=-1 (last token), hidden_dim]\n",
    "        return last_hidden_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 8.5: Embedding Utility\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from typing import List, Dict, Any, Optional, Union\n",
    "from transformers import PreTrainedModel, PreTrainedTokenizer\n",
    "\n",
    "# Make sure logger is accessible if you want to use it here, or pass it as an argument.\n",
    "# For now, we'll rely on the global logger if it's set up, or print for critical errors.\n",
    "# Ensure 'logger' is defined globally, typically in an earlier cell (e.g., Cell 3 or 4).\n",
    "# If not, you might need to pass logger as an argument or use print for simplicity here.\n",
    "if 'logger' not in globals():\n",
    "    import logging\n",
    "    logger = logging.getLogger(__name__ + \".prototype_util\")\n",
    "    if not logger.hasHandlers(): # Avoid adding multiple handlers if re-run\n",
    "        handler = logging.StreamHandler()\n",
    "        formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "        handler.setFormatter(formatter)\n",
    "        logger.addHandler(handler)\n",
    "        logger.setLevel(logging.INFO)\n",
    "\n",
    "\n",
    "def get_task_example_prototype_embedding(\n",
    "    raw_examples: List[Dict[str, str]],\n",
    "    k: int,\n",
    "    task_text_field: Union[str, List[str]], # Added: to know how to extract input text\n",
    "    backbone_model: PreTrainedModel,\n",
    "    tokenizer: PreTrainedTokenizer,\n",
    "    device: str,\n",
    "    max_seq_length: int, # Added: for tokenization consistency\n",
    "    config: Any # Using 'Any' for now, assuming it's AdaptiveLearnerConfig like object\n",
    ") -> Optional[torch.Tensor]:\n",
    "    \"\"\"\n",
    "    Generates a task example prototype embedding by averaging the embeddings of k examples.\n",
    "\n",
    "    Args:\n",
    "        raw_examples: A list of raw example dictionaries (e.g., from load_benchmark_task).\n",
    "                      Each dict should have 'input' and 'output' keys, though only 'input' is used here.\n",
    "        k: The number of examples to use for the prototype.\n",
    "        task_text_field: The key (or list of keys) for the input text field(s) in raw_examples,\n",
    "                         similar to how load_benchmark_task handles it.\n",
    "        backbone_model: The frozen backbone LLM.\n",
    "        tokenizer: The tokenizer for the backbone LLM.\n",
    "        device: The device to run inferences on (e.g., 'cuda').\n",
    "        max_seq_length: Maximum sequence length for tokenization.\n",
    "        config: The AdaptiveLearnerConfig object (or similar) for parameters like\n",
    "                replay_backbone_encoding_batch_size.\n",
    "\n",
    "    Returns:\n",
    "        A torch.Tensor representing the prototype embedding, or None if an error occurs.\n",
    "    \"\"\"\n",
    "    if not raw_examples:\n",
    "        logger.warning(\"get_task_example_prototype_embedding: No raw examples provided. Returning None.\")\n",
    "        return None\n",
    "    if k <= 0:\n",
    "        logger.warning(f\"get_task_example_prototype_embedding: k must be > 0, but got {k}. Returning None.\")\n",
    "        return None\n",
    "\n",
    "    # Select k examples (e.g., the first k, or a random k if preferred and raw_examples is large enough)\n",
    "    # For simplicity and reproducibility in this step, let's take the first min(k, len(raw_examples)).\n",
    "    selected_examples = raw_examples[:min(k, len(raw_examples))]\n",
    "    if not selected_examples: # Should not happen if raw_examples is not empty and k > 0\n",
    "        logger.error(\"get_task_example_prototype_embedding: No examples selected. This is unexpected. Returning None.\")\n",
    "        return None\n",
    "\n",
    "    logger.info(f\"Generating prototype from {len(selected_examples)} examples (requested k={k}).\")\n",
    "\n",
    "    example_input_texts = []\n",
    "    for ex in selected_examples:\n",
    "        input_val = None\n",
    "        if isinstance(task_text_field, list): # Handle sentence-pair tasks\n",
    "            input_parts = [str(ex.get(tf, \"\")) for tf in task_text_field]\n",
    "            input_val = \" [SEP] \".join(input_parts) # Using [SEP] as a common separator\n",
    "        else: # Single sentence task\n",
    "            input_val = str(ex.get(task_text_field, \"\"))\n",
    "        \n",
    "        if not input_val: # Skip if input text is empty\n",
    "            logger.warning(f\"Empty input text for example: {ex}. Skipping for prototype.\")\n",
    "            continue\n",
    "        example_input_texts.append(input_val)\n",
    "\n",
    "    if not example_input_texts:\n",
    "        logger.warning(\"get_task_example_prototype_embedding: No valid input texts found in selected examples. Returning None.\")\n",
    "        return None\n",
    "\n",
    "    # Store original model training state and set to eval\n",
    "    original_training_state = backbone_model.training\n",
    "    backbone_model.eval()\n",
    "\n",
    "    all_example_embeddings = []\n",
    "    # Use replay_backbone_encoding_batch_size for batching if k is large\n",
    "    # This config parameter is typically used in CMTReplay.encode_input_batch\n",
    "    # Let's use it here for consistency if available.\n",
    "    batch_size_for_encoding = getattr(config, 'replay_backbone_encoding_batch_size', 1)\n",
    "    if batch_size_for_encoding <= 0: batch_size_for_encoding = 1\n",
    "\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i in range(0, len(example_input_texts), batch_size_for_encoding):\n",
    "            batch_texts = example_input_texts[i:i + batch_size_for_encoding]\n",
    "            \n",
    "            inputs = tokenizer(\n",
    "                batch_texts,\n",
    "                return_tensors=\"pt\",\n",
    "                padding=\"longest\", # Pad to the longest sequence in the current batch\n",
    "                truncation=True,\n",
    "                max_length=max_seq_length,\n",
    "                add_special_tokens=True\n",
    "            ).to(device)\n",
    "\n",
    "            # Get hidden states from the backbone model\n",
    "            # Similar to Router.get_text_embedding or CMTReplay.encode_input_batch\n",
    "            try:\n",
    "                outputs = backbone_model(**inputs, output_hidden_states=True)\n",
    "                last_hidden_state_batch = outputs.hidden_states[-1] # [batch_size, seq_len, hidden_dim]\n",
    "\n",
    "                # Mean pooling of token embeddings, respecting attention mask\n",
    "                attention_mask_expanded = inputs['attention_mask'].unsqueeze(-1).expand(last_hidden_state_batch.size()).float()\n",
    "                sum_embeddings = torch.sum(last_hidden_state_batch * attention_mask_expanded, 1) # Sum along seq_len\n",
    "                sum_mask = torch.clamp(attention_mask_expanded.sum(1), min=1e-9) # Sum of mask elements per sequence\n",
    "                pooled_embeddings_batch = sum_embeddings / sum_mask # [batch_size, hidden_dim]\n",
    "                \n",
    "                all_example_embeddings.append(pooled_embeddings_batch)\n",
    "\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error during backbone model inference for prototype examples: {e}\")\n",
    "                # Restore original model training state before returning\n",
    "                if original_training_state:\n",
    "                    backbone_model.train()\n",
    "                else:\n",
    "                    backbone_model.eval() # Ensure it's left in eval if it started that way\n",
    "                return None\n",
    "    \n",
    "    # Restore original model training state\n",
    "    if original_training_state:\n",
    "        backbone_model.train()\n",
    "    else:\n",
    "        backbone_model.eval() # Ensure it's left in eval if it started that way\n",
    "\n",
    "    if not all_example_embeddings:\n",
    "        logger.warning(\"get_task_example_prototype_embedding: No embeddings were generated from examples. Returning None.\")\n",
    "        return None\n",
    "\n",
    "    # Concatenate embeddings from all batches and average them\n",
    "    try:\n",
    "        concatenated_embeddings = torch.cat(all_example_embeddings, dim=0) # Shape: [total_valid_examples, hidden_dim]\n",
    "        prototype_embedding = torch.mean(concatenated_embeddings, dim=0) # Shape: [hidden_dim]\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error concatenating or averaging example embeddings: {e}\")\n",
    "        return None\n",
    "\n",
    "    logger.info(f\"Successfully generated prototype embedding of shape: {prototype_embedding.shape}\")\n",
    "    return prototype_embedding.to(device) # Ensure it's on the correct device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 8.6: Advanced Embedding Helper Functions\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from typing import List, Dict, Any, Optional, Union, Tuple\n",
    "from transformers import PreTrainedModel, PreTrainedTokenizer\n",
    "\n",
    "if 'logger' not in globals():\n",
    "    import logging\n",
    "    logger = logging.getLogger(__name__ + \".advanced_embed_utils\")\n",
    "    if not logger.hasHandlers():\n",
    "        handler = logging.StreamHandler()\n",
    "        formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "        handler.setFormatter(formatter)\n",
    "        logger.addHandler(handler)\n",
    "        logger.setLevel(logging.INFO)\n",
    "\n",
    "def generate_gradient_sketch(\n",
    "    examples: List[Dict[str, str]], # Should contain 'input' and 'output' for loss calculation\n",
    "    model: PreTrainedModel,\n",
    "    tokenizer: PreTrainedTokenizer,\n",
    "    config: Any, # AdaptiveLearnerConfig\n",
    "    device: str\n",
    ") -> Optional[torch.Tensor]:\n",
    "    \"\"\"\n",
    "    Generates a gradient sketch for a few task examples.\n",
    "    Passes examples through the frozen base model, calculates loss,\n",
    "    and extracts gradients w.r.t. specified layer weights.\n",
    "    \"\"\"\n",
    "    if not config.use_advanced_embeddings or not hasattr(config, 'k_for_grad_sketch') or config.k_for_grad_sketch <= 0:\n",
    "        return None\n",
    "    if not examples:\n",
    "        logger.warning(\"GradientSketch: No examples provided.\")\n",
    "        return None\n",
    "    if not hasattr(config, 'grad_sketch_layer_names') or not config.grad_sketch_layer_names:\n",
    "        logger.warning(\"GradientSketch: No grad_sketch_layer_names defined in config.\")\n",
    "        return None\n",
    "\n",
    "    selected_examples = examples[:min(config.k_for_grad_sketch, len(examples))]\n",
    "    if not selected_examples:\n",
    "        logger.warning(\"GradientSketch: No examples selected for sketch.\")\n",
    "        return None\n",
    "\n",
    "    # --- Prepare model and identify target parameters ---\n",
    "    original_training_state = model.training\n",
    "    model.eval() # Ensure model is in eval mode\n",
    "\n",
    "    # Store original requires_grad states\n",
    "    original_requires_grad = {name: p.requires_grad for name, p in model.named_parameters()}\n",
    "\n",
    "    # Set requires_grad to False for all, then True only for target layers\n",
    "    model.requires_grad_(False)\n",
    "    \n",
    "    target_params_with_grads = []\n",
    "    target_param_names_found = []\n",
    "\n",
    "    for name_part in config.grad_sketch_layer_names:\n",
    "        # Assuming layer_names in config are module names, and we target their '.weight'\n",
    "        full_param_name = f\"{name_part}.weight\"\n",
    "        try:\n",
    "            param_module = model.get_submodule(name_part)\n",
    "            if hasattr(param_module, 'weight') and isinstance(param_module.weight, torch.nn.Parameter):\n",
    "                param_module.weight.requires_grad_(True)\n",
    "                target_params_with_grads.append(param_module.weight)\n",
    "                target_param_names_found.append(full_param_name)\n",
    "            else:\n",
    "                logger.warning(f\"GradientSketch: Could not find/access weight for module: {name_part}\")\n",
    "        except AttributeError:\n",
    "            logger.warning(f\"GradientSketch: Module {name_part} not found in model.\")\n",
    "\n",
    "    if not target_params_with_grads:\n",
    "        logger.error(\"GradientSketch: No target parameters found or set for gradient extraction. Aborting sketch generation.\")\n",
    "        # Restore original requires_grad states\n",
    "        for name, p in model.named_parameters():\n",
    "            if name in original_requires_grad:\n",
    "                p.requires_grad_(original_requires_grad[name])\n",
    "        if original_training_state: model.train()\n",
    "        return None\n",
    "    \n",
    "    logger.info(f\"GradientSketch: Identified {len(target_params_with_grads)} target parameters for sketch: {target_param_names_found}\")\n",
    "\n",
    "    # --- Process examples and accumulate gradients ---\n",
    "    # Use a temporary dataset/loader for batching\n",
    "    # Ensure tokenizer.pad_token_id is set (already handled in CausalLMTrainingDataset)\n",
    "    temp_dataset = CausalLMTrainingDataset(selected_examples, tokenizer, config.max_seq_length)\n",
    "    # Batch size for grad sketch can be small, e.g., 1 or k_for_grad_sketch itself\n",
    "    temp_loader = torch.utils.data.DataLoader(temp_dataset, batch_size=min(len(selected_examples), config.k_for_grad_sketch), shuffle=False)\n",
    "\n",
    "    accumulated_total_loss = torch.tensor(0.0, device=device)\n",
    "\n",
    "    for batch_data in temp_loader:\n",
    "        input_ids = batch_data[\"input_ids\"].to(device)\n",
    "        attention_mask = batch_data[\"attention_mask\"].to(device)\n",
    "        labels = batch_data[\"labels\"].to(device)\n",
    "\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        loss = outputs.loss\n",
    "        if loss is not None and torch.isfinite(loss):\n",
    "            accumulated_total_loss += loss # Accumulate loss before backward if needed, or just backward per batch\n",
    "            loss.backward() # Accumulates gradients on target_params_with_grads\n",
    "        else:\n",
    "            logger.warning(f\"GradientSketch: Loss was None or non-finite for a batch. Skipping backward for this batch.\")\n",
    "    \n",
    "    # --- Extract, concatenate, and process gradients ---\n",
    "    all_gradients_list = []\n",
    "    for param in target_params_with_grads:\n",
    "        if param.grad is not None:\n",
    "            all_gradients_list.append(param.grad.data.detach().cpu().flatten())\n",
    "        else:\n",
    "            # Param might not have received grad if it wasn't involved in loss computation for any example\n",
    "            logger.warning(f\"GradientSketch: Parameter {param.shape} had no gradient. Filling with zeros.\")\n",
    "            all_gradients_list.append(torch.zeros(param.numel(), dtype=torch.float32, device='cpu'))\n",
    "\n",
    "\n",
    "    # --- Cleanup: Zero gradients and restore original requires_grad states ---\n",
    "    model.zero_grad() # Zero out all gradients on the model\n",
    "    for name, p in model.named_parameters(): # Restore original requires_grad states\n",
    "        if name in original_requires_grad:\n",
    "            p.requires_grad_(original_requires_grad[name])\n",
    "    \n",
    "    if original_training_state: # Restore original model training state\n",
    "        model.train()\n",
    "    # else: model.eval() # Already in eval\n",
    "\n",
    "    if not all_gradients_list:\n",
    "        logger.error(\"GradientSketch: No gradients were extracted.\")\n",
    "        return None\n",
    "\n",
    "    concatenated_gradients = torch.cat(all_gradients_list)\n",
    "    \n",
    "    # Subsetting / Projection\n",
    "    if concatenated_gradients.numel() == 0:\n",
    "        logger.error(\"GradientSketch: Concatenated gradients are empty.\")\n",
    "        return None\n",
    "\n",
    "    if concatenated_gradients.numel() > config.grad_sketch_max_elements:\n",
    "        gradient_sketch = concatenated_gradients[:config.grad_sketch_max_elements]\n",
    "    elif concatenated_gradients.numel() < config.grad_sketch_max_elements:\n",
    "        padding_size = config.grad_sketch_max_elements - concatenated_gradients.numel()\n",
    "        gradient_sketch = F.pad(concatenated_gradients, (0, padding_size), 'constant', 0)\n",
    "    else:\n",
    "        gradient_sketch = concatenated_gradients\n",
    "    \n",
    "    logger.info(f\"GradientSketch: Generated sketch of shape {gradient_sketch.shape}, Loss sum: {accumulated_total_loss.item():.4f}\")\n",
    "    return gradient_sketch.to(device=device, dtype=torch.float32) # Ensure consistent dtype for router\n",
    "\n",
    "def generate_context_stats(\n",
    "    examples: List[Dict[str, str]], # 'input' field used\n",
    "    tokenizer: PreTrainedTokenizer,\n",
    "    model: PreTrainedModel, # Frozen base model for token entropy\n",
    "    config: Any, # AdaptiveLearnerConfig\n",
    "    device: str\n",
    ") -> Optional[torch.Tensor]:\n",
    "    \"\"\"\n",
    "    Generates context statistics: average sequence length and average token entropy.\n",
    "    \"\"\"\n",
    "    if not config.use_advanced_embeddings:\n",
    "        return None\n",
    "    if not examples:\n",
    "        logger.warning(\"ContextStats: No examples provided.\")\n",
    "        return None\n",
    "\n",
    "    stats = []\n",
    "    # For now, k_for_grad_sketch can be reused, or a new config param can be added.\n",
    "    num_examples_for_stats = getattr(config, 'k_for_context_stats', config.k_for_grad_sketch)\n",
    "    selected_examples = examples[:min(num_examples_for_stats, len(examples))]\n",
    "    if not selected_examples:\n",
    "        return None\n",
    "        \n",
    "    input_texts = [ex['input'] for ex in selected_examples]\n",
    "    tokenized_inputs = tokenizer(\n",
    "        input_texts, \n",
    "        return_tensors=\"pt\", \n",
    "        padding=\"longest\", \n",
    "        truncation=True, \n",
    "        max_length=config.max_seq_length\n",
    "    )\n",
    "\n",
    "    # 1. Average Sequence Length (of actual tokens, not padding)\n",
    "    if config.use_context_stats_avg_seq_length:\n",
    "        sequence_lengths = tokenized_inputs['attention_mask'].sum(dim=1).float()\n",
    "        avg_seq_len = sequence_lengths.mean().item()\n",
    "        stats.append(avg_seq_len)\n",
    "\n",
    "    # 2. Average Token Entropy (model-based predictive entropy)\n",
    "    if config.use_context_stats_avg_token_entropy:\n",
    "        original_training_state = model.training\n",
    "        model.eval()\n",
    "        avg_token_entropy_overall = 0.0\n",
    "        num_valid_seqs_for_entropy = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            ids = tokenized_inputs['input_ids'].to(device)\n",
    "            mask = tokenized_inputs['attention_mask'].to(device)\n",
    "            \n",
    "            outputs = model(input_ids=ids, attention_mask=mask)\n",
    "            logits = outputs.logits # [batch_size, seq_len, vocab_size]\n",
    "\n",
    "            # Consider entropy of predictions for actual tokens (masked by attention_mask)\n",
    "            # and perhaps only for the part of the sequence that corresponds to the input, not generated part.\n",
    "            # For simplicity, let's average entropy over all non-padded tokens.\n",
    "            token_entropies_batch = []\n",
    "            for b in range(logits.shape[0]):\n",
    "                seq_logits = logits[b, :, :] # [seq_len, vocab_size]\n",
    "                seq_mask = mask[b, :]       # [seq_len]\n",
    "                \n",
    "                active_logits = seq_logits[seq_mask == 1] # [num_active_tokens, vocab_size]\n",
    "                if active_logits.shape[0] == 0:\n",
    "                    continue\n",
    "\n",
    "                probs = F.softmax(active_logits.float(), dim=-1) # Ensure float for softmax stability\n",
    "                log_probs = F.log_softmax(active_logits.float(), dim=-1)\n",
    "                entropy_per_token = -torch.sum(probs * log_probs, dim=-1) # [num_active_tokens]\n",
    "                \n",
    "                if entropy_per_token.numel() > 0:\n",
    "                    token_entropies_batch.append(entropy_per_token.mean().item())\n",
    "            \n",
    "            if token_entropies_batch:\n",
    "                avg_token_entropy_overall = np.mean(token_entropies_batch)\n",
    "            else: # Default if no active tokens\n",
    "                avg_token_entropy_overall = 0.0 \n",
    "\n",
    "        stats.append(avg_token_entropy_overall)\n",
    "        if original_training_state: model.train()\n",
    "\n",
    "    # 3. Domain ID (Optional, not implemented yet)\n",
    "    # if hasattr(config, 'num_domains_for_one_hot') and config.num_domains_for_one_hot > 0:\n",
    "    #     # domain_id = get_domain_id_for_task(...) # Needs a helper\n",
    "    #     # one_hot_domain = F.one_hot(torch.tensor(domain_id), num_classes=config.num_domains_for_one_hot)\n",
    "    #     # stats.extend(one_hot_domain.float().tolist())\n",
    "    #     pass\n",
    "\n",
    "    if not stats:\n",
    "        return None\n",
    "        \n",
    "    context_stats_tensor = torch.tensor(stats, dtype=torch.float32, device=device)\n",
    "    logger.info(f\"ContextStats: Generated stats vector: {context_stats_tensor.tolist()}\")\n",
    "    return context_stats_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Vn6XtMehm0RN"
   },
   "source": [
    "PEFT Manager and Router Classes (Updated for Training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "JBcEVY1-m0RN"
   },
   "outputs": [],
   "source": [
    "# Cell 10: PEFT Manager and Router Classes\n",
    "# ROBUST PROFILE DILUTION FIX v3\n",
    "# + ENHANCED DIAGNOSTIC LOGGING in LinearRouter save/load_router_state\n",
    "# + MVE-Router-Playbook-Phase1 (Advanced Embeddings) modifications\n",
    "\n",
    "import torch.optim as optim\n",
    "import os\n",
    "import torch\n",
    "import json\n",
    "from pathlib import Path\n",
    "import time\n",
    "from peft import PeftModel, LoraConfig, TaskType, get_peft_model, PeftConfig\n",
    "from transformers import PreTrainedModel, AutoTokenizer\n",
    "from typing import List, Dict, Any, Tuple, Optional, Union\n",
    "from tqdm.notebook import tqdm\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import numpy as np # Added for potential use in advanced features, e.g. context stats\n",
    "\n",
    "# Ensure CausalLMTrainingDataset is available if used by helpers called from router\n",
    "# (It's defined in Cell 16, so this assumes Cell 16 has run or it's defined globally)\n",
    "# from YOUR_NOTEBOOK_FILE import CausalLMTrainingDataset # Or ensure it's defined before this cell\n",
    "\n",
    "# Import helper functions for advanced embeddings (assuming they are in Cell 8.6)\n",
    "# These will be called by the Router's get_advanced_task_representation\n",
    "# from YOUR_NOTEBOOK_FILE import generate_gradient_sketch, generate_context_stats # Or ensure they are defined\n",
    "\n",
    "if 'logger' not in globals():\n",
    "    import logging\n",
    "    logger = logging.getLogger(__name__ + \".peft_router_adv_embed\") # Updated logger name\n",
    "    if not logger.hasHandlers():\n",
    "        handler = logging.StreamHandler(); formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "        handler.setFormatter(formatter); logger.addHandler(handler); logger.setLevel(logging.INFO)\n",
    "\n",
    "class PEFTManager:\n",
    "    def __init__(self, config: Any, model: PreTrainedModel):\n",
    "        self.config = config; self.model = model; self.device = torch.device(config.device)\n",
    "        self.lora_modules_metadata: Dict[str, Dict[str, Any]] = {}; self.active_lora_ids: List[str] = []\n",
    "        self.lora_modules_base_dir = os.path.join(self.config.output_dir, \"lora_modules\"); os.makedirs(self.lora_modules_base_dir, exist_ok=True)\n",
    "        self.router_states_dir = os.path.join(self.config.output_dir, \"router_states\"); os.makedirs(self.router_states_dir, exist_ok=True)\n",
    "        if self.config.router_type == \"linear\": self.router = LinearRouter(self.config, self.model)\n",
    "        elif self.config.router_type == \"mlp\": self.router = MLPRouter(self.config, self.model)\n",
    "        else: raise ValueError(f\"Unknown router type: {self.config.router_type}\")\n",
    "        logger.info(f\"PEFTManager init: Router object instantiated. Initial learnable profiles in router: {len(getattr(self.router, 'module_embeddings', {})) if hasattr(self.router, 'module_embeddings') else len(getattr(self.router, 'output_projection_layers', {})) }\")\n",
    "        if self.config.load_router_state_from_tag:\n",
    "            path_to_load_router_state = os.path.join(self.router_states_dir, f\"router_state_{self.config.load_router_state_from_tag}.pth\")\n",
    "            logger.info(f\"PEFTManager init: Attempting to load router state from '{path_to_load_router_state}' directly into router.\")\n",
    "            if os.path.exists(path_to_load_router_state): self.router.load_router_state(path_to_load_router_state); num_learnable_profiles_after_load = len(self.router.module_embeddings) if hasattr(self.router, 'module_embeddings') else len(self.router.output_projection_layers); logger.info(f\"PEFTManager init: Router state loaded. Router now has {num_learnable_profiles_after_load} learnable profile(s) from state file.\")\n",
    "            else: logger.warning(f\"PEFTManager init: Specified router state file NOT FOUND at {path_to_load_router_state}. Router remains with its initial fresh parameters.\")\n",
    "        else: logger.info(\"PEFTManager init: No router state to load (load_router_state_from_tag is None). Router starts fresh.\")\n",
    "        self._scan_disk_and_update_metadata_globally()\n",
    "\n",
    "    def _scan_disk_and_update_metadata_globally(self):\n",
    "        logger.info(f\"PEFTManager: (_scan_disk_and_update_metadata_globally) Scanning disk ({self.lora_modules_base_dir}) for LoRA adapter metadata.\")\n",
    "        found_on_disk_count = 0; updated_router_metadata_count = 0\n",
    "        if not os.path.exists(self.lora_modules_base_dir): logger.info(\"PEFTManager: LoRA modules base directory not found.\"); return\n",
    "        lora_parent_dirs = [d for d in os.listdir(self.lora_modules_base_dir) if os.path.isdir(os.path.join(self.lora_modules_base_dir, d))]\n",
    "        if not lora_parent_dirs: logger.info(\"PEFTManager: No LoRA parent directories found on disk.\")\n",
    "        else:\n",
    "            for tagged_lora_id_on_disk in lora_parent_dirs:\n",
    "                lora_parent_dir_path = os.path.join(self.lora_modules_base_dir, tagged_lora_id_on_disk); metadata_file_path = os.path.join(lora_parent_dir_path, \"metadata.json\")\n",
    "                # Try to find adapter_config.json in either the parent dir or a nested dir with the same name\n",
    "                adapter_config_file_path_nested = os.path.join(lora_parent_dir_path, tagged_lora_id_on_disk, \"adapter_config.json\")\n",
    "                adapter_config_file_path_parent = os.path.join(lora_parent_dir_path, \"adapter_config.json\")\n",
    "                disk_metadata = None\n",
    "                if os.path.exists(adapter_config_file_path_nested) or os.path.exists(adapter_config_file_path_parent):\n",
    "                    if os.path.exists(metadata_file_path):\n",
    "                        try:\n",
    "                            with open(metadata_file_path, \"r\") as f: disk_metadata = json.load(f)\n",
    "                            if disk_metadata.get(\"adapter_name\") != tagged_lora_id_on_disk: disk_metadata[\"adapter_name\"] = tagged_lora_id_on_disk\n",
    "                        except Exception as e: logger.warning(f\"PEFTManager: Error reading metadata.json for {tagged_lora_id_on_disk}: {e}. Using basic.\"); disk_metadata = {\"adapter_name\": tagged_lora_id_on_disk, \"source\": \"disk_scan_metadata_read_error\"}\n",
    "                    else: logger.warning(f\"PEFTManager: Adapter {tagged_lora_id_on_disk} on disk missing metadata.json. Using basic info.\"); disk_metadata = {\"adapter_name\": tagged_lora_id_on_disk, \"source\": \"disk_scan_no_meta_json\"}\n",
    "                if disk_metadata:\n",
    "                    self.lora_modules_metadata[tagged_lora_id_on_disk] = disk_metadata; found_on_disk_count += 1\n",
    "                    # Update router's non-learnable metadata if this LoRA is already known (i.e., has a learnable profile)\n",
    "                    if tagged_lora_id_on_disk in self.router.lora_id_to_idx and \\\n",
    "                       ((hasattr(self.router, 'module_embeddings') and tagged_lora_id_on_disk in self.router.module_embeddings) or \\\n",
    "                        (hasattr(self.router, 'output_projection_layers') and tagged_lora_id_on_disk in self.router.output_projection_layers)):\n",
    "                        self.router.module_metadata[tagged_lora_id_on_disk] = disk_metadata; updated_router_metadata_count +=1\n",
    "        logger.info(f\"PEFTManager: Disk scan finished. Found/updated metadata for {found_on_disk_count} LoRAs in PEFTManager's general records.\")\n",
    "        if updated_router_metadata_count > 0: logger.info(f\"PEFTManager: Updated router's non-learnable metadata for {updated_router_metadata_count} LoRA profiles already actively managed by the router.\")\n",
    "        num_learnable_profiles_final = len(self.router.module_embeddings) if hasattr(self.router, 'module_embeddings') else len(self.router.output_projection_layers)\n",
    "        logger.info(f\"PEFTManager: After init and disk scan, router has {num_learnable_profiles_final} learnable profile(s).\")\n",
    "\n",
    "    def create_lora_module(self, tagged_lora_id: str, task_type: TaskType = TaskType.CAUSAL_LM) -> PreTrainedModel:\n",
    "        lora_config_obj = LoraConfig(r=self.config.lora_r, lora_alpha=self.config.lora_alpha, lora_dropout=self.config.lora_dropout, target_modules=self.config.target_modules, bias=\"none\", task_type=task_type )\n",
    "        if not isinstance(self.model, PeftModel): self.model = get_peft_model(self.model, lora_config_obj, adapter_name=tagged_lora_id)\n",
    "        else:\n",
    "            if tagged_lora_id not in self.model.peft_config: self.model.add_adapter(tagged_lora_id, lora_config_obj)\n",
    "        self.model.set_adapter(tagged_lora_id); self.active_lora_ids = self.model.active_adapters\n",
    "        metadata_for_lora = {\"task_type\": task_type.value if hasattr(task_type, 'value') else str(task_type), \"created_at\": time.time(), \"adapter_name\": tagged_lora_id,\n",
    "            \"original_task_id_if_available\": tagged_lora_id.split(f\"_{self.config.experiment_tag}_\")[-1].split(f\"{self.config.experiment_tag}_\")[-1] if self.config.experiment_tag and f\"{self.config.experiment_tag}_\" in tagged_lora_id else tagged_lora_id,\n",
    "            \"experiment_tag\": self.config.experiment_tag, \"config_details\": {\"r\": self.config.lora_r, \"lora_alpha\": self.config.lora_alpha, \"lora_dropout\": self.config.lora_dropout, \"target_modules\": self.config.target_modules} }\n",
    "        self.lora_modules_metadata[tagged_lora_id] = metadata_for_lora\n",
    "        self.router.add_module(tagged_lora_id, metadata_for_lora)\n",
    "        specific_lora_module_dir = os.path.join(self.lora_modules_base_dir, tagged_lora_id); os.makedirs(specific_lora_module_dir, exist_ok=True)\n",
    "        with open(os.path.join(specific_lora_module_dir, \"metadata.json\"), \"w\") as f: json.dump(metadata_for_lora, f, indent=2)\n",
    "        logger.info(f\"PEFTManager: Created LoRA '{tagged_lora_id}' and called router.add_module.\")\n",
    "        return self.model\n",
    "\n",
    "    def load_lora_module(self, tagged_lora_id_to_load: str, set_as_active: bool = True) -> Optional[PreTrainedModel]:\n",
    "        logger.info(f\"PEFTManager.load_lora_module: Attempting to load LoRA weights for '{tagged_lora_id_to_load}'.\")\n",
    "        lora_parent_dir = os.path.join(self.lora_modules_base_dir, tagged_lora_id_to_load); actual_adapter_files_dir = os.path.join(lora_parent_dir, tagged_lora_id_to_load)\n",
    "        path_to_try_loading_from = \"\"; config_found = False\n",
    "        if os.path.exists(os.path.join(actual_adapter_files_dir, \"adapter_config.json\")): path_to_try_loading_from = actual_adapter_files_dir; config_found = True\n",
    "        elif os.path.exists(os.path.join(lora_parent_dir, \"adapter_config.json\")): path_to_try_loading_from = lora_parent_dir; config_found = True\n",
    "        if not config_found: logger.warning(f\"PEFTManager.load_lora_module: Adapter config NOT found for '{tagged_lora_id_to_load}'. Cannot load weights.\"); return None\n",
    "        if not isinstance(self.model, PeftModel):\n",
    "            try: self.model = PeftModel.from_pretrained(self.model, path_to_try_loading_from, adapter_name=tagged_lora_id_to_load, is_trainable=True)\n",
    "            except Exception as e: logger.error(f\"PEFTManager.load_lora_module: Failed to load '{tagged_lora_id_to_load}' as first adapter: {e}\"); return None\n",
    "        else:\n",
    "            if tagged_lora_id_to_load not in self.model.peft_config:\n",
    "                try: self.model.load_adapter(path_to_try_loading_from, adapter_name=tagged_lora_id_to_load, is_trainable=True)\n",
    "                except Exception as e: logger.error(f\"PEFTManager.load_lora_module: Failed to load adapter weights '{tagged_lora_id_to_load}': {e}\"); return None\n",
    "            else: logger.info(f\"PEFTManager.load_lora_module: Adapter weights for '{tagged_lora_id_to_load}' already exist on PeftModel.\")\n",
    "        loaded_metadata_for_router = self.lora_modules_metadata.get(tagged_lora_id_to_load)\n",
    "        if not loaded_metadata_for_router:\n",
    "            metadata_file_path = os.path.join(lora_parent_dir, \"metadata.json\")\n",
    "            if os.path.exists(metadata_file_path):\n",
    "                with open(metadata_file_path, \"r\") as f: loaded_metadata_for_router = json.load(f)\n",
    "            else:\n",
    "                logger.warning(f\"PEFTManager.load_lora_module: metadata.json not found for {tagged_lora_id_to_load}. Reconstructing basic for router.\")\n",
    "                lora_conf_on_model = self.model.peft_config.get(tagged_lora_id_to_load)\n",
    "                if lora_conf_on_model: loaded_metadata_for_router = {\"adapter_name\": tagged_lora_id_to_load, \"source\": \"load_lora_module_reconstructed\", \"original_task_id_if_available\": tagged_lora_id_to_load.split(f\"_{self.config.experiment_tag}_\")[-1].split(f\"{self.config.experiment_tag}_\")[-1] if self.config.experiment_tag and f\"{self.config.experiment_tag}_\" in tagged_lora_id_to_load else tagged_lora_id_to_load, \"experiment_tag\": self.config.experiment_tag }\n",
    "                else: loaded_metadata_for_router = {\"adapter_name\": tagged_lora_id_to_load, \"source\": \"error_no_meta_no_conf_on_load\"}\n",
    "            self.lora_modules_metadata[tagged_lora_id_to_load] = loaded_metadata_for_router\n",
    "        if loaded_metadata_for_router: self.router.add_module(tagged_lora_id_to_load, loaded_metadata_for_router)\n",
    "        else: logger.error(f\"PEFTManager.load_lora_module: Could not obtain or reconstruct metadata for {tagged_lora_id_to_load}. Not adding to router.\")\n",
    "        if set_as_active: self.model.set_adapter(tagged_lora_id_to_load); self.active_lora_ids = self.model.active_adapters\n",
    "        logger.info(f\"PEFTManager.load_lora_module: Successfully processed (weights loaded/confirmed) LoRA adapter '{tagged_lora_id_to_load}'\")\n",
    "        return self.model\n",
    "\n",
    "    def select_lora_for_input(self, task_description: str, task_examples_for_prototype: Optional[List[Dict[str, Any]]] = None, task_text_field: Optional[Union[str, List[str]]] = None, top_k: int = 1) -> List[Tuple[str, float]]:\n",
    "        if self.router is None: logger.warning(\"PEFTManager: Router not initialized. Cannot select LoRA.\"); return []\n",
    "        # task_examples_for_prototype here is passed as task_examples_for_feature_gen to Router's select_modules\n",
    "        return self.router.select_modules(task_description=task_description, task_examples_for_feature_gen=task_examples_for_prototype, task_text_field=task_text_field, top_k=top_k)\n",
    "\n",
    "    def save_lora_module(self, tagged_lora_id_to_save: str, peft_model_instance: PeftModel):\n",
    "        if not isinstance(peft_model_instance, PeftModel) or tagged_lora_id_to_save not in peft_model_instance.peft_config: logger.error(f\"Cannot save adapter '{tagged_lora_id_to_save}'. Not a PeftModel or adapter not found in config.\"); return\n",
    "        lora_parent_dir = os.path.join(self.lora_modules_base_dir, tagged_lora_id_to_save); os.makedirs(lora_parent_dir, exist_ok=True)\n",
    "        try:\n",
    "            peft_model_instance.save_pretrained(lora_parent_dir, selected_adapters=[tagged_lora_id_to_save])\n",
    "            logger.info(f\"Saved LoRA adapter '{tagged_lora_id_to_save}' (likely into a nested subdir within {lora_parent_dir})\")\n",
    "            metadata_file_path = os.path.join(lora_parent_dir, \"metadata.json\")\n",
    "            if not os.path.exists(metadata_file_path):\n",
    "                meta_to_save = self.lora_modules_metadata.get(tagged_lora_id_to_save)\n",
    "                if not meta_to_save:\n",
    "                    lora_conf = peft_model_instance.peft_config.get(tagged_lora_id_to_save)\n",
    "                    if lora_conf: meta_to_save = {\"task_type\": str(lora_conf.task_type), \"saved_at\": time.time(), \"adapter_name\": tagged_lora_id_to_save,\n",
    "                            \"original_task_id_if_available\": tagged_lora_id_to_save.split(f\"_{self.config.experiment_tag}_\")[-1].split(f\"{self.config.experiment_tag}_\")[-1] if self.config.experiment_tag and f\"{self.config.experiment_tag}_\" in tagged_lora_id_to_save else tagged_lora_id_to_save,\n",
    "                            \"experiment_tag\": self.config.experiment_tag, \"config_details\": {\"r\":lora_conf.r, \"lora_alpha\": lora_conf.lora_alpha, \"lora_dropout\": lora_conf.lora_dropout, \"target_modules\": list(lora_conf.target_modules) if isinstance(lora_conf.target_modules, (list, tuple, set)) else str(lora_conf.target_modules) }}\n",
    "                    else: meta_to_save = {}\n",
    "                if meta_to_save:\n",
    "                    with open(metadata_file_path, \"w\") as f: json.dump(meta_to_save, f, indent=2)\n",
    "                    logger.info(f\"Saved/Reconstructed metadata.json for adapter '{tagged_lora_id_to_save}' in {lora_parent_dir}.\")\n",
    "                else: logger.warning(f\"Could not find or reconstruct metadata for adapter '{tagged_lora_id_to_save}'. metadata.json not saved.\")\n",
    "        except Exception as e: logger.error(f\"Error saving LoRA adapter '{tagged_lora_id_to_save}': {e}\")\n",
    "\n",
    "    def save_router_state(self):\n",
    "        if self.router is not None and hasattr(self.router, 'save_router_state'):\n",
    "            if self.config.experiment_tag: current_router_state_file = os.path.join(self.router_states_dir, f\"router_state_{self.config.experiment_tag}.pth\"); logger.info(f\"PEFTManager attempting to save router state for experiment '{self.config.experiment_tag}' to {current_router_state_file}\"); self.router.save_router_state(current_router_state_file)\n",
    "            else: current_router_state_file = os.path.join(self.router_states_dir, \"router_state_generic.pth\"); logger.warning(f\"Experiment tag not set. Saving router state to generic path: {current_router_state_file}\"); self.router.save_router_state(current_router_state_file)\n",
    "        else: logger.warning(\"PEFTManager: Router not available or does not support saving state.\")\n",
    "\n",
    "    def activate_lora_modules(self, lora_id_or_ids_to_activate: Union[str, List[str]]):\n",
    "        if not isinstance(self.model, PeftModel): logger.warning(\"Base model not PeftModel.\"); return\n",
    "        adapters_to_verify = [lora_id_or_ids_to_activate] if isinstance(lora_id_or_ids_to_activate, str) else lora_id_or_ids_to_activate\n",
    "        if not all(l_id in self.model.peft_config for l_id in adapters_to_verify): logger.error(f\"One or more adapters not found in PeftModel config: {adapters_to_verify}. Available: {list(self.model.peft_config.keys()) if hasattr(self.model, 'peft_config') else 'N/A'}\"); return\n",
    "        try: self.model.set_adapter(lora_id_or_ids_to_activate); self.active_lora_ids = self.model.active_adapters\n",
    "        except Exception as e: logger.error(f\"Error activating LoRA(s) {lora_id_or_ids_to_activate}: {e}\")\n",
    "\n",
    "    def get_active_lora_modules(self) -> List[str]: return self.model.active_adapters if isinstance(self.model, PeftModel) and hasattr(self.model, 'active_adapters') else []\n",
    "    def get_current_peft_model(self) -> PreTrainedModel: return self.model\n",
    "\n",
    "    def train_router(\n",
    "        self,\n",
    "        # Training data format: List of ((task_desc, Optional[List[Dict from dataset (original_example_data)]], Optional[task_text_field]), target_lora_id_str)\n",
    "        training_data: List[Tuple[Tuple[str, Optional[List[Dict[str, Any]]], Optional[Union[str, List[str]]]], str]],\n",
    "        learning_rate: Optional[float] = None\n",
    "    ):\n",
    "        if not hasattr(self.router, 'train_step'):\n",
    "            logger.warning(\"Router missing 'train_step'. Cannot train router.\")\n",
    "            return\n",
    "        if not training_data:\n",
    "            logger.info(\"No data provided for router training.\")\n",
    "            return\n",
    "\n",
    "        num_router_train_epochs = getattr(self.config, 'router_training_epochs_per_call', 1)\n",
    "        if num_router_train_epochs <= 0: num_router_train_epochs = 1\n",
    "\n",
    "        lr_to_use = learning_rate if learning_rate is not None else self.config.router_learning_rate\n",
    "\n",
    "        logger.info(f\"Starting router training: {len(training_data)} samples, {num_router_train_epochs} epochs (from config), LR: {lr_to_use:.1e}.\")\n",
    "        self.router.train_step(training_data, num_router_train_epochs, lr_to_use)\n",
    "        logger.info(\"Router training finished.\")\n",
    "\n",
    "    def update_expert_profile_in_router(\n",
    "        self,\n",
    "        lora_id_str: str,\n",
    "        task_description: str,\n",
    "        validation_examples_formatted: List[Dict[str, Any]], # List[Dict from dataset (original_example_data)]\n",
    "        task_text_field: Union[str, List[str]]\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Computes the advanced representation for a trained/adapted LoRA module (expert)\n",
    "        and updates its profile in the router.\n",
    "        \"\"\"\n",
    "        if not self.router or not hasattr(self.router, 'get_advanced_task_representation'):\n",
    "            logger.warning(\"PEFTManager: Router not available or missing get_advanced_task_representation. Cannot update expert profile.\")\n",
    "            return\n",
    "\n",
    "        # For LinearRouter, check module_embeddings. For MLPRouter, it would be output_projection_layers.\n",
    "        is_lora_in_router_learnable_params = False\n",
    "        if hasattr(self.router, 'module_embeddings') and lora_id_str in self.router.module_embeddings:\n",
    "            is_lora_in_router_learnable_params = True\n",
    "        elif hasattr(self.router, 'output_projection_layers') and lora_id_str in self.router.output_projection_layers:\n",
    "            # MLPRouter doesn't store embeddings directly, it has output layers. This update logic might differ.\n",
    "            # For now, assuming dynamic profile update applies to LinearRouter's learnable embeddings.\n",
    "            logger.warning(f\"PEFTManager: LoRA ID '{lora_id_str}' found in MLPRouter's output_projection_layers. Dynamic embedding update logic for MLPRouter needs specific implementation if profiles are not direct embeddings.\")\n",
    "            return # Or implement MLPRouter specific update logic\n",
    "\n",
    "        if not is_lora_in_router_learnable_params:\n",
    "            logger.warning(f\"PEFTManager: LoRA ID '{lora_id_str}' not found in router's learnable parameters. Cannot update its profile.\")\n",
    "            return\n",
    "\n",
    "        if not hasattr(self.config, 'use_advanced_embeddings') or not self.config.use_advanced_embeddings:\n",
    "            logger.info(\"PEFTManager: Advanced embeddings disabled. Skipping expert profile update.\")\n",
    "            return\n",
    "\n",
    "        logger.info(f\"PEFTManager: Updating expert profile in router for LoRA '{lora_id_str}' using advanced embeddings.\")\n",
    "\n",
    "        new_expert_representation_tensor = self.router.get_advanced_task_representation(\n",
    "            task_description=task_description,\n",
    "            task_examples_for_feature_gen=validation_examples_formatted, # Pass List[Dict[str,Any]]\n",
    "            task_text_field=task_text_field\n",
    "        )\n",
    "\n",
    "        if new_expert_representation_tensor is not None:\n",
    "            # This assumes LinearRouter, where module_embeddings stores the nn.Parameters\n",
    "            if hasattr(self.router, 'module_embeddings') and lora_id_str in self.router.module_embeddings:\n",
    "                with torch.no_grad():\n",
    "                    target_param = self.router.module_embeddings[lora_id_str]\n",
    "                    if new_expert_representation_tensor.shape[0] != target_param.shape[0]:\n",
    "                        logger.error(f\"PEFTManager: Dimension Mismatch! New expert repr dim {new_expert_representation_tensor.shape[0]} vs existing router profile dim {target_param.shape[0]} for '{lora_id_str}'. NOT UPDATING.\")\n",
    "                        return\n",
    "                    target_param.data = new_expert_representation_tensor.to(\n",
    "                        device=target_param.device, dtype=target_param.dtype\n",
    "                    )\n",
    "                logger.info(f\"PEFTManager: Successfully updated profile for LoRA '{lora_id_str}' in router with new advanced representation.\")\n",
    "            else:\n",
    "                logger.error(f\"PEFTManager: Attempted to update profile for '{lora_id_str}', but it's not in LinearRouter's module_embeddings or router type is not LinearRouter with module_embeddings.\")\n",
    "        else:\n",
    "            logger.error(f\"PEFTManager: Failed to generate advanced representation for LoRA '{lora_id_str}'. Profile not updated.\")\n",
    "\n",
    "\n",
    "class Router(nn.Module):\n",
    "    def __init__(self, config: Any, model_for_embeddings: PreTrainedModel):\n",
    "        super().__init__(); self.config = config; self.model_for_embeddings = model_for_embeddings\n",
    "        self.device = torch.device(config.device); self.module_metadata: Dict[str, Dict[str, Any]] = {}\n",
    "        self.lora_id_to_idx: Dict[str, int] = {}; self.idx_to_lora_id: Dict[int, str] = {}; self.next_lora_idx = 0\n",
    "        self.optimizer = None; model_name_for_tokenizer = self.config.model_name\n",
    "        custom_model_path = os.environ.get(\"CUSTOM_MODEL_PATH\")\n",
    "        if custom_model_path and os.path.exists(custom_model_path): model_name_for_tokenizer = custom_model_path\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name_for_tokenizer)\n",
    "        if self.tokenizer.pad_token is None:\n",
    "            if self.tokenizer.eos_token is not None: self.tokenizer.pad_token = self.tokenizer.eos_token\n",
    "            elif self.tokenizer.unk_token is not None: self.tokenizer.pad_token = self.tokenizer.unk_token\n",
    "            else: logger.warning(f\"Tokenizer for {model_name_for_tokenizer} has no pad_token, eos_token, or unk_token. Adding new pad_token '<|pad|>'.\"); self.tokenizer.add_special_tokens({'pad_token': '<|pad|>'})\n",
    "        self.router_internal_dtype = torch.float32\n",
    "        try: self.embedding_model_dtype = next(self.model_for_embeddings.parameters()).dtype\n",
    "        except StopIteration: self.embedding_model_dtype = torch.float32\n",
    "        logger.info(f\"Router common init: Embedding model dtype: {self.embedding_model_dtype}, Router internal param dtype: {self.router_internal_dtype}\")\n",
    "\n",
    "    def add_module(self, module_id_str: str, metadata: Dict[str, Any]):\n",
    "        self.module_metadata[module_id_str] = metadata\n",
    "        if module_id_str not in self.lora_id_to_idx:\n",
    "            self.lora_id_to_idx[module_id_str] = self.next_lora_idx; self.idx_to_lora_id[self.next_lora_idx] = module_id_str\n",
    "            self.next_lora_idx += 1\n",
    "        self._on_module_added(module_id_str)\n",
    "\n",
    "    def _on_module_added(self, module_id_str: str): pass\n",
    "\n",
    "    def get_advanced_task_representation(\n",
    "        self,\n",
    "        task_description: str,\n",
    "        task_examples_for_feature_gen: Optional[List[Dict[str, Any]]] = None, # List of original_example_data dicts\n",
    "        task_text_field: Optional[Union[str, List[str]]] = None\n",
    "    ) -> Optional[torch.Tensor]:\n",
    "        if not hasattr(self.config, 'use_advanced_embeddings') or not self.config.use_advanced_embeddings:\n",
    "            logger.info(\"Router: Advanced embeddings disabled by config. Falling back to simpler representation (desc only for now).\")\n",
    "            # Fallback: Just task description embedding\n",
    "            inputs_desc = self.tokenizer(task_description, return_tensors=\"pt\", truncation=True, max_length=self.config.max_seq_length, padding=\"max_length\").to(self.device)\n",
    "            original_training_state_fb = self.model_for_embeddings.training\n",
    "            if hasattr(self.model_for_embeddings, 'eval'): self.model_for_embeddings.eval()\n",
    "            fb_task_description_embedding: Optional[torch.Tensor] = None\n",
    "            with torch.no_grad():\n",
    "                try:\n",
    "                    outputs_desc = self.model_for_embeddings(**inputs_desc, output_hidden_states=True); last_hidden_state_desc = outputs_desc.hidden_states[-1]\n",
    "                    attention_mask_desc_expanded = inputs_desc['attention_mask'].unsqueeze(-1).expand(last_hidden_state_desc.size()).float()\n",
    "                    sum_embeddings_desc = torch.sum(last_hidden_state_desc * attention_mask_desc_expanded, 1); sum_mask_desc = torch.clamp(attention_mask_desc_expanded.sum(1), min=1e-9)\n",
    "                    fb_task_description_embedding = (sum_embeddings_desc / sum_mask_desc)[0].to(self.embedding_model_dtype)\n",
    "                except Exception as e: logger.error(f\"Router (Fallback): Error getting task desc embedding: {e}\")\n",
    "            if original_training_state_fb and hasattr(self.model_for_embeddings, 'train'): self.model_for_embeddings.train()\n",
    "            return fb_task_description_embedding.squeeze().to(self.router_internal_dtype) if fb_task_description_embedding is not None else None\n",
    "\n",
    "        all_feature_components = []\n",
    "        original_training_state = self.model_for_embeddings.training # Save initial model state\n",
    "\n",
    "        # --- Signal 1: Text Meta-Features (Task Description Embedding) ---\n",
    "        if hasattr(self.model_for_embeddings, 'eval'): self.model_for_embeddings.eval() # Eval for desc embedding\n",
    "        task_description_embedding: Optional[torch.Tensor] = None\n",
    "        try:\n",
    "            inputs_desc = self.tokenizer(task_description, return_tensors=\"pt\", truncation=True, max_length=self.config.max_seq_length, padding=\"max_length\").to(self.device)\n",
    "            with torch.no_grad():\n",
    "                outputs_desc = self.model_for_embeddings(**inputs_desc, output_hidden_states=True)\n",
    "                last_hidden_state_desc = outputs_desc.hidden_states[-1]\n",
    "                attention_mask_desc_expanded = inputs_desc['attention_mask'].unsqueeze(-1).expand(last_hidden_state_desc.size()).float()\n",
    "                sum_embeddings_desc = torch.sum(last_hidden_state_desc * attention_mask_desc_expanded, 1)\n",
    "                sum_mask_desc = torch.clamp(attention_mask_desc_expanded.sum(1), min=1e-9)\n",
    "                task_description_embedding = (sum_embeddings_desc / sum_mask_desc)[0].to(self.router_internal_dtype)\n",
    "                all_feature_components.append(task_description_embedding)\n",
    "            logger.info(f\"Router: Generated task_description_embedding, shape {task_description_embedding.shape}\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Router: Error getting task description embedding (Signal 1): {e}\")\n",
    "            # No return yet, other signals might still work. If all fail, then return None.\n",
    "\n",
    "        # --- Prepare examples for Gradient Sketch and Context Stats ---\n",
    "        # These examples need to be formatted as List[Dict{'input':str, 'output':str}] for CausalLMTrainingDataset in grad_sketch\n",
    "        # and List[Dict{'input':str}] for context_stats.\n",
    "        # task_examples_for_feature_gen is List[Dict from dataset (original_example_data)]\n",
    "        \n",
    "        formatted_examples_for_grad_sketch: List[Dict[str,str]] = []\n",
    "        formatted_examples_for_context_stats: List[Dict[str,str]] = [] # Context stats typically only needs 'input'\n",
    "\n",
    "        if task_examples_for_feature_gen and task_text_field:\n",
    "            for ex_orig_data in task_examples_for_feature_gen: # ex_orig_data is a dict like {'sentence': ..., 'label': ...}\n",
    "                input_val_str, output_val_str = None, None\n",
    "                \n",
    "                # Construct input_val_str\n",
    "                if isinstance(task_text_field, list): # sentence-pair\n",
    "                    input_parts = [str(ex_orig_data.get(tf, \"\")) for tf in task_text_field]\n",
    "                    input_val_str = \" [SEP] \".join(input_parts)\n",
    "                else: # single sentence\n",
    "                    input_val_str = str(ex_orig_data.get(task_text_field, \"\"))\n",
    "\n",
    "                # Construct output_val_str (needed for grad sketch's loss)\n",
    "                # This requires knowing the label field and potentially a class_names_map.\n",
    "                # This part is tricky as the Router itself doesn't usually know these.\n",
    "                # For now, let's assume if 'output' key is directly in ex_orig_data, use it.\n",
    "                # Otherwise, this part needs refinement in how examples are passed or processed.\n",
    "                # A simpler approach for the MVE: Assume 'output' is already prepared in `task_examples_for_feature_gen`\n",
    "                # if those dicts are already {'input': ..., 'output': ...}.\n",
    "                # If `task_examples_for_feature_gen` contains dicts like `ex['original_example_data']` from `main`,\n",
    "                # then `ex_orig_data` here IS that `original_example_data`.\n",
    "                # The `main` function needs to pass `class_names_map` if labels need mapping here.\n",
    "                # Let's assume the `main` function provides 'input' and 'output' (textual) in `task_examples_for_feature_gen`\n",
    "                # if they are structured as List[Dict[str,str]]. If they are List[Dict[str,Any]], we parse here.\n",
    "                \n",
    "                # Simplification: If `task_examples_for_feature_gen` comes from `main` as `List[ex['original_example_data']]`,\n",
    "                # then `ex_orig_data` IS `ex['original_example_data']`. The `main` function should pre-format\n",
    "                # these into `{'input': ..., 'output': ...}` text lists before passing to router.\n",
    "                # OR, we try to handle it here if it's a common structure.\n",
    "                # The `generate_gradient_sketch` uses `CausalLMTrainingDataset` which expects `{'input': ..., 'output': ...}`.\n",
    "\n",
    "                if 'input' in ex_orig_data and 'output' in ex_orig_data: # If already formatted\n",
    "                    formatted_examples_for_grad_sketch.append({'input': str(ex_orig_data['input']), 'output': str(ex_orig_data['output'])})\n",
    "                    formatted_examples_for_context_stats.append({'input': str(ex_orig_data['input'])})\n",
    "                elif input_val_str: # If only input can be constructed\n",
    "                     formatted_examples_for_context_stats.append({'input': input_val_str})\n",
    "                     # For grad sketch, if 'output' is missing, it might not work.\n",
    "                     # This indicates that `task_examples_for_feature_gen` should ideally be pre-formatted by the caller.\n",
    "                     # For robustness, grad_sketch will check `examples`.\n",
    "\n",
    "        # --- Signal 2: Gradient Sketch ---\n",
    "        if formatted_examples_for_grad_sketch and hasattr(self.config, 'k_for_grad_sketch') and self.config.k_for_grad_sketch > 0:\n",
    "            # generate_gradient_sketch expects `model` to be the frozen base model.\n",
    "            # It internally handles requires_grad for target layers.\n",
    "            gradient_sketch = generate_gradient_sketch(\n",
    "                examples=formatted_examples_for_grad_sketch,\n",
    "                model=self.model_for_embeddings,\n",
    "                tokenizer=self.tokenizer,\n",
    "                config=self.config,\n",
    "                device=self.device\n",
    "            )\n",
    "            if gradient_sketch is not None:\n",
    "                all_feature_components.append(gradient_sketch.to(self.router_internal_dtype))\n",
    "                logger.info(f\"Router: Generated gradient_sketch, shape {gradient_sketch.shape}\")\n",
    "        else:\n",
    "            logger.info(\"Router: Not enough/no formatted examples for gradient sketch. Skipping.\")\n",
    "\n",
    "        # --- Signal 3: Context Stats ---\n",
    "        # model_for_embeddings should be in eval mode for context_stats's entropy calc\n",
    "        if hasattr(self.model_for_embeddings, 'eval'): self.model_for_embeddings.eval()\n",
    "        if formatted_examples_for_context_stats:\n",
    "            context_stats = generate_context_stats(\n",
    "                examples=formatted_examples_for_context_stats,\n",
    "                tokenizer=self.tokenizer,\n",
    "                model=self.model_for_embeddings,\n",
    "                config=self.config,\n",
    "                device=self.device\n",
    "            )\n",
    "            if context_stats is not None:\n",
    "                all_feature_components.append(context_stats.to(self.router_internal_dtype))\n",
    "                logger.info(f\"Router: Generated context_stats, shape {context_stats.shape}\")\n",
    "        else:\n",
    "            logger.info(\"Router: No formatted examples for context stats. Skipping.\")\n",
    "\n",
    "        # --- Finalize and Restore Model State ---\n",
    "        if original_training_state and hasattr(self.model_for_embeddings, 'train'): # Restore original model mode\n",
    "            self.model_for_embeddings.train()\n",
    "        elif not original_training_state and hasattr(self.model_for_embeddings, 'eval'):\n",
    "             self.model_for_embeddings.eval()\n",
    "\n",
    "\n",
    "        if not all_feature_components:\n",
    "            logger.error(\"Router: All feature components for advanced representation failed to generate or were skipped.\")\n",
    "            return None\n",
    "\n",
    "        try:\n",
    "            final_representation = torch.cat(all_feature_components, dim=-1)\n",
    "            final_representation = F.normalize(final_representation, p=2, dim=-1) # L2 Normalize\n",
    "            logger.info(f\"Router: Generated final_advanced_representation, shape {final_representation.shape}\")\n",
    "            return final_representation.to(self.router_internal_dtype)\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Router: Error concatenating or normalizing advanced features: {e}\")\n",
    "            return None\n",
    "\n",
    "    def select_modules(self, task_description: str, task_examples_for_feature_gen: Optional[List[Dict[str, Any]]] = None, task_text_field: Optional[Union[str, List[str]]] = None, top_k: int = 1) -> List[Tuple[str, float]]:\n",
    "        # This method will be implemented by subclasses like LinearRouter, MLPRouter\n",
    "        # They will call self.get_advanced_task_representation\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def train_step(self, training_data: List[Tuple[Tuple[str, Optional[List[Dict[str, Any]]], Optional[Union[str, List[str]]]], str]], epochs: int, learning_rate: float ):\n",
    "        # This method will be implemented by subclasses\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def save_router_state(self, path: str): raise NotImplementedError(\"Subclasses must implement save_router_state\")\n",
    "    def load_router_state(self, path: str): raise NotImplementedError(\"Subclasses must implement load_router_state\")\n",
    "\n",
    "\n",
    "class LinearRouter(Router):\n",
    "    def __init__(self, config: Any, model_for_embeddings: PreTrainedModel):\n",
    "        super().__init__(config, model_for_embeddings)\n",
    "\n",
    "        current_embed_dim = 0\n",
    "        if not hasattr(self.config, 'use_advanced_embeddings') or not self.config.use_advanced_embeddings:\n",
    "            base_embed_dim = self.model_for_embeddings.config.hidden_size\n",
    "            if hasattr(self.config, 'k_examples_for_prototype') and self.config.k_examples_for_prototype > 0:\n",
    "                current_embed_dim = base_embed_dim * 2\n",
    "                logger.info(f\"LinearRouter initialized (ADV. EMBEDDINGS OFF) for RICH features (desc+prototype). Concatenated embed_dim: {current_embed_dim}.\")\n",
    "            else:\n",
    "                current_embed_dim = base_embed_dim\n",
    "                logger.info(f\"LinearRouter initialized (ADV. EMBEDDINGS OFF) for DESCRIPTION features ONLY. Embed_dim: {current_embed_dim}.\")\n",
    "        else: # Calculate new embed_dim for advanced embeddings\n",
    "            dim_desc_embed = self.model_for_embeddings.config.hidden_size\n",
    "            current_embed_dim += dim_desc_embed\n",
    "            dim_grad_sketch = 0\n",
    "            if hasattr(self.config, 'k_for_grad_sketch') and self.config.k_for_grad_sketch > 0 and \\\n",
    "               hasattr(self.config, 'grad_sketch_layer_names') and self.config.grad_sketch_layer_names and \\\n",
    "               hasattr(self.config, 'grad_sketch_max_elements') and self.config.grad_sketch_max_elements > 0:\n",
    "                dim_grad_sketch = self.config.grad_sketch_max_elements\n",
    "                current_embed_dim += dim_grad_sketch\n",
    "            dim_context_stats = 0\n",
    "            if hasattr(self.config, 'use_context_stats_avg_seq_length') and self.config.use_context_stats_avg_seq_length:\n",
    "                dim_context_stats += 1\n",
    "            if hasattr(self.config, 'use_context_stats_avg_token_entropy') and self.config.use_context_stats_avg_token_entropy:\n",
    "                dim_context_stats += 1\n",
    "            current_embed_dim += dim_context_stats\n",
    "            logger.info(f\"LinearRouter initialized for ADVANCED EMBEDDINGS. Desc Dim: {dim_desc_embed}, GradSketch Dim: {dim_grad_sketch}, ContextStats Dim: {dim_context_stats}. Total Embed_dim: {current_embed_dim}.\")\n",
    "\n",
    "        self.embed_dim = current_embed_dim\n",
    "        if self.embed_dim == 0:\n",
    "            logger.error(\"LinearRouter: Calculated embed_dim is 0. Defaulting to base model hidden_size. Check config.\")\n",
    "            self.embed_dim = self.model_for_embeddings.config.hidden_size\n",
    "        self.module_embeddings = nn.ParameterDict()\n",
    "\n",
    "    def _setup_optimizer(self, learning_rate: float):\n",
    "        params_to_optimize = [p for p_name, p in self.module_embeddings.items() if isinstance(p, nn.Parameter)]\n",
    "        if params_to_optimize: self.optimizer = optim.AdamW(params_to_optimize, lr=learning_rate, weight_decay=self.config.weight_decay); logger.info(f\"LinearRouter optimizer (re)set. LR: {learning_rate:.1e} with {len(params_to_optimize)} embedding parameters.\")\n",
    "        else: self.optimizer = None; logger.info(f\"LinearRouter: No parameters in module_embeddings to optimize yet.\")\n",
    "\n",
    "    def _on_module_added(self, module_id_str: str):\n",
    "        was_router_state_loaded_this_peft_manager_session = bool(self.config.load_router_state_from_tag)\n",
    "        is_part_of_current_experiment_run = self.config.experiment_tag and self.config.experiment_tag in module_id_str\n",
    "\n",
    "        if module_id_str not in self.module_embeddings:\n",
    "            create_new_learnable_param = False\n",
    "            if not was_router_state_loaded_this_peft_manager_session: # Fresh router context\n",
    "                if is_part_of_current_experiment_run: create_new_learnable_param = True\n",
    "                else: logger.info(f\"LinearRouter._on_module_added (Fresh Router context): Module '{module_id_str}' (likely from disk scan, not tagged for current exp '{self.config.experiment_tag}') will be known by router but NO new learnable parameter created now.\")\n",
    "            else: # Loaded router context\n",
    "                if is_part_of_current_experiment_run: create_new_learnable_param = True # New LoRA from current experiment\n",
    "                else: logger.info(f\"LinearRouter._on_module_added (Loaded Router context): Module '{module_id_str}' (from disk scan, not current exp '{self.config.experiment_tag}', not in loaded state's learnable params) - NO new learnable parameter created now.\")\n",
    "\n",
    "            if create_new_learnable_param:\n",
    "                # Initialize with random data. Will be overwritten by update_expert_profile_in_router if advanced embeddings are on.\n",
    "                new_embedding = nn.Parameter(torch.randn(self.embed_dim, device=self.device, dtype=self.router_internal_dtype))\n",
    "                self.module_embeddings[module_id_str] = new_embedding\n",
    "                logger.info(f\"LinearRouter._on_module_added: CREATED new learnable embedding for '{module_id_str}' with dim {self.embed_dim}.\")\n",
    "                if self.optimizer and hasattr(self.optimizer, 'add_param_group') and isinstance(self.module_embeddings[module_id_str], nn.Parameter):\n",
    "                     try: self.optimizer.add_param_group({'params': [self.module_embeddings[module_id_str]]}); logger.info(f\"Added '{module_id_str}' embedding to existing LinearRouter optimizer.\")\n",
    "                     except Exception as e: logger.warning(f\"Could not add '{module_id_str}' to optimizer: {e}. Will be picked up by _setup_optimizer.\"); self._setup_optimizer(self.config.router_learning_rate)\n",
    "                elif not self.optimizer : self._setup_optimizer(self.config.router_learning_rate)\n",
    "        else: # Module ID already in self.module_embeddings\n",
    "            current_param = self.module_embeddings[module_id_str]\n",
    "            if not isinstance(current_param, nn.Parameter):\n",
    "                 logger.error(f\"LinearRouter._on_module_added: {module_id_str} in module_embeddings but is not nn.Parameter! Type: {type(current_param)}. Attempting to re-init as Parameter.\")\n",
    "                 self.module_embeddings[module_id_str] = nn.Parameter(current_param.data.to(self.device).to(self.router_internal_dtype)); current_param = self.module_embeddings[module_id_str]\n",
    "            if current_param.shape[0] != self.embed_dim:\n",
    "                logger.warning(f\"LinearRouter._on_module_added: Existing learnable embedding '{module_id_str}' has dim {current_param.shape[0]}, but router currently expects {self.embed_dim}. This can happen if config changed. Parameter will be reinitialized if profile is updated dynamically or router is retrained with this module as target.\")\n",
    "\n",
    "    def _get_all_module_embeddings_tensor_and_map(self) -> Tuple[Optional[torch.Tensor], List[str]]:\n",
    "        if not self.module_embeddings or len(self.module_embeddings) == 0: return None, []\n",
    "        valid_lora_ids = [lora_id for lora_id in self.lora_id_to_idx.keys() if lora_id in self.module_embeddings and isinstance(self.module_embeddings[lora_id], nn.Parameter) and self.module_embeddings[lora_id].shape[0] == self.embed_dim]\n",
    "        if not valid_lora_ids: return None, []\n",
    "        sorted_lora_ids = sorted(valid_lora_ids, key=lambda k: self.lora_id_to_idx[k])\n",
    "        embeddings_list = [self.module_embeddings[lora_id] for lora_id in sorted_lora_ids]\n",
    "        if not embeddings_list: return None, []\n",
    "        return torch.stack(embeddings_list), sorted_lora_ids\n",
    "\n",
    "    def select_modules(self, task_description: str, task_examples_for_feature_gen: Optional[List[Dict[str, Any]]] = None, task_text_field: Optional[Union[str, List[str]]] = None, top_k: int = 1) -> List[Tuple[str, float]]:\n",
    "        self.eval();\n",
    "        if not self.module_embeddings or len(self.module_embeddings) == 0 : logger.info(\"LinearRouter.select_modules: No module embeddings available.\"); return []\n",
    "        # task_examples_for_feature_gen is List[Dict from dataset (original_example_data)]\n",
    "        advanced_task_repr = self.get_advanced_task_representation(task_description, task_examples_for_feature_gen, task_text_field)\n",
    "        if advanced_task_repr is None: logger.warning(\"LinearRouter.select_modules: Failed to get advanced task representation.\"); return []\n",
    "        all_embeddings, ordered_ids = self._get_all_module_embeddings_tensor_and_map()\n",
    "        if all_embeddings is None or len(ordered_ids) == 0: logger.info(\"LinearRouter.select_modules: No module embeddings tensor available for comparison.\"); return []\n",
    "        if advanced_task_repr.shape[0] != all_embeddings.shape[1]:\n",
    "            logger.error(f\"LinearRouter.select_modules: Dimension mismatch! Task Repr dim {advanced_task_repr.shape[0]}, Module Embeds dim {all_embeddings.shape[1]}. Skipping selection.\")\n",
    "            return []\n",
    "        if advanced_task_repr.dtype != all_embeddings.dtype: logger.warning(f\"LinearRouter.select_modules: Dtype mismatch! Task Repr: {advanced_task_repr.dtype}, Module Embeds: {all_embeddings.dtype}. Casting task_repr.\"); advanced_task_repr = advanced_task_repr.to(all_embeddings.dtype)\n",
    "        sim = F.cosine_similarity(advanced_task_repr.unsqueeze(0), all_embeddings); probs = F.softmax(sim, dim=0)\n",
    "        top_probs, top_indices = torch.topk(probs, min(top_k, len(probs)))\n",
    "        return [(ordered_ids[idx.item()], prob.item()) for idx, prob in zip(top_indices, top_probs)]\n",
    "\n",
    "    def train_step(self, training_data: List[Tuple[Tuple[str, Optional[List[Dict[str, Any]]], Optional[Union[str, List[str]]]], str]], epochs: int, learning_rate: float):\n",
    "        # training_data tuple: ((task_desc, task_examples_for_feature_gen, task_txt_fld), target_id_str)\n",
    "        # task_examples_for_feature_gen is List[Dict from dataset (original_example_data)]\n",
    "        if not self.module_embeddings or len(self.module_embeddings) == 0: logger.warning(\"LinearRouter: No module embeddings to train.\"); return\n",
    "        if self.optimizer is None: self._setup_optimizer(learning_rate);\n",
    "        if self.optimizer is None: logger.warning(\"LinearRouter: Optimizer could not be set up. Cannot train.\"); return\n",
    "        self.train(); criterion = nn.CrossEntropyLoss()\n",
    "        for epoch in range(epochs):\n",
    "            epoch_loss = 0.0; num_batches = 0\n",
    "            for (task_desc, examples_for_features, task_txt_fld), target_id_str in tqdm(training_data, desc=f\"Router Lin Ep{epoch+1}\", leave=False):\n",
    "                if target_id_str not in self.module_embeddings or target_id_str not in self.lora_id_to_idx: logger.warning(f\"Target ID '{target_id_str}' for router training not found in module_embeddings or lora_id_to_idx. Skipping.\"); continue\n",
    "                self.optimizer.zero_grad()\n",
    "                advanced_task_repr = self.get_advanced_task_representation(task_desc, examples_for_features, task_txt_fld )\n",
    "                if advanced_task_repr is None: logger.warning(f\"Router training: Failed to get advanced task representation for desc '{task_desc[:30]}...'. Skipping batch.\"); continue\n",
    "                all_embeds, ordered_ids_stack = self._get_all_module_embeddings_tensor_and_map()\n",
    "                if all_embeds is None or len(ordered_ids_stack) == 0: logger.warning(\"Router training: No module embeddings available to compare against. Skipping batch.\"); continue\n",
    "\n",
    "                if advanced_task_repr.shape[0] != all_embeds.shape[1]:\n",
    "                    logger.error(f\"LinearRouter.train_step: Dimension mismatch! Task Repr dim {advanced_task_repr.shape[0]}, Module Embeds dim {all_embeds.shape[1]}. Skipping batch.\")\n",
    "                    continue\n",
    "                if advanced_task_repr.dtype != all_embeds.dtype: advanced_task_repr = advanced_task_repr.to(all_embeds.dtype)\n",
    "\n",
    "                logits = F.cosine_similarity(advanced_task_repr.unsqueeze(0), all_embeds).unsqueeze(0)\n",
    "                try: target_idx = ordered_ids_stack.index(target_id_str)\n",
    "                except ValueError: logger.error(f\"Target '{target_id_str}' not in ordered_ids_stack derived from module_embeddings ({ordered_ids_stack}). LinRouter loss calc error.\"); continue\n",
    "                target_indices = torch.tensor([target_idx], device=self.device); loss = criterion(logits, target_indices)\n",
    "                if torch.isnan(loss).any() or torch.isinf(loss).any(): logger.error(f\"LinearRouter TRAIN Ep{epoch+1}: Loss NaN/Inf! Skipping step.\"); continue\n",
    "                loss.backward(); self.optimizer.step(); epoch_loss += loss.item(); num_batches +=1\n",
    "            if num_batches > 0: logger.info(f\"LinearRouter Ep {epoch+1} Avg Loss: {epoch_loss/num_batches:.4f}\")\n",
    "\n",
    "    def save_router_state(self, path: str):\n",
    "        module_embeddings_to_save = {}\n",
    "        num_params_being_saved = 0\n",
    "        if self.module_embeddings:\n",
    "            for k, v_param in self.module_embeddings.items():\n",
    "                if isinstance(v_param, nn.Parameter):\n",
    "                    module_embeddings_to_save[k] = v_param.cpu().data\n",
    "                    num_params_being_saved += 1\n",
    "        lora_id_to_idx_to_save = {k: v for k, v in self.lora_id_to_idx.items() if k in module_embeddings_to_save}\n",
    "        idx_to_lora_id_to_save = {v: k for k, v in lora_id_to_idx_to_save.items()}\n",
    "        module_metadata_to_save = {k: v for k, v in self.module_metadata.items() if k in module_embeddings_to_save}\n",
    "        state = {\n",
    "            'module_embeddings_state_dict': module_embeddings_to_save,\n",
    "            'optimizer_state_dict': self.optimizer.state_dict() if self.optimizer and num_params_being_saved > 0 else None,\n",
    "            'lora_id_to_idx': lora_id_to_idx_to_save,\n",
    "            'idx_to_lora_id': idx_to_lora_id_to_save,\n",
    "            'next_lora_idx': self.next_lora_idx,\n",
    "            'module_metadata': module_metadata_to_save,\n",
    "            'embed_dim': self.embed_dim\n",
    "        }\n",
    "        torch.save(state, path)\n",
    "        logger.info(f\"LinearRouter state saved to {path}. Saved embed_dim: {self.embed_dim}. Num profiles with learnable embeddings saved: {num_params_being_saved}. lora_id_to_idx entries saved: {len(lora_id_to_idx_to_save)}\")\n",
    "\n",
    "    def load_router_state(self, path: str):\n",
    "        if not os.path.exists(path): logger.warning(f\"LinearRouter: No state file found at {path}. Router not loaded.\"); return\n",
    "        state = torch.load(path, map_location='cpu')\n",
    "        logger.info(f\"LinearRouter.load_router_state: Loading from {path}. Saved state keys: {list(state.keys())}\")\n",
    "        saved_embed_dim_from_file = state.get('embed_dim')\n",
    "\n",
    "        # Determine current expected embed_dim based on config (for comparison)\n",
    "        current_config_expected_embed_dim = 0\n",
    "        if not hasattr(self.config, 'use_advanced_embeddings') or not self.config.use_advanced_embeddings:\n",
    "            base_dim = self.model_for_embeddings.config.hidden_size\n",
    "            current_config_expected_embed_dim = base_dim * 2 if (hasattr(self.config, 'k_examples_for_prototype') and self.config.k_examples_for_prototype > 0) else base_dim\n",
    "        else:\n",
    "            current_config_expected_embed_dim += self.model_for_embeddings.config.hidden_size\n",
    "            if hasattr(self.config, 'k_for_grad_sketch') and self.config.k_for_grad_sketch > 0 and hasattr(self.config, 'grad_sketch_max_elements'):\n",
    "                current_config_expected_embed_dim += self.config.grad_sketch_max_elements\n",
    "            if hasattr(self.config, 'use_context_stats_avg_seq_length') and self.config.use_context_stats_avg_seq_length: current_config_expected_embed_dim +=1\n",
    "            if hasattr(self.config, 'use_context_stats_avg_token_entropy') and self.config.use_context_stats_avg_token_entropy: current_config_expected_embed_dim +=1\n",
    "        \n",
    "        if saved_embed_dim_from_file is None:\n",
    "            first_embed_param_tensor = next(iter(state.get('module_embeddings_state_dict', {}).values()), None)\n",
    "            if first_embed_param_tensor is not None: saved_embed_dim_from_file = first_embed_param_tensor.shape[0]; logger.info(f\"LinearRouter.load_router_state: 'embed_dim' not found in state. Inferred as {saved_embed_dim_from_file} from loaded parameters.\")\n",
    "            else: saved_embed_dim_from_file = current_config_expected_embed_dim; logger.info(f\"LinearRouter.load_router_state: 'embed_dim' not found and no embeddings in state. Using current config-derived embed_dim: {saved_embed_dim_from_file}.\")\n",
    "        \n",
    "        self.embed_dim = saved_embed_dim_from_file # Set router's embed_dim to what was saved\n",
    "        logger.info(f\"LinearRouter.load_router_state: Router internal embed_dim set to {self.embed_dim} (from loaded state).\")\n",
    "        if self.embed_dim != current_config_expected_embed_dim:\n",
    "             logger.warning(f\"LinearRouter.load_router_state: Loaded state embed_dim ({self.embed_dim}) MISMATCHES current config expected ({current_config_expected_embed_dim}). Ensure config (e.g., use_advanced_embeddings, k_examples_for_prototype) is consistent with the loaded state.\")\n",
    "\n",
    "        self.module_embeddings.clear()\n",
    "        loaded_module_embeddings_state_dict = state.get('module_embeddings_state_dict', {})\n",
    "        logger.info(f\"LinearRouter.load_router_state: Found {len(loaded_module_embeddings_state_dict)} profiles in 'module_embeddings_state_dict' from file.\")\n",
    "\n",
    "        for module_id, param_tensor_cpu in loaded_module_embeddings_state_dict.items():\n",
    "            if param_tensor_cpu.shape[0] != self.embed_dim:\n",
    "                logger.error(f\"LinearRouter.load_router_state: Dimension mismatch for module '{module_id}'. Saved tensor dim: {param_tensor_cpu.shape[0]}, Router's loaded embed_dim: {self.embed_dim}. SKIPPING this parameter.\")\n",
    "                continue\n",
    "            self.module_embeddings[module_id] = nn.Parameter(param_tensor_cpu.to(self.device).to(self.router_internal_dtype))\n",
    "\n",
    "        self.lora_id_to_idx.clear(); self.idx_to_lora_id.clear(); self.module_metadata.clear()\n",
    "        loaded_lora_id_to_idx_from_file = state.get('lora_id_to_idx', {})\n",
    "        loaded_module_metadata_from_file = state.get('module_metadata', {})\n",
    "\n",
    "        for module_id in self.module_embeddings.keys():\n",
    "            if module_id in loaded_lora_id_to_idx_from_file:\n",
    "                self.lora_id_to_idx[module_id] = loaded_lora_id_to_idx_from_file[module_id]\n",
    "                self.idx_to_lora_id[loaded_lora_id_to_idx_from_file[module_id]] = module_id\n",
    "            else: self.lora_id_to_idx[module_id] = self.next_lora_idx; self.idx_to_lora_id[self.next_lora_idx] = module_id; self.next_lora_idx +=1; logger.warning(f\"LoRA ID {module_id} from loaded embeddings was not in loaded lora_id_to_idx. Added it.\")\n",
    "            if module_id in loaded_module_metadata_from_file: self.module_metadata[module_id] = loaded_module_metadata_from_file[module_id]\n",
    "            else: self.module_metadata[module_id] = {\"adapter_name\": module_id, \"source\": \"reconstructed_on_load_missing_meta\"}\n",
    "            # self._on_module_added(module_id) # Not strictly needed here as params are directly set, but harmless.\n",
    "\n",
    "        self.next_lora_idx = state.get('next_lora_idx', self.next_lora_idx)\n",
    "        if len(list(self.module_embeddings.parameters())) > 0:\n",
    "            self._setup_optimizer(self.config.router_learning_rate)\n",
    "            if self.optimizer and state.get('optimizer_state_dict'):\n",
    "                try:\n",
    "                    self.optimizer.load_state_dict(state['optimizer_state_dict'])\n",
    "                    for opt_state_group in self.optimizer.state.values():\n",
    "                        for k_opt, v_tensor_opt in opt_state_group.items():\n",
    "                            if isinstance(v_tensor_opt, torch.Tensor): opt_state_group[k_opt] = v_tensor_opt.to(self.device)\n",
    "                    logger.info(\"LinearRouter optimizer state loaded and moved to device.\")\n",
    "                except Exception as e: logger.error(f\"LinearRouter: Failed to load optimizer state: {e}. Optimizer might be reset.\"); self._setup_optimizer(self.config.router_learning_rate)\n",
    "        else: self.optimizer = None\n",
    "        num_actually_loaded_embeddings = len(self.module_embeddings)\n",
    "        logger.info(f\"LinearRouter state loaded from {path}. Router now has {num_actually_loaded_embeddings} module profile(s) with learnable embeddings. Router embed_dim is {self.embed_dim}.\")\n",
    "\n",
    "\n",
    "class MLPRouter(Router): # (Updated input_dim calculation, rest largely similar to previous state)\n",
    "    def __init__(self, config: Any, model_for_embeddings: PreTrainedModel):\n",
    "        super().__init__(config, model_for_embeddings)\n",
    "\n",
    "        current_input_dim_for_mlp = 0\n",
    "        if not hasattr(self.config, 'use_advanced_embeddings') or not self.config.use_advanced_embeddings:\n",
    "            base_embed_dim = self.model_for_embeddings.config.hidden_size\n",
    "            if hasattr(self.config, 'k_examples_for_prototype') and self.config.k_examples_for_prototype > 0:\n",
    "                current_input_dim_for_mlp = base_embed_dim * 2\n",
    "            else: current_input_dim_for_mlp = base_embed_dim\n",
    "            logger.info(f\"MLPRouter initialized (ADV. EMBEDDINGS OFF). MLP input_dim: {current_input_dim_for_mlp}.\")\n",
    "        else: # Calculate new input_dim for advanced embeddings\n",
    "            dim_desc_embed = self.model_for_embeddings.config.hidden_size\n",
    "            current_input_dim_for_mlp += dim_desc_embed\n",
    "            dim_grad_sketch = 0\n",
    "            if hasattr(self.config, 'k_for_grad_sketch') and self.config.k_for_grad_sketch > 0 and hasattr(self.config, 'grad_sketch_max_elements'):\n",
    "                dim_grad_sketch = self.config.grad_sketch_max_elements\n",
    "                current_input_dim_for_mlp += dim_grad_sketch\n",
    "            dim_context_stats = 0\n",
    "            if hasattr(self.config, 'use_context_stats_avg_seq_length') and self.config.use_context_stats_avg_seq_length: dim_context_stats +=1\n",
    "            if hasattr(self.config, 'use_context_stats_avg_token_entropy') and self.config.use_context_stats_avg_token_entropy: dim_context_stats +=1\n",
    "            current_input_dim_for_mlp += dim_context_stats\n",
    "            logger.info(f\"MLPRouter initialized for ADVANCED EMBEDDINGS. Desc Dim: {dim_desc_embed}, GradSketch Dim: {dim_grad_sketch}, ContextStats Dim: {dim_context_stats}. Total MLP input_dim: {current_input_dim_for_mlp}.\")\n",
    "        \n",
    "        if current_input_dim_for_mlp == 0:\n",
    "            logger.error(\"MLPRouter: Calculated input_dim_for_mlp is 0. Defaulting. Check config.\")\n",
    "            current_input_dim_for_mlp = self.model_for_embeddings.config.hidden_size\n",
    "\n",
    "        self.input_dim_for_mlp = current_input_dim_for_mlp # Store for potential re-init on load\n",
    "        self.hidden_dim = config.router_hidden_size\n",
    "        self.mlp = nn.Sequential(nn.Linear(self.input_dim_for_mlp, self.hidden_dim), nn.GELU(), nn.LayerNorm(self.hidden_dim, eps=1e-5), nn.Linear(self.hidden_dim, self.hidden_dim), nn.GELU(), nn.LayerNorm(self.hidden_dim, eps=1e-5)).to(self.device).to(self.router_internal_dtype)\n",
    "        self.output_projection_layers = nn.ModuleDict(); self._setup_optimizer(config.router_learning_rate)\n",
    "\n",
    "    def _setup_optimizer(self, learning_rate: float):\n",
    "        params = list(self.mlp.parameters());\n",
    "        if self.output_projection_layers and len(self.output_projection_layers) > 0 : params.extend(list(self.output_projection_layers.parameters()))\n",
    "        if params: self.optimizer = optim.AdamW(params, lr=learning_rate, weight_decay=self.config.weight_decay)\n",
    "        else: self.optimizer = None\n",
    "\n",
    "    def _on_module_added(self, module_id_str: str):\n",
    "        if module_id_str not in self.output_projection_layers:\n",
    "            self.output_projection_layers[module_id_str] = nn.Linear(self.hidden_dim, 1).to(self.device).to(self.router_internal_dtype)\n",
    "            logger.info(f\"MLPRouter: Created output projection layer for '{module_id_str}'.\")\n",
    "            if self.optimizer and hasattr(self.optimizer, 'add_param_group'): self.optimizer.add_param_group({'params': self.output_projection_layers[module_id_str].parameters()})\n",
    "            elif not self.optimizer : self._setup_optimizer(self.config.router_learning_rate)\n",
    "\n",
    "    def _get_logits_for_all_modules_ordered(self, advanced_task_representation: torch.Tensor) -> Tuple[Optional[torch.Tensor], List[str]]:\n",
    "        if not self.output_projection_layers or len(self.output_projection_layers) == 0: return None, []\n",
    "        if advanced_task_representation.shape[0] != self.input_dim_for_mlp:\n",
    "            logger.error(f\"MLPRouter._get_logits: Advanced task repr dim {advanced_task_representation.shape[0]} != MLP input dim {self.input_dim_for_mlp}. Skipping.\")\n",
    "            return None, []\n",
    "        hidden = self.mlp(advanced_task_representation.to(self.router_internal_dtype))\n",
    "        valid_ids = [id_ for id_ in self.lora_id_to_idx if id_ in self.output_projection_layers]\n",
    "        if not valid_ids: return None, []\n",
    "        sorted_ids = sorted(valid_ids, key=lambda k: self.lora_id_to_idx[k])\n",
    "        logits_list = [self.output_projection_layers[id_](hidden).squeeze() for id_ in sorted_ids]\n",
    "        if not logits_list: return None, []\n",
    "        return torch.stack(logits_list), sorted_ids\n",
    "\n",
    "    def select_modules(self, task_description: str, task_examples_for_feature_gen: Optional[List[Dict[str, Any]]] = None, task_text_field: Optional[Union[str, List[str]]] = None, top_k: int = 1) -> List[Tuple[str, float]]:\n",
    "        self.eval();\n",
    "        if not self.output_projection_layers or len(self.output_projection_layers) == 0: return []\n",
    "        advanced_task_repr = self.get_advanced_task_representation(task_description, task_examples_for_feature_gen, task_text_field)\n",
    "        if advanced_task_repr is None: logger.warning(\"MLPRouter.select_modules: Failed to get advanced task representation.\"); return []\n",
    "        logits, ordered_ids = self._get_logits_for_all_modules_ordered(advanced_task_repr)\n",
    "        if logits is None or len(ordered_ids) == 0 : return []\n",
    "        probs = F.softmax(logits, dim=0); top_probs, top_indices = torch.topk(probs, min(top_k, len(probs)))\n",
    "        return [(ordered_ids[idx.item()], prob.item()) for idx, prob in zip(top_indices, top_probs)]\n",
    "\n",
    "    def train_step(self, training_data: List[Tuple[Tuple[str, Optional[List[Dict[str, Any]]], Optional[Union[str, List[str]]]], str]], epochs: int, learning_rate: float):\n",
    "        if self.optimizer is None and (len(list(self.mlp.parameters())) > 0 or len(self.output_projection_layers)>0) : self._setup_optimizer(learning_rate)\n",
    "        elif self.optimizer is None: logger.warning(\"MLPRouter: No params to optimize.\"); return\n",
    "        self.train(); criterion = nn.CrossEntropyLoss()\n",
    "        for epoch in range(epochs):\n",
    "            epoch_loss = 0.0; num_batches = 0\n",
    "            for (task_desc, examples_for_features, task_txt_fld), target_id_str in tqdm(training_data, desc=f\"Router MLP Ep{epoch+1}\", leave=False):\n",
    "                if not self.output_projection_layers or len(self.output_projection_layers) == 0 or target_id_str not in self.lora_id_to_idx or target_id_str not in self.output_projection_layers: logger.warning(f\"Skipping MLP training step for {target_id_str} - not in output_projection_layers or lora_id_to_idx.\"); continue\n",
    "                self.optimizer.zero_grad();\n",
    "                advanced_task_repr = self.get_advanced_task_representation(task_desc, examples_for_features, task_txt_fld)\n",
    "                if advanced_task_repr is None: logger.warning(f\"MLP Router training: Failed to get advanced task representation for desc '{task_desc[:30]}...'. Skipping batch.\"); continue\n",
    "                logits, ordered_ids_stack = self._get_logits_for_all_modules_ordered(advanced_task_repr)\n",
    "                if logits is None or len(ordered_ids_stack) == 0: logger.warning(\"MLP Router training: No logits from _get_logits_for_all_modules_ordered. Skipping batch.\"); continue\n",
    "                try: target_idx = ordered_ids_stack.index(target_id_str)\n",
    "                except ValueError: logger.error(f\"Target '{target_id_str}' not in MLP Router's ordered_ids_stack ({ordered_ids_stack}).\"); continue\n",
    "                loss = criterion(logits.unsqueeze(0), torch.tensor([target_idx], device=self.device))\n",
    "                if torch.isnan(loss).any() or torch.isinf(loss).any(): logger.error(f\"MLPRouter TRAIN Ep{epoch+1}: Loss NaN/Inf!\"); continue\n",
    "                loss.backward(); self.optimizer.step(); epoch_loss += loss.item(); num_batches +=1\n",
    "            if num_batches > 0: logger.info(f\"MLPRouter Ep {epoch+1} Avg Loss: {epoch_loss/num_batches:.4f}\")\n",
    "\n",
    "    def save_router_state(self, path: str):\n",
    "        # MLPRouter saves MLP state and output projection layers, plus metadata.\n",
    "        # Also save input_dim_for_mlp to check on load.\n",
    "        state = {\n",
    "            'mlp_state_dict': self.mlp.state_dict(),\n",
    "            'output_projection_layers_state_dict': {k: v.state_dict() for k, v in self.output_projection_layers.items()},\n",
    "            'optimizer_state_dict': self.optimizer.state_dict() if self.optimizer else None,\n",
    "            'lora_id_to_idx': self.lora_id_to_idx,\n",
    "            'idx_to_lora_id': self.idx_to_lora_id,\n",
    "            'next_lora_idx': self.next_lora_idx,\n",
    "            'module_metadata': self.module_metadata,\n",
    "            'input_dim_for_mlp': self.input_dim_for_mlp # Save this\n",
    "        }\n",
    "        torch.save(state, path)\n",
    "        logger.info(f\"MLPRouter state saved to {path}. Saved input_dim_for_mlp: {self.input_dim_for_mlp}. Num output_projection_layers saved: {len(self.output_projection_layers)}\")\n",
    "\n",
    "    def load_router_state(self, path: str):\n",
    "        if not os.path.exists(path): logger.warning(f\"MLPRouter: No state file found at {path}. Router not loaded.\"); return\n",
    "        state = torch.load(path, map_location='cpu') # Load to CPU first\n",
    "        logger.info(f\"MLPRouter.load_router_state: Loading from {path}. Saved state keys: {list(state.keys())}\")\n",
    "\n",
    "        saved_input_dim = state.get('input_dim_for_mlp')\n",
    "        if saved_input_dim and saved_input_dim != self.input_dim_for_mlp:\n",
    "            logger.warning(f\"MLPRouter: Loaded state input_dim_for_mlp ({saved_input_dim}) MISMATCHES current config expected ({self.input_dim_for_mlp}). Re-initializing MLP based on loaded dim.\")\n",
    "            self.input_dim_for_mlp = saved_input_dim\n",
    "            self.mlp = nn.Sequential(nn.Linear(self.input_dim_for_mlp, self.hidden_dim), nn.GELU(), nn.LayerNorm(self.hidden_dim, eps=1e-5), nn.Linear(self.hidden_dim, self.hidden_dim), nn.GELU(), nn.LayerNorm(self.hidden_dim, eps=1e-5))\n",
    "        \n",
    "        self.mlp.load_state_dict(state['mlp_state_dict'])\n",
    "        self.mlp.to(self.device).to(self.router_internal_dtype) # Move to device and set dtype\n",
    "\n",
    "        self.output_projection_layers.clear()\n",
    "        for module_id, proj_state_dict in state.get('output_projection_layers_state_dict', {}).items():\n",
    "            # _on_module_added will create these, but we need to load state into them.\n",
    "            # So, create first, then load state.\n",
    "            self.output_projection_layers[module_id] = nn.Linear(self.hidden_dim, 1).to(self.device).to(self.router_internal_dtype)\n",
    "            self.output_projection_layers[module_id].load_state_dict(proj_state_dict)\n",
    "\n",
    "        self.lora_id_to_idx = state.get('lora_id_to_idx', {}); self.idx_to_lora_id = state.get('idx_to_lora_id', {});\n",
    "        self.next_lora_idx = state.get('next_lora_idx', 0); self.module_metadata = state.get('module_metadata', {})\n",
    "\n",
    "        for lora_id_loaded in self.lora_id_to_idx.keys():\n",
    "            if lora_id_loaded not in self.module_metadata: self.module_metadata[lora_id_loaded] = {\"adapter_name\": lora_id_loaded, \"source\": \"reconstructed_from_loaded_state_mlp\"}\n",
    "            # _on_module_added will be called implicitly if these module_ids trigger new proj layers,\n",
    "            # but here we loaded them directly. Ensure optimizer knows about them.\n",
    "        \n",
    "        if len(list(self.mlp.parameters())) > 0 or len(self.output_projection_layers) > 0 :\n",
    "            self._setup_optimizer(self.config.router_learning_rate)\n",
    "            if self.optimizer and state.get('optimizer_state_dict'):\n",
    "                try:\n",
    "                    self.optimizer.load_state_dict(state['optimizer_state_dict'])\n",
    "                    for opt_state_group in self.optimizer.state.values():\n",
    "                        for k_opt, v_tensor_opt in opt_state_group.items():\n",
    "                            if isinstance(v_tensor_opt, torch.Tensor): opt_state_group[k_opt] = v_tensor_opt.to(self.device)\n",
    "                    logger.info(\"MLPRouter optimizer state loaded.\")\n",
    "                except Exception as e: logger.error(f\"MLPRouter: Failed to load optimizer state: {e}. Optimizer might be reset.\"); self._setup_optimizer(self.config.router_learning_rate)\n",
    "        else: self.optimizer = None\n",
    "        logger.info(f\"MLPRouter state loaded from {path}. Router has {len(self.output_projection_layers)} output projection layer(s). MLP input_dim: {self.input_dim_for_mlp}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-lMZypxAm0RO"
   },
   "source": [
    "## Neuromodulation\n",
    "\n",
    "The Neuromodulation module implements the Î³-Gain mechanism that dynamically modulates LoRA plasticity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "l4bC48pOm0RO"
   },
   "outputs": [],
   "source": [
    "class NeuromodulationManager:\n",
    "    \"\"\"\n",
    "    Neuromodulation Manager for The Adaptive Learner\n",
    "    Implements the Î³-Gain mechanism that dynamically modulates LoRA plasticity\n",
    "    \"\"\"\n",
    "    def __init__(self, config: AdaptiveLearnerConfig):\n",
    "        self.config = config\n",
    "        self.device = torch.device(config.device)\n",
    "        self.metrics_history = {}\n",
    "        self.gamma_gain_history = {}\n",
    "        self.metrics_dir = os.path.join(config.output_dir, \"gamma_metrics\")\n",
    "        os.makedirs(self.metrics_dir, exist_ok=True)\n",
    "\n",
    "    def compute_gamma_gain(self, task_id: str, metrics: Dict[str, float]) -> float:\n",
    "        if task_id not in self.metrics_history:\n",
    "            self.metrics_history[task_id] = []\n",
    "            self.gamma_gain_history[task_id] = []\n",
    "\n",
    "        self.metrics_history[task_id].append(metrics)\n",
    "        normalized_metrics = {}\n",
    "        for metric_name in self.config.gamma_gain_metrics:\n",
    "            if metric_name not in metrics:\n",
    "                logger.warning(f\"Metric {metric_name} not found in metrics dict for gamma_gain\")\n",
    "                continue\n",
    "            value = metrics[metric_name]\n",
    "            if len(self.metrics_history[task_id]) > 1:\n",
    "                prev_values = [h.get(metric_name, 0) for h in self.metrics_history[task_id][:-1]]\n",
    "                min_val, max_val = min(prev_values + [value]), max(prev_values + [value])\n",
    "                normalized = (value - min_val) / (max_val - min_val) if max_val > min_val else 0.5\n",
    "            else: # Default normalization for first step\n",
    "                if metric_name == \"accuracy\": normalized = value\n",
    "                elif metric_name == \"nll\": normalized = max(0, min(1, 1 - value / 10))\n",
    "                elif metric_name == \"gradient_norm\": normalized = max(0, min(1, value / 10))\n",
    "                elif metric_name == \"entropy\": normalized = max(0, min(1, 1 - value / 5))\n",
    "                else: normalized = value # Should not happen if metrics are in gamma_gain_metrics\n",
    "            normalized_metrics[metric_name] = normalized\n",
    "\n",
    "        gamma_gain = 0.0\n",
    "        total_weight = 0.0\n",
    "        for metric_name, weight in self.config.gamma_gain_weights.items():\n",
    "            if metric_name in normalized_metrics:\n",
    "                gamma_gain += weight * normalized_metrics[metric_name]\n",
    "                total_weight += weight\n",
    "\n",
    "        if total_weight > 0: gamma_gain /= total_weight\n",
    "        else: gamma_gain = 0.5 # Default if no relevant metrics\n",
    "\n",
    "        gamma_gain = gamma_gain * self.config.gamma_gain_lambda\n",
    "        self.gamma_gain_history[task_id].append(gamma_gain)\n",
    "\n",
    "        # Reduced verbosity here:\n",
    "        # logger.info(f\"Task {task_id} metrics: {metrics}\")\n",
    "        # logger.info(f\"Task {task_id} normalized metrics: {normalized_metrics}\") # This line is commented out\n",
    "        logger.info(f\"Task {task_id} NM - Metrics: { {k: round(v, 3) if isinstance(v, float) else v for k,v in metrics.items()} }, Î³-Gain: {gamma_gain:.4f}\")\n",
    "\n",
    "        return gamma_gain\n",
    "\n",
    "    def save_metrics(self, task_id: str):\n",
    "        if task_id not in self.metrics_history:\n",
    "            logger.warning(f\"No metrics history found for task {task_id}\")\n",
    "            return\n",
    "        task_dir = os.path.join(self.metrics_dir, task_id); os.makedirs(task_dir, exist_ok=True)\n",
    "        with open(os.path.join(task_dir, \"metrics_history.json\"), \"w\") as f: json.dump(self.metrics_history[task_id], f, indent=2)\n",
    "        with open(os.path.join(task_dir, \"gamma_gain_history.json\"), \"w\") as f: json.dump(self.gamma_gain_history[task_id], f, indent=2)\n",
    "        logger.info(f\"Saved metrics and Î³-Gain history for task {task_id}\")\n",
    "\n",
    "    def plot_gamma_gain(self, task_id: str, save_path: Optional[str] = None) -> plt.Figure:\n",
    "        if task_id not in self.gamma_gain_history or not self.gamma_gain_history[task_id]:\n",
    "            logger.warning(f\"No Î³-Gain history found for task {task_id} to plot.\")\n",
    "            fig, ax = plt.subplots(figsize=(8, 6)); ax.text(0.5, 0.5, f\"No Î³-Gain history for task {task_id}\", ha=\"center\", va=\"center\", fontsize=12); ax.set_axis_off()\n",
    "            return fig\n",
    "\n",
    "        fig, ax = plt.subplots(figsize=(10, 6))\n",
    "        gamma_values = self.gamma_gain_history[task_id]\n",
    "        steps = list(range(1, len(gamma_values) + 1))\n",
    "        ax.plot(steps, gamma_values, 'o-', linewidth=2, markersize=8, label=f\"Task {task_id} Î³-Gain\")\n",
    "        ax.axhline(y=self.config.gamma_gain_lambda, color='r', linestyle='--', alpha=0.7, label=f\"Î» = {self.config.gamma_gain_lambda}\")\n",
    "        ax.set_xlabel(\"Training Step within Task\", fontsize=12); ax.set_ylabel(\"Î³-Gain Value\", fontsize=12)\n",
    "        ax.set_title(f\"Î³-Gain History for Task {task_id}\", fontsize=14); ax.grid(alpha=0.3); ax.legend(fontsize=10)\n",
    "        y_min = 0; y_max = max(1.0, self.config.gamma_gain_lambda * 1.2, max(gamma_values) * 1.2 if gamma_values else 0)\n",
    "        ax.set_ylim(y_min, y_max)\n",
    "        if save_path: plt.savefig(save_path, dpi=300, bbox_inches=\"tight\")\n",
    "        return fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qfDr_80dm0RO"
   },
   "source": [
    "## Consolidation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "jNm387L7m0RO"
   },
   "outputs": [],
   "source": [
    "# Cell 11: ConsolidationManager (Corrected _consolidate_aflora)\n",
    "\n",
    "import os\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn # Ensure nn is imported\n",
    "import torch.nn.functional as F # Ensure F is imported\n",
    "from peft import PeftModel # Ensure PeftModel is imported\n",
    "from typing import List, Dict, Any, Tuple, Optional, Union # Ensure these are imported\n",
    "\n",
    "# Ensure 'logger' is defined globally (e.g., Cell 3 or 4)\n",
    "if 'logger' not in globals():\n",
    "    import logging\n",
    "    logger = logging.getLogger(__name__ + \".consolidation_mgr_cell11\")\n",
    "    if not logger.hasHandlers():\n",
    "        handler = logging.StreamHandler()\n",
    "        formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "        handler.setFormatter(formatter)\n",
    "        logger.addHandler(handler)\n",
    "        logger.setLevel(logging.INFO)\n",
    "\n",
    "# Ensure AdaptiveLearnerConfig is defined or imported if type hinting is used strictly\n",
    "# from your_config_module import AdaptiveLearnerConfig # Or ensure it's defined before this cell\n",
    "\n",
    "class ConsolidationManager:\n",
    "    \"\"\"\n",
    "    Manages the consolidation of LoRA parameters to prevent catastrophic forgetting\n",
    "    \"\"\"\n",
    "    def __init__(self, config: 'AdaptiveLearnerConfig'): # Use string literal for forward reference if defined later\n",
    "        self.config = config\n",
    "        self.device = torch.device(config.device)\n",
    "        self.importance_scores: Dict[str, Dict[str, torch.Tensor]] = {} # {lora_id: {param_name: importance_tensor}}\n",
    "        self.consolidation_history: Dict[str, List[Dict[str, Any]]] = {}\n",
    "        self.consolidation_dir = os.path.join(config.output_dir, \"consolidation\")\n",
    "        os.makedirs(self.consolidation_dir, exist_ok=True)\n",
    "\n",
    "    def consolidate(self, lora_id: str, model: PeftModel, gamma_gain: float, data_loader: Optional[Any] = None) -> None:\n",
    "        if lora_id not in self.consolidation_history: self.consolidation_history[lora_id] = []\n",
    "        self.consolidation_history[lora_id].append({\"timestamp\": time.time(), \"gamma_gain\": gamma_gain, \"method\": self.config.consolidation_method})\n",
    "\n",
    "        if self.config.consolidation_method == \"aflora\":\n",
    "            self._consolidate_aflora(lora_id, model, gamma_gain)\n",
    "        elif self.config.consolidation_method == \"ewc\":\n",
    "            if data_loader is None and self.config.ewc_data_loader_num_samples > 0 : \n",
    "                logger.warning(f\"EWC consolidation for {lora_id} requires a data_loader (ewc_data_loader_num_samples > 0). Skipping.\")\n",
    "                return\n",
    "            self._consolidate_ewc(lora_id, model, gamma_gain, data_loader)\n",
    "        elif self.config.consolidation_method == \"none\": logger.info(f\"Consolidation 'none' for {lora_id}, skipping.\") # Added lora_id for clarity\n",
    "        else: logger.warning(f\"Unknown consolidation method: {self.config.consolidation_method} for {lora_id}\")\n",
    "\n",
    "    def _consolidate_aflora(self, lora_id: str, model: PeftModel, gamma_gain: float) -> None:\n",
    "        logger.info(f\"Applying AFLoRA for {lora_id} with Î³-Gain {gamma_gain:.4f}\")\n",
    "        if lora_id not in self.importance_scores: self.importance_scores[lora_id] = {}\n",
    "\n",
    "        lora_params_with_grads = {name: param for name, param in model.named_parameters() if \"lora_\" in name and param.requires_grad and param.grad is not None}\n",
    "        \n",
    "        # THIS IS THE CORRECTED LINE:\n",
    "        if not lora_params_with_grads: \n",
    "            logger.warning(f\"AFLoRA: No LoRA params with grads for {lora_id}.\") # Removed stray backslash before quote\n",
    "            return\n",
    "        # END OF CORRECTED LINE\n",
    "\n",
    "        for name, param in lora_params_with_grads.items():\n",
    "            importance = gamma_gain * torch.abs(param.detach()) * torch.abs(param.grad.detach())\n",
    "            if name in self.importance_scores[lora_id]: self.importance_scores[lora_id][name] += importance\n",
    "            else: self.importance_scores[lora_id][name] = importance\n",
    "\n",
    "        for name, param in lora_params_with_grads.items(): # Iterate again to apply mask\n",
    "            if name in self.importance_scores[lora_id]:\n",
    "                current_param_importance = self.importance_scores[lora_id][name]\n",
    "                mask = (current_param_importance > self.config.aflora_importance_threshold).float()\n",
    "                param.grad = param.grad * (1 - mask) # Freeze important params by zeroing their grad contribution\n",
    "        logger.info(f\"AFLoRA applied for module {lora_id}\")\n",
    "\n",
    "    def _consolidate_ewc(self, lora_id: str, model: PeftModel, gamma_gain: float, data_loader: Optional[Any]) -> None:\n",
    "        logger.info(f\"Applying EWC for {lora_id} with actual gamma_gain_param_val {gamma_gain:.4f}\")\n",
    "        \n",
    "        bypass_gamma_for_fisher_accumulation = getattr(self.config, 'ewc_fixed_lambda_bypass_gamma', False)\n",
    "        if bypass_gamma_for_fisher_accumulation:\n",
    "            logger.info(f\"EWC (Fixed Lambda Mode): Bypassing gamma_gain for Fisher accumulation. Effective gamma_gain for accumulation set to 1.0.\")\n",
    "            effective_gamma_for_accumulation = 1.0\n",
    "        else:\n",
    "            effective_gamma_for_accumulation = gamma_gain\n",
    "\n",
    "        lora_params_to_penalize = {name: param for name, param in model.named_parameters() if \"lora_\" in name and param.requires_grad}\n",
    "        if not lora_params_to_penalize: \n",
    "            logger.warning(f\"EWC: No trainable LoRA params found for {lora_id}.\")\n",
    "            return\n",
    "\n",
    "        old_params_values = {name: param.clone().detach() for name, param in lora_params_to_penalize.items()}\n",
    "\n",
    "        fisher_diag = {}\n",
    "        if data_loader is not None and self.config.ewc_data_loader_num_samples > 0:\n",
    "            fisher_diag = self._compute_fisher_matrix(model, lora_params_to_penalize, data_loader)\n",
    "            if not fisher_diag: \n",
    "                logger.warning(f\"EWC: Fisher matrix computation failed or empty for {lora_id} using data_loader. EWC penalty might be ineffective if no prior importance.\")\n",
    "        elif self.config.ewc_data_loader_num_samples == 0:\n",
    "             logger.info(f\"EWC: ewc_data_loader_num_samples is 0 for {lora_id}. Fisher matrix not computed in this step. Relying on existing importance scores if any.\")\n",
    "        else: \n",
    "            logger.warning(f\"EWC: data_loader is None for {lora_id} but ewc_data_loader_num_samples ({self.config.ewc_data_loader_num_samples}) > 0. Fisher matrix not computed. EWC penalty might be ineffective.\")\n",
    "\n",
    "\n",
    "        if lora_id not in self.importance_scores: self.importance_scores[lora_id] = {}\n",
    "\n",
    "        for name, f_val in fisher_diag.items(): \n",
    "            weighted_f_val = effective_gamma_for_accumulation * f_val \n",
    "            if name in self.importance_scores[lora_id]: \n",
    "                self.importance_scores[lora_id][name] += weighted_f_val\n",
    "            else: \n",
    "                self.importance_scores[lora_id][name] = weighted_f_val\n",
    "        \n",
    "        num_params_penalized = 0\n",
    "        for name, param in lora_params_to_penalize.items():\n",
    "            if name in old_params_values and name in self.importance_scores[lora_id] and self.importance_scores[lora_id][name].sum() > 0: \n",
    "                if param.grad is None: \n",
    "                    param.grad = torch.zeros_like(param)\n",
    "\n",
    "                delta = param.detach() - old_params_values[name] \n",
    "                current_importance = self.importance_scores[lora_id][name].to(delta.device)\n",
    "                penalty_grad_component = self.config.ewc_lambda * current_importance * delta\n",
    "                param.grad += penalty_grad_component\n",
    "                num_params_penalized +=1\n",
    "        \n",
    "        if num_params_penalized > 0:\n",
    "            logger.info(f\"EWC penalty gradient added for {num_params_penalized} parameters in module {lora_id}. Bypass gamma for Fisher accum: {bypass_gamma_for_fisher_accumulation}\")\n",
    "        elif fisher_diag: \n",
    "             logger.info(f\"EWC: Fisher was computed for {lora_id}, but no EWC penalty applied (e.g. importance scores might be zero or params not in list).\")\n",
    "        elif not fisher_diag and (not self.importance_scores[lora_id] or all(v.sum() == 0 for v in self.importance_scores[lora_id].values())): # Check if importance scores are empty or all zero\n",
    "            logger.info(f\"EWC: No Fisher computed and no effective prior importance scores for {lora_id}. No EWC penalty applied.\")\n",
    "\n",
    "\n",
    "    def _compute_fisher_matrix(self, model: PeftModel, lora_params: Dict[str, nn.Parameter], data_loader: Any) -> Dict[str, torch.Tensor]:\n",
    "        fisher_matrices = {name: torch.zeros_like(param.data) for name, param in lora_params.items()}\n",
    "        model.eval() \n",
    "\n",
    "        num_samples_processed = 0\n",
    "        samples_to_process_count = 0\n",
    "        max_samples_for_fisher = self.config.ewc_data_loader_num_samples\n",
    "        \n",
    "        for batch_idx, batch in enumerate(data_loader):\n",
    "            if samples_to_process_count >= max_samples_for_fisher:\n",
    "                break\n",
    "\n",
    "            model.zero_grad() \n",
    "            input_ids = batch[\"input_ids\"].to(self.device)\n",
    "            attention_mask = batch[\"attention_mask\"].to(self.device)\n",
    "            labels = batch[\"labels\"].to(self.device) \n",
    "\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            logits = outputs.logits\n",
    "            shift_logits = logits[:, :-1, :].contiguous()\n",
    "            shift_labels = labels[:, 1:].contiguous()\n",
    "            flat_shift_logits = shift_logits.view(-1, shift_logits.size(-1))\n",
    "            flat_shift_labels = shift_labels.view(-1)\n",
    "            active_loss_mask = flat_shift_labels != -100\n",
    "            active_logits = flat_shift_logits[active_loss_mask]   \n",
    "            active_labels = flat_shift_labels[active_loss_mask]   \n",
    "\n",
    "            if active_logits.numel() == 0: \n",
    "                logger.warning(f\"EWC Fisher: Batch {batch_idx} had no active target tokens. Skipping.\")\n",
    "                continue\n",
    "\n",
    "            log_probs_of_targets = F.log_softmax(active_logits, dim=-1)\n",
    "            selected_log_probs = log_probs_of_targets[torch.arange(active_labels.size(0)), active_labels]\n",
    "            log_likelihood_sum = selected_log_probs.sum()\n",
    "            \n",
    "            if not torch.isnan(log_likelihood_sum) and not torch.isinf(log_likelihood_sum):\n",
    "                log_likelihood_sum.backward()\n",
    "                for name, param in lora_params.items():\n",
    "                    if param.grad is not None:\n",
    "                        fisher_matrices[name] += param.grad.data.pow(2) \n",
    "                num_samples_processed += input_ids.size(0) \n",
    "            else:\n",
    "                logger.warning(f\"EWC Fisher: log_likelihood_sum was NaN/Inf for batch {batch_idx}. Skipping backward for this batch.\")\n",
    "            samples_to_process_count += input_ids.size(0)\n",
    "\n",
    "        if num_samples_processed == 0:\n",
    "            logger.warning(\"EWC Fisher: No samples successfully processed for Fisher. Returning empty Fisher matrix.\")\n",
    "            return {}\n",
    "\n",
    "        fisher_matrices = {name: f_val / num_samples_processed for name, f_val in fisher_matrices.items()}\n",
    "        logger.info(f\"Computed Fisher matrix over {num_samples_processed} samples (target was {max_samples_for_fisher}).\")\n",
    "        return fisher_matrices\n",
    "\n",
    "    def get_importance_scores(self, lora_id: str) -> Dict[str, torch.Tensor]:\n",
    "        return self.importance_scores.get(lora_id, {})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-A0VXJnGm0RO"
   },
   "source": [
    "Cell 14: Generative Replay Manager, CMTReplay, PCGRReplay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "_x9JqNuPm0RO"
   },
   "outputs": [],
   "source": [
    "# Cell 14: Generative Replay Manager, CMTReplay, PCGRReplay (Updated for replay_backbone_encoding_batch_size)\n",
    "from transformers import get_linear_schedule_with_warmup # Ensure this import is present\n",
    "import torch # Ensure torch is imported\n",
    "import torch.nn as nn # Ensure nn is imported\n",
    "import torch.nn.functional as F # Ensure F is imported\n",
    "import numpy as np # Ensure numpy is imported\n",
    "from tqdm.notebook import tqdm # Ensure tqdm is imported\n",
    "\n",
    "class GenerativeReplayManager: # Unchanged from previous correct version\n",
    "    def __init__(self, config: AdaptiveLearnerConfig, backbone_model: PreTrainedModel, tokenizer: PreTrainedTokenizer):\n",
    "        self.config = config; self.backbone_model = backbone_model; self.tokenizer = tokenizer\n",
    "        self.device = torch.device(config.device); self.replay_buffer: Dict[str, List[Dict[str, str]]] = {}\n",
    "        if config.replay_method == \"cmt\": self.replay_model = CMTReplay(config, self.backbone_model, self.tokenizer)\n",
    "        elif config.replay_method == \"pcgr\": self.replay_model = PCGRReplay(config, self.backbone_model, self.tokenizer)\n",
    "        elif config.replay_method == \"none\": self.replay_model = None; logger.info(\"Generative replay disabled.\")\n",
    "        else: self.replay_model = None; logger.warning(f\"Unknown replay method: {config.replay_method}, disabling replay.\")\n",
    "        self.replay_dir = os.path.join(config.output_dir, \"replay\"); os.makedirs(self.replay_dir, exist_ok=True)\n",
    "\n",
    "    def add_task_samples(self, task_id: str, raw_samples: List[Dict[str, str]], train_replay_model_now: bool = True) -> None:\n",
    "        if task_id not in self.replay_buffer: self.replay_buffer[task_id] = []\n",
    "        current_samples_in_buffer = self.replay_buffer[task_id]\n",
    "        valid_raw_samples = [rs for rs in raw_samples if isinstance(rs, dict) and 'input' in rs and 'output' in rs]\n",
    "        if len(valid_raw_samples) != len(raw_samples): logger.warning(\"ReplayManager: Some samples excluded due to format.\")\n",
    "        all_raw_samples_for_task = current_samples_in_buffer + valid_raw_samples\n",
    "        if len(all_raw_samples_for_task) > self.config.replay_buffer_size: self.replay_buffer[task_id] = self._reservoir_sample(all_raw_samples_for_task, self.config.replay_buffer_size)\n",
    "        else: self.replay_buffer[task_id] = all_raw_samples_for_task\n",
    "        if train_replay_model_now and self.replay_model is not None and self.replay_buffer[task_id] and (self.config.replay_alpha > 0 or self.config.feature_replay_alpha > 0):\n",
    "            if hasattr(self.replay_model, 'backbone_model') and isinstance(self.backbone_model, PreTrainedModel): self.replay_model.backbone_model = self.backbone_model\n",
    "            self.train_replay_model(task_id, self.replay_buffer[task_id]) \n",
    "\n",
    "    def get_raw_samples_for_replay(self, task_id: str, num_samples: int) -> List[Dict[str, str]]: \n",
    "        if task_id not in self.replay_buffer or not self.replay_buffer[task_id]: return []\n",
    "        buffered_samples = self.replay_buffer[task_id]\n",
    "        if num_samples >= len(buffered_samples): return list(buffered_samples)\n",
    "        indices = np.random.choice(len(buffered_samples), num_samples, replace=False); return [buffered_samples[i] for i in indices]\n",
    "\n",
    "    def generate_replay_units(self, task_id_to_generate_for: str, num_units: int) -> List[Dict[str, Any]]: \n",
    "        if self.replay_model is None: return []\n",
    "        if hasattr(self.replay_model, 'task_prototypes'):\n",
    "             if task_id_to_generate_for not in self.replay_model.task_prototypes: return []\n",
    "        else: logger.warning(\"ReplayManager: Replay model missing 'task_prototypes'.\"); return []\n",
    "        return self.replay_model.generate_samples(task_id_to_generate_for, num_units)\n",
    "\n",
    "    def train_replay_model(self, task_id: str, raw_samples_for_task: List[Dict[str, str]]) -> None: \n",
    "        if self.replay_model is None or not raw_samples_for_task: return\n",
    "        self.replay_model.train_model(task_id, raw_samples_for_task)\n",
    "\n",
    "    def get_tasks_with_replay_data(self, exclude_task_id: Optional[str] = None) -> List[str]: \n",
    "        valid_replay_tasks = []\n",
    "        if self.replay_model and hasattr(self.replay_model, 'task_prototypes') and self.replay_model.task_prototypes:\n",
    "            for task_id_with_prototype in self.replay_model.task_prototypes.keys():\n",
    "                if (exclude_task_id is None or task_id_with_prototype != exclude_task_id) and task_id_with_prototype in self.replay_buffer and self.replay_buffer[task_id_with_prototype]:\n",
    "                    valid_replay_tasks.append(task_id_with_prototype)\n",
    "        elif not self.replay_model or not hasattr(self.replay_model, 'task_prototypes'):\n",
    "             for task_id_in_buffer in self.replay_buffer.keys():\n",
    "                 if self.replay_buffer[task_id_in_buffer] and (exclude_task_id is None or task_id_in_buffer != exclude_task_id):\n",
    "                    valid_replay_tasks.append(task_id_in_buffer)\n",
    "        return list(set(valid_replay_tasks))\n",
    "\n",
    "    def _reservoir_sample(self, samples: List[Any], n: int) -> List[Any]: \n",
    "        if not samples or n <= 0: return [];\n",
    "        if len(samples) <= n: return samples\n",
    "        reservoir = samples[:n];\n",
    "        for i in range(n, len(samples)): j = np.random.randint(0, i + 1);\n",
    "        if j < n: reservoir[j] = samples[i]\n",
    "        return reservoir\n",
    "\n",
    "class CMTReplay: # Updated backbone_encoding_batch_size\n",
    "    _first_train_call_ever = True\n",
    "    def __init__(self, config: AdaptiveLearnerConfig, backbone_model: PreTrainedModel, tokenizer: PreTrainedTokenizer): \n",
    "        self.config = config; self.backbone_model = backbone_model; self.tokenizer = tokenizer\n",
    "        self.device = torch.device(config.device)\n",
    "        try: self.model_dtype = next(backbone_model.parameters()).dtype; self.hidden_dim = backbone_model.config.hidden_size\n",
    "        except (StopIteration, AttributeError) as e: logger.warning(f\"CMTReplay: Model issue ({e}). Defaults.\"); self.model_dtype = torch.float16; self.hidden_dim = getattr(backbone_model.config, \"hidden_size\", 2048)\n",
    "        self.memory_size = config.cmt_memory_size; self.cmt_internal_dtype = torch.float32\n",
    "        layers_enc = []; current_dim_enc = self.hidden_dim\n",
    "        for i in range(config.cmt_compressor_layers):\n",
    "            next_dim = self.memory_size if i == config.cmt_compressor_layers - 1 else max(1, (current_dim_enc + self.memory_size) // 2)\n",
    "            layers_enc.append(nn.Linear(current_dim_enc, next_dim))\n",
    "            if i < config.cmt_compressor_layers - 1 : layers_enc.append(nn.LayerNorm(next_dim, eps=1e-5)); layers_enc.append(nn.GELU())\n",
    "            current_dim_enc = next_dim\n",
    "        self.encoder = nn.Sequential(*layers_enc).to(self.device).to(self.cmt_internal_dtype)\n",
    "        layers_dec = []; current_dim_dec = self.memory_size\n",
    "        for i in range(config.cmt_compressor_layers):\n",
    "            next_dim = self.hidden_dim if i == config.cmt_compressor_layers - 1 else max(1, (current_dim_dec + self.hidden_dim) // 2)\n",
    "            layers_dec.append(nn.Linear(current_dim_dec, next_dim))\n",
    "            if i < config.cmt_compressor_layers - 1 : layers_dec.append(nn.LayerNorm(next_dim, eps=1e-5)); layers_dec.append(nn.GELU())\n",
    "            current_dim_dec = next_dim\n",
    "        self.decoder = nn.Sequential(*layers_dec).to(self.device).to(self.cmt_internal_dtype)\n",
    "        self.optimizer = torch.optim.AdamW(list(self.encoder.parameters()) + list(self.decoder.parameters()), lr=config.replay_model_learning_rate, weight_decay=config.replay_model_weight_decay)\n",
    "        self.task_prototypes: Dict[str, torch.Tensor] = {}\n",
    "\n",
    "    def _tokenize_batch_for_encode(self, text_inputs: List[str]) -> Dict[str, torch.Tensor]: \n",
    "        return self.tokenizer(text_inputs, return_tensors=\"pt\", padding=\"longest\", truncation=True, max_length=self.config.max_seq_length, add_special_tokens=True)\n",
    "\n",
    "    def encode_input_batch(self, tokenized_batch: Dict[str, torch.Tensor]) -> torch.Tensor: \n",
    "        target_device = next(self.backbone_model.parameters()).device\n",
    "        inputs_on_device = {k: v.to(target_device) for k,v in tokenized_batch.items() if torch.is_tensor(v)}\n",
    "        original_mode = self.backbone_model.training\n",
    "        if hasattr(self.backbone_model, 'eval'): self.backbone_model.eval()\n",
    "        batch_size = inputs_on_device['input_ids'].shape[0]\n",
    "        pooled_output = torch.zeros(batch_size, self.hidden_dim, device=target_device, dtype=self.model_dtype)\n",
    "        try:\n",
    "            with torch.no_grad():\n",
    "                outputs = self.backbone_model(**inputs_on_device, output_hidden_states=True)\n",
    "                last_hidden_state = outputs.hidden_states[-1]\n",
    "                attention_mask = inputs_on_device.get('attention_mask', torch.ones_like(last_hidden_state[..., 0]))\n",
    "                mask_expanded = attention_mask.unsqueeze(-1).to(last_hidden_state.dtype)\n",
    "                sum_masked_hidden = (last_hidden_state * mask_expanded).sum(dim=1)\n",
    "                sum_mask = torch.clamp(mask_expanded.sum(dim=1), min=1e-9)\n",
    "                pooled_output = sum_masked_hidden / sum_mask\n",
    "        except Exception as e: logger.error(f\"CMTReplay.encode_input_batch: Error: {e}\")\n",
    "        if hasattr(self.backbone_model, 'train') and original_mode: self.backbone_model.train()\n",
    "        if torch.isnan(pooled_output).any() or torch.isinf(pooled_output).any():\n",
    "            logger.error(f\"CMTReplay.encode_input_batch produced NaN/Inf! Shape: {pooled_output.shape}\"); return torch.zeros_like(pooled_output)\n",
    "        return pooled_output.to(self.model_dtype)\n",
    "\n",
    "    def train_model(self, task_id: str, raw_text_samples_for_task_buffer: List[Dict[str, str]]) -> None:\n",
    "        if not raw_text_samples_for_task_buffer: return\n",
    "        logger.info(f\"CMTReplay: Training AE for task '{task_id}' on {len(raw_text_samples_for_task_buffer)} samples from buffer.\")\n",
    "        num_epochs = 3; internal_ae_batch_size = self.config.replay_model_internal_batch_size\n",
    "        self.optimizer = torch.optim.AdamW(list(self.encoder.parameters()) + list(self.decoder.parameters()), lr=self.config.replay_model_learning_rate, weight_decay=self.config.replay_model_weight_decay)\n",
    "        self.encoder.train().to(self.cmt_internal_dtype); self.decoder.train().to(self.cmt_internal_dtype)\n",
    "        log_this_call_weights = False\n",
    "        if CMTReplay._first_train_call_ever: log_this_call_weights = True; CMTReplay._first_train_call_ever = False\n",
    "\n",
    "        all_input_texts = [sample['input'] for sample in raw_text_samples_for_task_buffer]\n",
    "        all_original_hidden_states_list = []\n",
    "        # MODIFIED: Use replay_backbone_encoding_batch_size\n",
    "        backbone_encoding_batch_size = self.config.replay_backbone_encoding_batch_size \n",
    "        if backbone_encoding_batch_size == 0 : backbone_encoding_batch_size = 1 \n",
    "\n",
    "        for i in range(0, len(all_input_texts), backbone_encoding_batch_size):\n",
    "            batch_texts = all_input_texts[i:i+backbone_encoding_batch_size]\n",
    "            if not batch_texts: continue\n",
    "            tokenized_batch = self._tokenize_batch_for_encode(batch_texts)\n",
    "            hidden_states_batch = self.encode_input_batch(tokenized_batch)\n",
    "            if not (torch.isnan(hidden_states_batch).any() or torch.isinf(hidden_states_batch).any()):\n",
    "                all_original_hidden_states_list.append(hidden_states_batch)\n",
    "\n",
    "        if not all_original_hidden_states_list: logger.warning(f\"CMTReplay: No valid hidden states for task '{task_id}'. Skipping AE.\"); return\n",
    "        all_original_hidden_states = torch.cat(all_original_hidden_states_list, dim=0).to(self.cmt_internal_dtype)\n",
    "\n",
    "        total_loss_all_epochs, total_uniformity_all_epochs, total_batches_all_epochs = 0.0, 0.0, 0\n",
    "        for epoch in range(num_epochs):\n",
    "            epoch_loss, epoch_uniformity, num_actual_batches_epoch = 0.0, 0.0, 0\n",
    "            permuted_indices = torch.randperm(all_original_hidden_states.size(0))\n",
    "            use_tqdm_for_batches = (all_original_hidden_states.size(0) / internal_ae_batch_size) > 5\n",
    "            batch_iterable = range(0, all_original_hidden_states.size(0), internal_ae_batch_size)\n",
    "            if use_tqdm_for_batches: batch_iterable = tqdm(batch_iterable, desc=f\"CMT AE Ep{epoch+1} Task:{task_id}\", leave=False)\n",
    "\n",
    "            for i_batch_ae, start_idx_ae in enumerate(batch_iterable):\n",
    "                end_idx_ae = min(start_idx_ae + internal_ae_batch_size, all_original_hidden_states.size(0))\n",
    "                batch_indices = permuted_indices[start_idx_ae:end_idx_ae]\n",
    "                if len(batch_indices) == 0: continue\n",
    "                batched_original_hidden_for_ae = all_original_hidden_states[batch_indices]\n",
    "                self.optimizer.zero_grad();\n",
    "                prediction_for_loss = self.encoder(batched_original_hidden_for_ae) \n",
    "                if torch.isnan(prediction_for_loss).any() or torch.isinf(prediction_for_loss).any(): logger.error(f\"CMT AE TRAIN: memory_codes NaN/Inf!\"); continue\n",
    "                reconstructed_batch = self.decoder(prediction_for_loss)\n",
    "                if torch.isnan(reconstructed_batch).any() or torch.isinf(reconstructed_batch).any(): logger.error(f\"CMT AE TRAIN: reconstructed_batch NaN/Inf!\"); continue\n",
    "                target_for_loss = batched_original_hidden_for_ae.detach() \n",
    "                is_problematic = False\n",
    "                if torch.isnan(reconstructed_batch).any() or torch.isinf(reconstructed_batch).any(): is_problematic = True\n",
    "                if torch.isnan(target_for_loss).any() or torch.isinf(target_for_loss).any(): is_problematic = True\n",
    "                recon_loss = F.mse_loss(reconstructed_batch, target_for_loss) \n",
    "                if recon_loss.item() < -1e-7: logger.error(f\"CMT NEGATIVE RECON LOSS DETECTED: {recon_loss.item():.6f}\")\n",
    "                if torch.isnan(recon_loss).any() or torch.isinf(recon_loss).any() or is_problematic:\n",
    "                     logger.error(f\"CMT AE TRAIN: recon_loss NaN/Inf or inputs problematic! Loss: {recon_loss.item() if torch.isfinite(recon_loss) else 'NaN/Inf'}. Skipping batch update.\"); continue\n",
    "                current_batch_loss_val = recon_loss; current_batch_uniformity = 0.0\n",
    "                if len(prediction_for_loss) > 1 and self.config.cmt_uniformity_weight > 0: \n",
    "                    normalized_codes = F.normalize(prediction_for_loss, p=2, dim=1); pdist_matrix = torch.cdist(normalized_codes, normalized_codes, p=2.0)\n",
    "                    eye_mask = ~torch.eye(pdist_matrix.shape[0], device=pdist_matrix.device, dtype=torch.bool)\n",
    "                    if pdist_matrix[eye_mask].numel() > 0:\n",
    "                        uniformity_penalty = -torch.log(pdist_matrix[eye_mask].clamp(min=1e-8)).mean()\n",
    "                        if torch.isfinite(uniformity_penalty): current_batch_loss_val += self.config.cmt_uniformity_weight * uniformity_penalty; current_batch_uniformity = uniformity_penalty.item()\n",
    "                if torch.isnan(current_batch_loss_val).any() or torch.isinf(current_batch_loss_val).any(): logger.error(f\"CMT AE TRAIN: final batch_loss NaN/Inf!\"); continue\n",
    "                current_batch_loss_val.backward()\n",
    "                torch.nn.utils.clip_grad_norm_(list(self.encoder.parameters()) + list(self.decoder.parameters()), self.config.replay_model_grad_clip_norm)\n",
    "                self.optimizer.step()\n",
    "                if log_this_call_weights and epoch == 0 and i_batch_ae == 0: logger.info(f\"CMT Initial WEIGHT DEBUG (Task {task_id}): Weights AFTER first AE optim step...\") \n",
    "                epoch_loss += current_batch_loss_val.item(); epoch_uniformity += current_batch_uniformity; num_actual_batches_epoch += 1\n",
    "            total_loss_all_epochs += epoch_loss; total_uniformity_all_epochs += epoch_uniformity; total_batches_all_epochs += num_actual_batches_epoch\n",
    "\n",
    "        avg_loss_overall = total_loss_all_epochs / total_batches_all_epochs if total_batches_all_epochs > 0 else float('nan')\n",
    "        avg_unif_overall = total_uniformity_all_epochs / total_batches_all_epochs if total_batches_all_epochs > 0 else float('nan')\n",
    "        logger.info(f\"CMTReplay AE training complete for task '{task_id}' ({len(raw_text_samples_for_task_buffer)} buffer samples). AvgLoss: {avg_loss_overall:.4f}, AvgUniformity: {avg_unif_overall:.4f}, LR: {self.optimizer.param_groups[0]['lr']:.2e}\")\n",
    "        self.encoder.eval()\n",
    "        if all_original_hidden_states.numel() > 0 :\n",
    "            with torch.no_grad():\n",
    "                memory_codes_for_proto = self.encoder(all_original_hidden_states)\n",
    "                if not (torch.isnan(memory_codes_for_proto).any() or torch.isinf(memory_codes_for_proto).any()):\n",
    "                    prototype = torch.mean(memory_codes_for_proto, dim=0); self.task_prototypes[task_id] = prototype.detach()\n",
    "\n",
    "    def generate_samples(self, task_id: str, num_samples: int) -> List[Dict[str, Any]]: \n",
    "        if task_id not in self.task_prototypes: return []\n",
    "        self.decoder.eval().to(self.cmt_internal_dtype); generated_units_list = []\n",
    "        with torch.no_grad():\n",
    "            prototype = self.task_prototypes[task_id].to(self.device)\n",
    "            if torch.isnan(prototype).any() or torch.isinf(prototype).any(): logger.error(f\"CMTReplay gen: Proto for {task_id} NaN/Inf!\"); return []\n",
    "            for _ in range(num_samples):\n",
    "                noise = torch.randn_like(prototype) * 0.1; memory_code_noisy = prototype + noise\n",
    "                hidden_reconstructed = self.decoder(memory_code_noisy)\n",
    "                if torch.isnan(hidden_reconstructed).any() or torch.isinf(hidden_reconstructed).any(): logger.error(f\"CMTReplay gen: Decoded hidden NaN/Inf for {task_id}!\"); continue\n",
    "                generated_units_list.append({\"hidden_state\": hidden_reconstructed.cpu().to(self.model_dtype), \"task_id\": task_id, \"is_generated\": True })\n",
    "        return generated_units_list\n",
    "\n",
    "class PCGRReplay: # Updated backbone_encoding_batch_size\n",
    "    _first_train_call_ever = True\n",
    "    def __init__(self, config: AdaptiveLearnerConfig, backbone_model: PreTrainedModel, tokenizer: PreTrainedTokenizer): \n",
    "        self.config = config; self.backbone_model = backbone_model; self.tokenizer = tokenizer\n",
    "        self.device = torch.device(config.device)\n",
    "        try: self.model_dtype = next(backbone_model.parameters()).dtype; self.hidden_dim = backbone_model.config.hidden_size\n",
    "        except (StopIteration, AttributeError) as e: logger.warning(f\"PCGRReplay: Model issue ({e}). Defaults used.\"); self.model_dtype = torch.float16; self.hidden_dim = getattr(backbone_model.config, \"hidden_size\", 2048)\n",
    "        self.latent_dim = config.pcgr_latent_dim; self.pcgr_internal_dtype = torch.float32\n",
    "        num_blocks = config.cmt_compressor_layers \n",
    "        enc_layers = []; current_dim_enc = self.hidden_dim\n",
    "        for i in range(num_blocks):\n",
    "            next_dim_enc = (self.latent_dim * 2) if i == num_blocks - 1 else max(1, current_dim_enc // 2)\n",
    "            enc_layers.append(nn.Linear(current_dim_enc, next_dim_enc))\n",
    "            if i < num_blocks - 1: enc_layers.append(nn.LayerNorm(next_dim_enc, eps=1e-5)); enc_layers.append(nn.GELU())\n",
    "            current_dim_enc = next_dim_enc\n",
    "        self.encoder_net = nn.Sequential(*enc_layers).to(self.device).to(self.pcgr_internal_dtype)\n",
    "        dec_layers = []; current_dim_dec = self.latent_dim + self.hidden_dim \n",
    "        for i in range(num_blocks):\n",
    "            next_dim_dec = self.hidden_dim\n",
    "            if i < num_blocks - 1 : next_dim_dec = max(self.hidden_dim, (current_dim_dec + self.hidden_dim) // 2 if i < num_blocks -2 else current_dim_dec // 2)\n",
    "            if i == num_blocks - 1: next_dim_dec = self.hidden_dim\n",
    "            dec_layers.append(nn.Linear(current_dim_dec, next_dim_dec))\n",
    "            if i < num_blocks - 1: dec_layers.append(nn.LayerNorm(next_dim_dec, eps=1e-5)); dec_layers.append(nn.GELU())\n",
    "            current_dim_dec = next_dim_dec\n",
    "        self.decoder_net = nn.Sequential(*dec_layers).to(self.device).to(self.pcgr_internal_dtype)\n",
    "        self.task_prototypes: Dict[str, torch.Tensor] = {}; self.batch_counter = 0\n",
    "        self.optimizer = torch.optim.AdamW(list(self.encoder_net.parameters()) + list(self.decoder_net.parameters()), lr=config.replay_model_learning_rate, weight_decay=config.replay_model_weight_decay)\n",
    "\n",
    "    def _tokenize_batch_for_encode(self, text_inputs: List[str]) -> Dict[str, torch.Tensor]: \n",
    "        return self.tokenizer(text_inputs, return_tensors=\"pt\", padding=\"longest\", truncation=True, max_length=self.config.max_seq_length, add_special_tokens=True)\n",
    "\n",
    "    def encode_input_batch(self, tokenized_batch: Dict[str, torch.Tensor]) -> torch.Tensor: \n",
    "        target_device = next(self.backbone_model.parameters()).device\n",
    "        inputs_on_device = {k: v.to(target_device) for k,v in tokenized_batch.items() if torch.is_tensor(v)}\n",
    "        original_mode = self.backbone_model.training;\n",
    "        if hasattr(self.backbone_model, 'eval'): self.backbone_model.eval()\n",
    "        batch_size = inputs_on_device['input_ids'].shape[0]\n",
    "        pooled_output = torch.zeros(batch_size, self.hidden_dim, device=target_device, dtype=self.model_dtype)\n",
    "        try:\n",
    "            with torch.no_grad():\n",
    "                outputs = self.backbone_model(**inputs_on_device, output_hidden_states=True); last_hidden_state = outputs.hidden_states[-1]\n",
    "                attention_mask = inputs_on_device.get('attention_mask', torch.ones_like(last_hidden_state[..., 0])); mask_expanded = attention_mask.unsqueeze(-1).to(last_hidden_state.dtype)\n",
    "                if last_hidden_state.ndim == 3 and mask_expanded.ndim == 3: pooled_output = (last_hidden_state * mask_expanded).sum(dim=1) / torch.clamp(mask_expanded.sum(dim=1), min=1e-9)\n",
    "                else: logger.error(f\"PCGRReplay.encode_input_batch: Unexpected dims. LHS: {last_hidden_state.ndim}, Mask: {mask_expanded.ndim}. Mean fallback.\"); pooled_output = last_hidden_state.mean(dim=1) if last_hidden_state.ndim ==3 else last_hidden_state.mean(dim=0, keepdim=True if last_hidden_state.ndim == 2 else False)\n",
    "        except Exception as e: logger.error(f\"PCGRReplay.encode_input_batch: Error: {e}\")\n",
    "        if hasattr(self.backbone_model, 'train') and original_mode: self.backbone_model.train()\n",
    "        if torch.isnan(pooled_output).any() or torch.isinf(pooled_output).any(): logger.error(f\"PCGRReplay.encode_input_batch NaN/Inf!\"); return torch.zeros_like(pooled_output)\n",
    "        return pooled_output.to(self.model_dtype)\n",
    "\n",
    "    def encode(self, hidden_states: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]: \n",
    "        h_encoded = self.encoder_net(hidden_states.to(self.pcgr_internal_dtype));\n",
    "        if torch.isnan(h_encoded).any() or torch.isinf(h_encoded).any(): logger.error(\"PCGR encode: h_encoded NaN/Inf!\"); zero_l = torch.zeros(hidden_states.size(0), self.latent_dim, device=self.device, dtype=self.pcgr_internal_dtype); return zero_l.clone(), zero_l.clone(), torch.ones_like(zero_l) * -10\n",
    "        mean, logvar = torch.chunk(h_encoded, 2, dim=1); std = torch.exp(0.5 * logvar.clamp(min=-20, max=20));\n",
    "        if torch.isnan(std).any() or torch.isinf(std).any(): logger.error(\"PCGR encode: std NaN/Inf!\"); std = torch.ones_like(std)\n",
    "        eps = torch.randn_like(std); z = mean + eps * std; return z, mean, logvar\n",
    "\n",
    "    def decode(self, z: torch.Tensor, prototype: torch.Tensor) -> torch.Tensor: \n",
    "        prototype_expanded = prototype.to(self.pcgr_internal_dtype).unsqueeze(0).expand(z.size(0), -1)\n",
    "        decoder_input_concat = torch.cat([z, prototype_expanded], dim=1)\n",
    "        decoded_hidden = self.decoder_net(decoder_input_concat)\n",
    "        if torch.isnan(decoded_hidden).any() or torch.isinf(decoded_hidden).any(): logger.error(\"PCGR decode: decoded NaN/Inf!\"); return torch.zeros_like(decoded_hidden, dtype=self.model_dtype)\n",
    "        return decoded_hidden.to(self.model_dtype)\n",
    "\n",
    "    def update_prototype(self, task_id: str, hidden_states_for_update: torch.Tensor): \n",
    "        if hidden_states_for_update.numel() == 0: return\n",
    "        current_task_mean_hidden = hidden_states_for_update.mean(dim=0)\n",
    "        if torch.isnan(current_task_mean_hidden).any() or torch.isinf(current_task_mean_hidden).any(): logger.error(f\"PCGR update_prototype: mean_hidden for {task_id} NaN/Inf!\"); return\n",
    "        if task_id not in self.task_prototypes: self.task_prototypes[task_id] = current_task_mean_hidden.detach()\n",
    "        else: self.task_prototypes[task_id] = (0.9 * self.task_prototypes[task_id] + 0.1 * current_task_mean_hidden).detach()\n",
    "\n",
    "    def train_model(self, task_id: str, raw_text_samples_for_task_buffer: List[Dict[str, str]]) -> None: \n",
    "        if not raw_text_samples_for_task_buffer: return\n",
    "        logger.info(f\"PCGRReplay: Training VAE for task '{task_id}' on {len(raw_text_samples_for_task_buffer)} samples from buffer.\")\n",
    "        num_epochs = 3; internal_vae_batch_size = self.config.replay_model_internal_batch_size\n",
    "        self.optimizer = torch.optim.AdamW(list(self.encoder_net.parameters()) + list(self.decoder_net.parameters()), lr=self.config.replay_model_learning_rate, weight_decay=self.config.replay_model_weight_decay)\n",
    "        self.encoder_net.train().to(self.pcgr_internal_dtype); self.decoder_net.train().to(self.pcgr_internal_dtype)\n",
    "        log_this_call_weights_pcgr = False\n",
    "        if PCGRReplay._first_train_call_ever: log_this_call_weights_pcgr = True; PCGRReplay._first_train_call_ever = False\n",
    "\n",
    "        all_input_texts = [sample['input'] for sample in raw_text_samples_for_task_buffer]\n",
    "        all_original_hidden_states_list = []\n",
    "        # MODIFIED: Use replay_backbone_encoding_batch_size\n",
    "        backbone_encoding_batch_size = self.config.replay_backbone_encoding_batch_size\n",
    "        if backbone_encoding_batch_size == 0 : backbone_encoding_batch_size = 1\n",
    "\n",
    "        for i in range(0, len(all_input_texts), backbone_encoding_batch_size):\n",
    "            batch_texts = all_input_texts[i:i+backbone_encoding_batch_size]\n",
    "            if not batch_texts: continue\n",
    "            tokenized_batch = self._tokenize_batch_for_encode(batch_texts)\n",
    "            hidden_states_batch = self.encode_input_batch(tokenized_batch)\n",
    "            if not (torch.isnan(hidden_states_batch).any() or torch.isinf(hidden_states_batch).any()):\n",
    "                all_original_hidden_states_list.append(hidden_states_batch)\n",
    "        if not all_original_hidden_states_list: logger.warning(f\"PCGRReplay: No valid hidden states for task '{task_id}'. Skipping VAE train.\"); return\n",
    "        all_original_hidden_states = torch.cat(all_original_hidden_states_list, dim=0) \n",
    "        self.update_prototype(task_id, all_original_hidden_states.detach()) \n",
    "        prototype_current = self.task_prototypes.get(task_id)\n",
    "        if prototype_current is None or torch.isnan(prototype_current).any() or torch.isinf(prototype_current).any(): logger.error(f\"PCGR Proto for {task_id} missing or NaN/Inf after update. Cannot train VAE.\"); return\n",
    "        all_original_hidden_states_float32 = all_original_hidden_states.to(self.pcgr_internal_dtype)\n",
    "\n",
    "        overall_loss, overall_recon, overall_kl, overall_batches = 0.0,0.0,0.0,0\n",
    "        for epoch in range(num_epochs):\n",
    "            epoch_loss, epoch_recon, epoch_kl, num_batches_epoch = 0.0,0.0,0.0,0\n",
    "            permuted_indices = torch.randperm(all_original_hidden_states_float32.size(0))\n",
    "            use_tqdm_for_batches = (all_original_hidden_states_float32.size(0) / internal_vae_batch_size) > 5\n",
    "            batch_iterable_pcgr = range(0, all_original_hidden_states_float32.size(0), internal_vae_batch_size)\n",
    "            if use_tqdm_for_batches: batch_iterable_pcgr = tqdm(batch_iterable_pcgr, desc=f\"PCGR VAE Ep{epoch+1} Task:{task_id}\", leave=False)\n",
    "\n",
    "            for i_batch_vae, start_idx_vae in enumerate(batch_iterable_pcgr):\n",
    "                end_idx_vae = min(start_idx_vae+internal_vae_batch_size, all_original_hidden_states_float32.size(0))\n",
    "                batch_indices = permuted_indices[start_idx_vae:end_idx_vae]\n",
    "                if len(batch_indices) == 0: continue\n",
    "                batched_hidden_originals_for_vae = all_original_hidden_states_float32[batch_indices] \n",
    "                self.optimizer.zero_grad();\n",
    "                z_latent, mean_latent, logvar_latent = self.encode(batched_hidden_originals_for_vae.to(self.model_dtype)) # Pass model_dtype\n",
    "                if torch.isnan(z_latent).any() or torch.isinf(z_latent).any(): logger.error(f\"PCGR VAE TRAIN: z_latent NaN/Inf!\"); continue\n",
    "                reconstructed_hidden_batch_model_dtype = self.decode(z_latent, prototype_current)\n",
    "                if torch.isnan(reconstructed_hidden_batch_model_dtype).any() or torch.isinf(reconstructed_hidden_batch_model_dtype).any(): logger.error(f\"PCGR VAE TRAIN: reconstructed_hidden_batch NaN/Inf!\"); continue\n",
    "                recon_loss = F.mse_loss(reconstructed_hidden_batch_model_dtype.to(self.pcgr_internal_dtype), batched_hidden_originals_for_vae.detach())\n",
    "                kl_div = -0.5 * torch.sum(1 + logvar_latent - mean_latent.pow(2) - logvar_latent.exp()); kl_loss = kl_div / batched_hidden_originals_for_vae.size(0)\n",
    "                if torch.isnan(recon_loss).any() or torch.isinf(recon_loss).any() or torch.isnan(kl_loss).any() or torch.isinf(kl_loss).any(): logger.error(f\"PCGR VAE TRAIN: losses NaN/Inf!\"); continue\n",
    "                current_total_loss_batch = recon_loss + self.config.pcgr_kl_weight * kl_loss\n",
    "                if torch.isnan(current_total_loss_batch).any() or torch.isinf(current_total_loss_batch).any(): logger.error(f\"PCGR VAE TRAIN: final batch_loss NaN/Inf!\"); continue\n",
    "                current_total_loss_batch.backward()\n",
    "                torch.nn.utils.clip_grad_norm_(list(self.encoder_net.parameters()) + list(self.decoder_net.parameters()), self.config.replay_model_grad_clip_norm)\n",
    "                self.optimizer.step()\n",
    "                if log_this_call_weights_pcgr and epoch==0 and i_batch_vae==0: logger.info(f\"PCGR Initial WEIGHT DEBUG (Task {task_id}): Weights AFTER first VAE optim step...\")\n",
    "                epoch_loss += current_total_loss_batch.item(); epoch_recon += recon_loss.item(); epoch_kl += kl_loss.item(); num_batches_epoch += 1\n",
    "            overall_loss += epoch_loss; overall_recon += epoch_recon; overall_kl += epoch_kl; overall_batches += num_batches_epoch\n",
    "\n",
    "        avg_loss_pcgr = overall_loss / overall_batches if overall_batches > 0 else float('nan')\n",
    "        avg_recon_pcgr = overall_recon / overall_batches if overall_batches > 0 else float('nan')\n",
    "        avg_kl_pcgr = overall_kl / overall_batches if overall_batches > 0 else float('nan')\n",
    "        logger.info(f\"PCGRReplay VAE training complete for task '{task_id}' ({len(raw_text_samples_for_task_buffer)} buffer samples). AvgLoss:{avg_loss_pcgr:.4f}, AvgRecon:{avg_recon_pcgr:.4f}, AvgKL:{avg_kl_pcgr:.4f}, LR: {self.optimizer.param_groups[0]['lr']:.2e}\")\n",
    "\n",
    "    def generate_samples(self, task_id: str, num_samples: int) -> List[Dict[str, Any]]: \n",
    "        if task_id not in self.task_prototypes: return []\n",
    "        self.decoder_net.eval().to(self.pcgr_internal_dtype); generated_units_list = []\n",
    "        with torch.no_grad():\n",
    "            prototype = self.task_prototypes[task_id].to(self.device)\n",
    "            if torch.isnan(prototype).any() or torch.isinf(prototype).any(): logger.error(f\"PCGRReplay gen: Proto for {task_id} NaN/Inf!\"); return []\n",
    "            for _ in range(num_samples):\n",
    "                z_prior = torch.randn(1, self.latent_dim, device=self.device, dtype=self.pcgr_internal_dtype)\n",
    "                hidden_reconstructed = self.decode(z_prior, prototype)\n",
    "                if torch.isnan(hidden_reconstructed).any() or torch.isinf(hidden_reconstructed).any(): logger.error(f\"PCGRReplay gen: Decoded hidden NaN/Inf for {task_id}!\"); continue\n",
    "                generated_units_list.append({\"hidden_state\": hidden_reconstructed.cpu().squeeze(0), \"task_id\": task_id, \"is_generated\": True})\n",
    "        return generated_units_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_xBEpv0Om0RP"
   },
   "source": [
    "## Main Function - Example Experiment\n",
    "\n",
    "Here we set up a simple experiment to demonstrate the adaptive learning capabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "68wCpHIHm0RP"
   },
   "outputs": [],
   "source": [
    "# Cell 16 (Main Function, load_benchmark_task, CausalLMTrainingDataset, EWCDataset - WITH SharedLoRA & EWC DataLoader fix & Adapter Name Sanitization & Early Stopping)\n",
    "\n",
    "from rouge_score import rouge_scorer\n",
    "from torch.utils.data import Dataset as TorchDataset, DataLoader as TorchDataLoader\n",
    "from datasets import load_dataset, DownloadMode, concatenate_datasets\n",
    "import gc\n",
    "import time\n",
    "import numpy as np\n",
    "import torch # Ensure torch is imported for torch.tensor\n",
    "import torch.nn.functional as F\n",
    "from typing import List, Dict, Any, Tuple, Optional, Union\n",
    "from peft import PeftModel\n",
    "import matplotlib.pyplot as plt # Ensure pyplot is imported for neuromod_manager.plot_gamma_gain\n",
    "\n",
    "# Ensure 'logger' is defined globally (e.g., Cell 3 or 4)\n",
    "if 'logger' not in globals():\n",
    "    import logging\n",
    "    logger = logging.getLogger(__name__ + \".main_experiment_cell16\")\n",
    "    if not logger.hasHandlers():\n",
    "        handler = logging.StreamHandler()\n",
    "        formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "        handler.setFormatter(formatter)\n",
    "        logger.addHandler(handler)\n",
    "        logger.setLevel(logging.INFO)\n",
    "\n",
    "# --- Causal LM Training Dataset ---\n",
    "class CausalLMTrainingDataset(TorchDataset):\n",
    "    def __init__(self, examples: List[Dict[str, str]], tokenizer: 'PreTrainedTokenizer', max_seq_length: int):\n",
    "        self.examples = examples; self.tokenizer = tokenizer; self.max_seq_length = max_seq_length\n",
    "        if self.tokenizer.pad_token_id is None: self.tokenizer.pad_token_id = self.tokenizer.eos_token_id if self.tokenizer.eos_token_id is not None else 0\n",
    "    def __len__(self): return len(self.examples)\n",
    "    def __getitem__(self, idx):\n",
    "        example = self.examples[idx]; input_text, target_text_response = example['input'], example['output']\n",
    "        target_text_response_str = str(target_text_response) if target_text_response is not None else \"\"\n",
    "        tokenized_prompt = self.tokenizer(input_text, add_special_tokens=True, truncation=False, padding=False)\n",
    "        response_prefix = \" \" if not input_text.endswith(\" \") and target_text_response_str else \"\"\n",
    "        tokenized_response = self.tokenizer(response_prefix + target_text_response_str + self.tokenizer.eos_token, add_special_tokens=False, truncation=False, padding=False)\n",
    "        prompt_ids, response_ids = tokenized_prompt.input_ids, tokenized_response.input_ids\n",
    "        combined_ids_list = prompt_ids + response_ids; effective_prompt_len = len(prompt_ids)\n",
    "        if len(combined_ids_list) > self.max_seq_length:\n",
    "            if effective_prompt_len >= self.max_seq_length:\n",
    "                 combined_ids_list = prompt_ids[:self.max_seq_length]; effective_prompt_len = self.max_seq_length\n",
    "            else: combined_ids_list = combined_ids_list[:self.max_seq_length]\n",
    "        input_ids_tensor = torch.tensor(combined_ids_list, dtype=torch.long)\n",
    "        labels = torch.full_like(input_ids_tensor, -100)\n",
    "        if effective_prompt_len < len(input_ids_tensor): labels[effective_prompt_len:] = input_ids_tensor[effective_prompt_len:]\n",
    "        attention_mask = torch.ones_like(input_ids_tensor)\n",
    "        if len(input_ids_tensor) < self.max_seq_length:\n",
    "            padding_length = self.max_seq_length - len(input_ids_tensor)\n",
    "            input_ids_tensor = F.pad(input_ids_tensor, (0, padding_length), value=self.tokenizer.pad_token_id)\n",
    "            attention_mask = F.pad(attention_mask, (0, padding_length), value=0)\n",
    "            labels = F.pad(labels, (0, padding_length), value=-100)\n",
    "        return {\"input_ids\": input_ids_tensor, \"attention_mask\": attention_mask, \"labels\": labels,\n",
    "                \"raw_input_text\": input_text, \"raw_output_text\": target_text_response_str}\n",
    "\n",
    "# --- Benchmark Task Loading Utility ---\n",
    "def load_benchmark_task(\n",
    "    dataset_name: str, tokenizer: 'PreTrainedTokenizer', text_field: Union[str, List[str]], label_field: str, num_train_samples: int, num_val_samples: int,\n",
    "    task_id_prefix: str = \"bench_\", class_names_map: Optional[Dict[int, str]] = None, random_seed: int = 42,\n",
    "    config_name: Optional[str] = None, max_seq_length: int = 512\n",
    ") -> Tuple[List[Dict[str,str]], List[Dict[str,str]], Dict[int,str], Union[str, List[str]]]:\n",
    "    logger.info(f\"Loading benchmark task: {dataset_name} (config: {config_name or 'default'}) {num_train_samples} train, {num_val_samples} val.\")\n",
    "    dataset = None; error_messages = []\n",
    "    current_default_config = globals().get('default_config')\n",
    "    output_dir_for_cache = current_default_config.output_dir if current_default_config and hasattr(current_default_config, 'output_dir') else \"./outputs_cache_fallback\"\n",
    "    hf_cache_dir = os.path.join(output_dir_for_cache, \".cache\", \"huggingface\", \"datasets\"); os.makedirs(hf_cache_dir, exist_ok=True)\n",
    "    load_kwargs = {\"name\": config_name, \"cache_dir\": hf_cache_dir, \"trust_remote_code\": True} if config_name else {\"cache_dir\": hf_cache_dir, \"trust_remote_code\": True}\n",
    "    try: dataset = load_dataset(dataset_name, **load_kwargs)\n",
    "    except Exception as e1: error_messages.append(f\"Plain load failed: {e1}\"); logger.warning(f\"Plain load {dataset_name} (config: {config_name}) failed: {e1}\"); dataset = None\n",
    "    if dataset is None:\n",
    "        try: dataset = load_dataset(dataset_name, download_mode=DownloadMode.FORCE_REDOWNLOAD, **load_kwargs)\n",
    "        except Exception as e2: error_messages.append(f\"Force redownload failed: {e2}\"); logger.error(f\"All load attempts for {dataset_name} (config: {config_name}) failed: {error_messages}\"); return [], [], {}, text_field\n",
    "    if dataset is None: logger.error(f\"Dataset {dataset_name} (config: {config_name}) could not be loaded.\"); return [], [], {}, text_field\n",
    "    train_split_name_used, val_split_name_used = \"train\", \"validation\"; train_ds_full, val_ds_full = None, None\n",
    "    if \"train\" not in dataset: logger.error(f\"'train' split not found. Available: {list(dataset.keys())}\"); return [], [], {}, text_field\n",
    "    else: train_ds_full = dataset[\"train\"]\n",
    "    if \"validation\" in dataset: val_ds_full = dataset[\"validation\"]; val_split_name_used = \"validation\"\n",
    "    elif \"test\" in dataset: val_ds_full = dataset[\"test\"]; val_split_name_used = \"test\";\n",
    "    elif len(dataset[\"train\"]) > (num_train_samples + num_val_samples):\n",
    "        current_train_len = len(dataset[\"train\"])\n",
    "        val_samples_from_train = max(1, int(current_train_len * 0.1)) if current_train_len <= (num_train_samples + num_val_samples) * 2 else num_val_samples\n",
    "        if current_train_len <= val_samples_from_train : logger.error(f\"Cannot split train: Not enough samples ({current_train_len}) for val ({val_samples_from_train}).\"); return [], [], {}, text_field\n",
    "        train_val_split = dataset[\"train\"].train_test_split(test_size=val_samples_from_train, seed=random_seed, shuffle=True)\n",
    "        train_ds_full, val_ds_full, val_split_name_used = train_val_split[\"train\"], train_val_split[\"test\"], \"train_split_for_val\"\n",
    "    else: logger.error(f\"Cannot obtain validation data for {dataset_name}. Train size: {len(train_ds_full) if train_ds_full else 'N/A'}\"); return [], [], {}, text_field\n",
    "    actual_num_train = min(num_train_samples, len(train_ds_full)) if train_ds_full else 0\n",
    "    actual_num_val = min(num_val_samples, len(val_ds_full)) if val_ds_full else 0\n",
    "    train_ds = train_ds_full.select(range(actual_num_train)) if actual_num_train > 0 and train_ds_full else (train_ds_full.select([]) if train_ds_full else None)\n",
    "    val_ds = val_ds_full.select(range(actual_num_val)) if actual_num_val > 0 and val_ds_full else (val_ds_full.select([]) if val_ds_full else None)\n",
    "    final_class_names_map = {}\n",
    "    if class_names_map: final_class_names_map = class_names_map\n",
    "    elif train_ds and label_field in train_ds.features and hasattr(train_ds.features[label_field], 'names') and train_ds.features[label_field].names: final_class_names_map = {i: name for i, name in enumerate(train_ds.features[label_field].names)}\n",
    "    elif train_ds and label_field in train_ds.column_names and len(train_ds) > 0:\n",
    "        unique_labels_original = sorted(list(set(train_ds[label_field])))\n",
    "        is_std_int = all(isinstance(l, int) and l >= 0 for l in unique_labels_original) and len(unique_labels_original) > 0 and max(unique_labels_original) == len(unique_labels_original) -1 and min(unique_labels_original) == 0\n",
    "        if is_std_int: final_class_names_map = {i: str(i) for i in unique_labels_original}\n",
    "        else: final_class_names_map = {i: str(ul) for i, ul in enumerate(unique_labels_original)}\n",
    "    if not final_class_names_map:\n",
    "        ds_name_lower, cfg_name_lower = dataset_name.lower(), (config_name.lower() if config_name else \"\")\n",
    "        if ds_name_lower == \"glue\":\n",
    "            if cfg_name_lower == \"sst2\": final_class_names_map = {0: \"negative\", 1: \"positive\"}\n",
    "            elif cfg_name_lower == \"mrpc\": final_class_names_map = {0: \"not_equivalent\", 1: \"equivalent\"}\n",
    "            elif cfg_name_lower == \"qnli\": final_class_names_map = {0: \"entailment\", 1: \"not_entailment\"}\n",
    "            elif cfg_name_lower == \"rte\": final_class_names_map = {0: \"entailment\", 1: \"not_entailment\"}\n",
    "            elif cfg_name_lower == \"cola\": final_class_names_map = {0: \"unacceptable\", 1: \"acceptable\"}\n",
    "    def _format_examples(ds_split, current_task_id_prefix_fmt, current_ds_name_fmt, current_cfg_name_fmt, current_class_map, original_text_field_name):\n",
    "        if ds_split is None: return []\n",
    "        formatted_examples = []\n",
    "        raw_task_identifier_for_example = f\"{current_task_id_prefix_fmt}{current_ds_name_fmt}_{current_cfg_name_fmt or 'default'}\"\n",
    "        for ex_idx, ex in enumerate(ds_split):\n",
    "            input_val_combined = None\n",
    "            original_example_data_for_prototype = ex\n",
    "            if isinstance(original_text_field_name, list): input_parts = [str(ex.get(tf, \"\")) for tf in original_text_field_name]; input_val_combined = \" [SEP] \".join(input_parts)\n",
    "            else: input_val_combined = str(ex.get(original_text_field_name, \"\"))\n",
    "            original_label_val = ex.get(label_field)\n",
    "            if input_val_combined is None or original_label_val is None: continue\n",
    "            output_val_str = str(original_label_val)\n",
    "            if current_class_map and isinstance(original_label_val, int) and original_label_val in current_class_map: output_val_str = current_class_map[original_label_val]\n",
    "            elif not current_class_map and isinstance(original_label_val, int): output_val_str = str(original_label_val)\n",
    "            formatted_examples.append({\"input\": input_val_combined, \"output\": output_val_str, \"task_id\": raw_task_identifier_for_example, \"original_example_data\": original_example_data_for_prototype })\n",
    "        return formatted_examples\n",
    "    train_examples = _format_examples(train_ds, task_id_prefix, dataset_name, config_name, final_class_names_map, text_field)\n",
    "    val_examples = _format_examples(val_ds, task_id_prefix, dataset_name, config_name, final_class_names_map, text_field)\n",
    "    logger.info(f\"Loaded {len(train_examples)} train, {len(val_examples)} val for {dataset_name} (config: {config_name}). Classes: {final_class_names_map if final_class_names_map else 'N/A (raw labels used)'}\")\n",
    "    if not train_examples and num_train_samples > 0 : logger.warning(f\"No train examples formatted for {dataset_name}\")\n",
    "    if not val_examples and num_val_samples > 0 : logger.warning(f\"No val examples formatted for {dataset_name}\")\n",
    "    return train_examples, val_examples, final_class_names_map, text_field\n",
    "\n",
    "# --- EWC Dataset Definition ---\n",
    "class EWCDataset(TorchDataset):\n",
    "    def __init__(self, raw_samples: List[Dict[str, str]], tokenizer: 'PreTrainedTokenizer', max_seq_length: int):\n",
    "        self.raw_samples = raw_samples; self.tokenizer = tokenizer; self.max_seq_length = max_seq_length\n",
    "        if self.tokenizer.pad_token_id is None: self.tokenizer.pad_token_id = self.tokenizer.eos_token_id if self.tokenizer.eos_token_id is not None else 0\n",
    "    def __len__(self): return len(self.raw_samples)\n",
    "    def __getitem__(self, idx):\n",
    "        raw_sample = self.raw_samples[idx]; input_text, target_text_response = raw_sample['input'], raw_sample['output']\n",
    "        target_text_response_str = str(target_text_response) if target_text_response is not None else \"\"\n",
    "        tokenized_prompt = self.tokenizer(input_text, add_special_tokens=True, truncation=False, padding=False)\n",
    "        response_prefix = \" \" if not input_text.endswith(\" \") and target_text_response_str else \"\"\n",
    "        tokenized_response = self.tokenizer(response_prefix + target_text_response_str + self.tokenizer.eos_token, add_special_tokens=False, truncation=False, padding=False)\n",
    "        prompt_ids, response_ids = tokenized_prompt.input_ids, tokenized_response.input_ids\n",
    "        combined_ids_list = prompt_ids + response_ids; effective_prompt_len = len(prompt_ids)\n",
    "        if len(combined_ids_list) > self.max_seq_length:\n",
    "            if effective_prompt_len >= self.max_seq_length: combined_ids_list = prompt_ids[:self.max_seq_length]; effective_prompt_len = self.max_seq_length\n",
    "            else: combined_ids_list = combined_ids_list[:self.max_seq_length]\n",
    "        input_ids_tensor = torch.tensor(combined_ids_list, dtype=torch.long)\n",
    "        labels = torch.full_like(input_ids_tensor, -100)\n",
    "        if effective_prompt_len < len(input_ids_tensor): labels[effective_prompt_len:] = input_ids_tensor[effective_prompt_len:]\n",
    "        attention_mask = torch.ones_like(input_ids_tensor)\n",
    "        if len(input_ids_tensor) < self.max_seq_length:\n",
    "            padding_length = self.max_seq_length - len(input_ids_tensor)\n",
    "            input_ids_tensor = F.pad(input_ids_tensor, (0, padding_length), value=self.tokenizer.pad_token_id)\n",
    "            attention_mask = F.pad(attention_mask, (0, padding_length), value=0)\n",
    "            labels = F.pad(labels, (0, padding_length), value=-100)\n",
    "        return {\"input_ids\": input_ids_tensor, \"attention_mask\": attention_mask, \"labels\": labels}\n",
    "\n",
    "def main(run_config: Optional['AdaptiveLearnerConfig'] = None):\n",
    "    config = run_config if run_config else globals().get('default_config')\n",
    "    if config is None: logger.error(\"CRITICAL: No configuration provided to main() and global default_config not found.\"); return\n",
    "\n",
    "    logger.info(f\"--- Main function started. SFT Mode: {config.run_sft_baseline}. Exp Tag: {config.experiment_tag} ---\\n\"\n",
    "                f\"Router type: {config.router_type}, Confidence threshold: {config.router_confidence_threshold}\\n\"\n",
    "                f\"Load router from tag: {config.load_router_state_from_tag}\\n\"\n",
    "                f\"k_examples_for_prototype: {getattr(config, 'k_examples_for_prototype', 'Not Set')}, \"\n",
    "                f\"Use Advanced Embeddings: {getattr(config, 'use_advanced_embeddings', 'Not Set')}\\n\"\n",
    "                f\"Use LoRA Early Stopping: {getattr(config, 'use_lora_early_stopping', 'Not Set')}\")\n",
    "\n",
    "    logger.info(\"Initializing components...\")\n",
    "    backbone = AdaptiveLearnerBackbone(config)\n",
    "    peft_manager = PEFTManager(config, backbone.model)\n",
    "    neuromod_manager = NeuromodulationManager(config)\n",
    "    consolidation_manager = ConsolidationManager(config)\n",
    "    replay_manager = GenerativeReplayManager(config, peft_manager.get_current_peft_model(), backbone.tokenizer)\n",
    "\n",
    "    sft_originals = {}\n",
    "    if config.run_sft_baseline:\n",
    "        logger.info(\"--- CONFIGURING FOR SFT BASELINE RUN ---\")\n",
    "        sft_originals = {\n",
    "            'replay_alpha': config.replay_alpha, 'feature_replay_alpha': config.feature_replay_alpha,\n",
    "            'consolidation_method': config.consolidation_method, 'router_confidence_threshold': config.router_confidence_threshold,\n",
    "            'replay_method': config.replay_method, 'experiment_tag': config.experiment_tag,\n",
    "            'load_router_state_from_tag': config.load_router_state_from_tag,\n",
    "            'k_examples_for_prototype': getattr(config, 'k_examples_for_prototype', 0),\n",
    "            'use_advanced_embeddings': getattr(config, 'use_advanced_embeddings', False),\n",
    "            'num_tasks_to_share_lora': getattr(config, 'num_tasks_to_share_lora', 0),\n",
    "            'use_lora_early_stopping': getattr(config, 'use_lora_early_stopping', False)\n",
    "        }\n",
    "        config.replay_alpha, config.feature_replay_alpha, config.consolidation_method = 0.0, 0.0, \"none\"\n",
    "        config.router_confidence_threshold = 1.1; config.replay_method = \"none\"\n",
    "        config.experiment_tag = f\"{sft_originals.get('experiment_tag', 'sft_baseline')}_SFT_RUN\"\n",
    "        config.load_router_state_from_tag = None; config.k_examples_for_prototype = 0\n",
    "        config.use_advanced_embeddings = False; config.num_tasks_to_share_lora = 0\n",
    "        config.use_lora_early_stopping = False\n",
    "        consolidation_manager.config = config; replay_manager.config = config\n",
    "        if replay_manager.replay_model is not None: logger.info(\"SFT: Disabling replay model.\"); replay_manager.replay_model = None\n",
    "        if peft_manager.router is not None: peft_manager.router.config = config\n",
    "\n",
    "    tasks_to_process = []\n",
    "    if config.benchmark_tasks_to_run:\n",
    "        for task_def in config.benchmark_tasks_to_run:\n",
    "            train_ex, val_ex, class_map, original_task_text_field = load_benchmark_task(\n",
    "                dataset_name=task_def[\"name\"], tokenizer=backbone.tokenizer, text_field=task_def[\"text_field\"],\n",
    "                label_field=task_def[\"label_field\"], num_train_samples=task_def.get(\"num_train_samples\", config.num_train_samples_benchmark),\n",
    "                num_val_samples=task_def.get(\"num_val_samples\", config.num_val_samples_benchmark),\n",
    "                task_id_prefix=task_def.get(\"id_prefix\", \"bench_\"), class_names_map=task_def.get(\"class_names_map\"),\n",
    "                random_seed=42, config_name=task_def.get(\"config\"), max_seq_length=config.max_seq_length)\n",
    "            if train_ex:\n",
    "                raw_task_id = f\"{task_def.get('id_prefix', 'bench_')}{task_def['name']}_{task_def.get('config', 'default')}\"\n",
    "                tasks_to_process.append({\"name\": raw_task_id, \"description\": f\"Benchmark: {task_def['name']} ({task_def.get('config', 'default')}) Task Description: {task_def.get('description_for_router', task_def['name'])}\",\n",
    "                    \"train_examples\": train_ex, \"val_examples\": val_ex, \"class_names_map_from_load\": class_map, \"original_task_text_field\": original_task_text_field })\n",
    "            else: logger.error(f\"Failed to load/format benchmark task: {task_def['name']} (config: {task_def.get('config')}). Skipping.\")\n",
    "    if not tasks_to_process:\n",
    "        logger.error(\"CRITICAL: No tasks to process. Exiting.\")\n",
    "        if config.use_wandb and 'wandb' in globals() and wandb.run: wandb.finish()\n",
    "        return\n",
    "\n",
    "    all_task_val_sets: Dict[str, List[Dict[str,str]]] = {}; accuracy_matrix: Dict[Tuple[int, int], float] = {}\n",
    "    task_id_to_matrix_idx: Dict[str, int] = {}; processed_task_info_for_eval = []\n",
    "    cumulative_global_steps = 0; num_epochs_lora_train = config.num_lora_train_epochs\n",
    "\n",
    "    num_tasks_to_share_lora = getattr(config, 'num_tasks_to_share_lora', 0)\n",
    "    shared_lora_adapter_id_actual = \"\"\n",
    "\n",
    "    for task_index, task_content_from_list in enumerate(tasks_to_process):\n",
    "        raw_current_task_id = task_content_from_list[\"name\"]; train_examples_raw = task_content_from_list[\"train_examples\"]\n",
    "        val_examples_raw = task_content_from_list[\"val_examples\"]; task_description_for_router = task_content_from_list[\"description\"]\n",
    "        current_task_class_names_map = task_content_from_list.get(\"class_names_map_from_load\")\n",
    "        current_task_original_text_field = task_content_from_list[\"original_task_text_field\"]\n",
    "        if not current_task_class_names_map and (train_examples_raw or val_examples_raw):\n",
    "            unique_outputs = sorted(list(set(ex['output'] for ex in train_examples_raw + val_examples_raw if 'output' in ex)));\n",
    "            current_task_class_names_map = {i: name for i, name in enumerate(unique_outputs)}\n",
    "        task_id_to_matrix_idx[raw_current_task_id] = task_index; all_task_val_sets[raw_current_task_id] = val_examples_raw\n",
    "        \n",
    "        logger.info(f\"\\\\n--- Starting Task {task_index+1}/{len(tasks_to_process)}: {raw_current_task_id} (Exp Tag: {config.experiment_tag}) ---\\\\n\"\n",
    "                    f\"  Task Description for Router: '{task_description_for_router[:100]}...'\")\n",
    "\n",
    "        if not train_examples_raw: logger.warning(f\"Task {raw_current_task_id} has no training examples. Skipping training phase.\");\n",
    "        else: logger.info(f\"  Train: {len(train_examples_raw)} examples, Val: {len(val_examples_raw)} examples, Classes: {current_task_class_names_map}, Max LoRA Epochs: {num_epochs_lora_train}\")\n",
    "\n",
    "        active_lora_to_train_tagged = \"\"; current_peft_model = peft_manager.get_current_peft_model()\n",
    "        examples_for_prototype_decision: Optional[List[Dict[str,Any]]] = None\n",
    "        if hasattr(config, 'k_examples_for_prototype') and config.k_examples_for_prototype > 0 and train_examples_raw:\n",
    "            examples_for_prototype_decision = [ex['original_example_data'] for ex in train_examples_raw[:config.k_examples_for_prototype] if 'original_example_data' in ex and isinstance(ex['original_example_data'], dict)]\n",
    "            if not examples_for_prototype_decision : logger.warning(f\"No valid 'original_example_data' found for prototype for task {raw_current_task_id}\")\n",
    "\n",
    "        is_shared_lora_task = (num_tasks_to_share_lora > 0 and task_index < num_tasks_to_share_lora)\n",
    "\n",
    "        if config.run_sft_baseline:\n",
    "            active_lora_to_train_tagged = config.sft_lora_id\n",
    "            if task_index == 0 or not (isinstance(peft_manager.model, PeftModel) and active_lora_to_train_tagged in peft_manager.model.peft_config):\n",
    "                current_peft_model = peft_manager.create_lora_module(active_lora_to_train_tagged)\n",
    "            else: peft_manager.activate_lora_modules(active_lora_to_train_tagged); current_peft_model = peft_manager.get_current_peft_model()\n",
    "        elif is_shared_lora_task:\n",
    "            if not shared_lora_adapter_id_actual:\n",
    "                shared_lora_base_name = \"SharedAdapter\"\n",
    "                task_name_parts = []\n",
    "                for i in range(num_tasks_to_share_lora):\n",
    "                    if i < len(tasks_to_process):\n",
    "                        task_id_part = tasks_to_process[i][\"name\"]\n",
    "                        meaningful_part = task_id_part.split('_')[-1] if '_' in task_id_part else task_id_part\n",
    "                        if \"glue\" in meaningful_part and len(task_id_part.split('_')) > 1:\n",
    "                            meaningful_part = task_id_part.split('_')[-1] if task_id_part.split('_')[-1] != \"glue\" else task_id_part.split('_')[-2]\n",
    "                        task_name_parts.append(meaningful_part[:4].replace(\".\", \"_\"))\n",
    "                shared_lora_base_name += \"_\" + \"_\".join(task_name_parts)\n",
    "                clean_experiment_tag = config.experiment_tag.replace(\".\", \"_\") if config.experiment_tag else \"default_exp\"\n",
    "                shared_lora_adapter_id_actual = f\"{clean_experiment_tag}_{shared_lora_base_name}\".replace(\".\", \"_\")\n",
    "                logger.info(f\"SharedLoRA: Task {task_index} ('{raw_current_task_id}') is the first shared task. Creating shared LoRA: '{shared_lora_adapter_id_actual}'\")\n",
    "                current_peft_model = peft_manager.create_lora_module(shared_lora_adapter_id_actual)\n",
    "                active_lora_to_train_tagged = shared_lora_adapter_id_actual\n",
    "            else:\n",
    "                logger.info(f\"SharedLoRA: Task {task_index} ('{raw_current_task_id}') reuses shared LoRA: '{shared_lora_adapter_id_actual}'\")\n",
    "                if not (isinstance(peft_manager.model, PeftModel) and shared_lora_adapter_id_actual in peft_manager.model.peft_config):\n",
    "                    logger.error(f\"SharedLoRA: Attempted to reuse '{shared_lora_adapter_id_actual}' but not found in model config. Creating it now.\")\n",
    "                    current_peft_model = peft_manager.create_lora_module(shared_lora_adapter_id_actual)\n",
    "                else: peft_manager.activate_lora_modules(shared_lora_adapter_id_actual)\n",
    "                current_peft_model = peft_manager.get_current_peft_model(); active_lora_to_train_tagged = shared_lora_adapter_id_actual\n",
    "        else:\n",
    "            potential_loras = peft_manager.select_lora_for_input(task_description=task_description_for_router, task_examples_for_prototype=examples_for_prototype_decision, task_text_field=current_task_original_text_field, top_k=1)\n",
    "            selected_existing = False\n",
    "            if potential_loras:\n",
    "                router_sel_id_tagged, conf = potential_loras[0]\n",
    "                if isinstance(peft_manager.model, PeftModel) and router_sel_id_tagged in peft_manager.model.peft_config and conf > config.router_confidence_threshold :\n",
    "                    active_lora_to_train_tagged = router_sel_id_tagged; peft_manager.activate_lora_modules(active_lora_to_train_tagged); current_peft_model = peft_manager.get_current_peft_model(); selected_existing = True; logger.info(f\"Router REUSED existing LoRA '{active_lora_to_train_tagged}' (Conf: {conf:.3f}) for raw task '{raw_current_task_id}'.\")\n",
    "                elif conf > config.router_confidence_threshold:\n",
    "                    loaded_model = peft_manager.load_lora_module(router_sel_id_tagged, set_as_active=True)\n",
    "                    if loaded_model: active_lora_to_train_tagged = router_sel_id_tagged; current_peft_model = loaded_model; selected_existing = True; logger.info(f\"Router REUSED (loaded) LoRA '{active_lora_to_train_tagged}' (Conf: {conf:.3f}) for raw task '{raw_current_task_id}'.\")\n",
    "                    else: logger.warning(f\"Router suggested reusing '{router_sel_id_tagged}' but it could not be loaded. Creating new.\"); selected_existing = False\n",
    "            if not selected_existing:\n",
    "                clean_experiment_tag = config.experiment_tag.replace(\".\", \"_\") if config.experiment_tag else \"default_exp\"\n",
    "                clean_raw_current_task_id = raw_current_task_id.replace(\".\", \"_\")\n",
    "                tag_prefix = f\"{clean_experiment_tag}_\"\n",
    "                active_lora_to_train_tagged = f\"{tag_prefix}{clean_raw_current_task_id}\".replace(\".\", \"_\")\n",
    "                current_peft_model = peft_manager.create_lora_module(active_lora_to_train_tagged)\n",
    "                logger.info(f\"Router/NewLogic: Using NEW LoRA '{active_lora_to_train_tagged}' for raw task '{raw_current_task_id}'. Router top choice (if any): {potential_loras}\")\n",
    "\n",
    "        processed_task_info_for_eval.append({\"id\": raw_current_task_id, \"class_names_map\": current_task_class_names_map,\n",
    "            \"num_train_examples\": len(train_examples_raw), \"lora_trained_on_this_task\": active_lora_to_train_tagged,\n",
    "            \"task_description_for_router\": task_description_for_router, \"original_task_text_field\": current_task_original_text_field })\n",
    "        replay_manager.backbone_model = current_peft_model\n",
    "        trainable_params = [p for p in current_peft_model.parameters() if p.requires_grad]; optimizer = None\n",
    "        if trainable_params and train_examples_raw: optimizer = torch.optim.AdamW(trainable_params, lr=config.learning_rate, weight_decay=config.weight_decay)\n",
    "\n",
    "        if train_examples_raw and optimizer:\n",
    "            current_task_train_dataset = CausalLMTrainingDataset(train_examples_raw, backbone.tokenizer, config.max_seq_length)\n",
    "            current_num_workers = getattr(config, 'num_workers', 0)\n",
    "            current_task_train_dataloader = TorchDataLoader(current_task_train_dataset, batch_size=config.batch_size, shuffle=True, num_workers=current_num_workers, pin_memory=(config.device==\"cuda\"))\n",
    "            task_samples_for_replay_model_training_this_task = []\n",
    "\n",
    "            best_val_metric_for_early_stop = -float('inf') if config.lora_early_stopping_metric == \"accuracy\" else float('inf')\n",
    "            early_stop_patience_counter = 0\n",
    "            stopped_early_this_task = False\n",
    "            actual_epochs_run_for_task = 0\n",
    "\n",
    "            for epoch_num in range(num_epochs_lora_train):\n",
    "                actual_epochs_run_for_task += 1\n",
    "                logger.info(f\"  Task {raw_current_task_id} - LoRA Training Epoch {epoch_num+1}/{num_epochs_lora_train} for LoRA: {active_lora_to_train_tagged}\")\n",
    "                epoch_losses, epoch_accuracies, epoch_grad_norms, epoch_entropies = [], [], [], []\n",
    "                batch_iterator_desc = f\"Epoch {epoch_num+1} LoRA:{active_lora_to_train_tagged[:20]} Task:{raw_current_task_id[:15]}\"\n",
    "                batch_iterator = tqdm(current_task_train_dataloader, desc=batch_iterator_desc, leave=False) if len(current_task_train_dataloader) > 1 else current_task_train_dataloader\n",
    "                for batch_idx, batch_data in enumerate(batch_iterator):\n",
    "                    global_step_for_wandb = cumulative_global_steps + ((actual_epochs_run_for_task -1) * len(current_task_train_dataloader)) + batch_idx\n",
    "                    current_batch_raw_samples_list = [{\"input\": batch_data[\"raw_input_text\"][i], \"output\": batch_data[\"raw_output_text\"][i]} for i in range(len(batch_data[\"input_ids\"]))]\n",
    "                    if epoch_num == 0 : task_samples_for_replay_model_training_this_task.extend(current_batch_raw_samples_list)\n",
    "                    loss_val, acc_metric_batch_avg, grad_n, entr_val = 5.0, 0.0, 0.0, 3.0\n",
    "                    sup_rep_n, sup_rep_loss_val = 0,0.0; feat_rep_n, feat_rep_loss_val = 0,0.0\n",
    "                    current_peft_model.train(); optimizer.zero_grad()\n",
    "                    ids_curr, attn_curr, lbls_curr = batch_data[\"input_ids\"].to(config.device), batch_data[\"attention_mask\"].to(config.device), batch_data[\"labels\"].to(config.device)\n",
    "                    outputs = current_peft_model(input_ids=ids_curr, attention_mask=attn_curr, labels=lbls_curr)\n",
    "                    loss_curr = outputs.loss\n",
    "                    loss_curr = torch.tensor(5.0, device=config.device, dtype=torch.float32) if loss_curr is None or not torch.isfinite(loss_curr) else loss_curr\n",
    "                    loss_val = loss_curr.item()\n",
    "                    tot_sup_loss, tot_feat_loss = torch.tensor(0.0, device=config.device, dtype=loss_curr.dtype), torch.tensor(0.0, device=config.device, dtype=loss_curr.dtype)\n",
    "                    if not config.run_sft_baseline and task_index > 0 and (config.replay_alpha > 0 or config.feature_replay_alpha > 0) and replay_manager.replay_model :\n",
    "                        avail_replay_tasks = replay_manager.get_tasks_with_replay_data(exclude_task_id=raw_current_task_id)\n",
    "                        if avail_replay_tasks:\n",
    "                            replay_task_id_raw = np.random.choice(avail_replay_tasks); num_replay_samples_this_step = max(1, ids_curr.size(0) // 2)\n",
    "                            if config.replay_alpha > 0 and np.random.rand() < 0.7:\n",
    "                                raw_re_ex_list = replay_manager.get_raw_samples_for_replay(replay_task_id_raw, num_samples=num_replay_samples_this_step)\n",
    "                                if raw_re_ex_list:\n",
    "                                    replay_dataset_temp = CausalLMTrainingDataset(raw_re_ex_list, backbone.tokenizer, config.max_seq_length)\n",
    "                                    current_replay_batch_size = min(len(raw_re_ex_list), config.batch_size)\n",
    "                                    replay_dl_temp = TorchDataLoader(replay_dataset_temp, batch_size=current_replay_batch_size, num_workers=config.num_workers)\n",
    "                                    for replay_batch_data in replay_dl_temp:\n",
    "                                        out_re_sup = current_peft_model(input_ids=replay_batch_data[\"input_ids\"].to(config.device), attention_mask=replay_batch_data[\"attention_mask\"].to(config.device), labels=replay_batch_data[\"labels\"].to(config.device))\n",
    "                                        if out_re_sup.loss is not None and torch.isfinite(out_re_sup.loss): tot_sup_loss += out_re_sup.loss * len(raw_re_ex_list); sup_rep_n += len(raw_re_ex_list)\n",
    "                                        break\n",
    "                            elif config.feature_replay_alpha > 0 and hasattr(replay_manager.replay_model, 'generate_samples'):\n",
    "                                gen_units = replay_manager.generate_replay_units(replay_task_id_raw, num_units=num_replay_samples_this_step)\n",
    "                                if gen_units:\n",
    "                                    h_past_batch = torch.stack([unit['hidden_state'] for unit in gen_units]).to(config.device).to(current_peft_model.dtype if hasattr(current_peft_model, 'dtype') else torch.float16)\n",
    "                                    if hasattr(replay_manager.replay_model, '_tokenize_batch_for_encode') and hasattr(replay_manager.replay_model, 'encode_input_batch'):\n",
    "                                        current_texts_for_feat_replay = [batch_data[\"raw_input_text\"][i] for i in range(min(num_replay_samples_this_step, len(batch_data[\"raw_input_text\"])))]\n",
    "                                        if current_texts_for_feat_replay:\n",
    "                                            tok_curr_feat_batch = replay_manager.replay_model._tokenize_batch_for_encode(current_texts_for_feat_replay)\n",
    "                                            h_curr_batch = replay_manager.replay_model.encode_input_batch(tok_curr_feat_batch)\n",
    "                                            h_curr_batch = h_curr_batch.to(config.device).to(h_past_batch.dtype)\n",
    "                                            if h_curr_batch.shape[0] == h_past_batch.shape[0] and h_curr_batch.shape[1] == h_past_batch.shape[1] :\n",
    "                                                loss_frs_mse = F.mse_loss(h_curr_batch.float(), h_past_batch.float())\n",
    "                                                if torch.isfinite(loss_frs_mse): tot_feat_loss += loss_frs_mse * h_past_batch.size(0); feat_rep_n += h_past_batch.size(0)\n",
    "                    comb_loss = loss_curr\n",
    "                    if sup_rep_n > 0 and config.replay_alpha > 0 and torch.isfinite(tot_sup_loss): avg_s_loss = tot_sup_loss / sup_rep_n; sup_rep_loss_val = avg_s_loss.item(); comb_loss += config.replay_alpha * avg_s_loss\n",
    "                    if feat_rep_n > 0 and config.feature_replay_alpha > 0 and torch.isfinite(tot_feat_loss): avg_f_loss = tot_feat_loss / feat_rep_n; feat_rep_loss_val = avg_f_loss.item(); comb_loss += config.feature_replay_alpha * avg_f_loss\n",
    "                    if torch.isfinite(comb_loss): comb_loss.backward(); grad_n = sum(p.grad.detach().data.norm(2).item() ** 2 for p in trainable_params if p.grad is not None and torch.is_tensor(p.grad)) ** 0.5; optimizer.step()\n",
    "                    else: grad_n = 0.0; logger.warning(f\"Skipping optimizer step due to invalid combined_loss for task {raw_current_task_id}, batch {batch_idx}. Loss: {comb_loss.item() if torch.is_tensor(comb_loss) else comb_loss}\")\n",
    "                    current_peft_model.eval(); batch_correct_preds = 0\n",
    "                    with torch.no_grad():\n",
    "                        for i in range(ids_curr.size(0)):\n",
    "                            prompt_ids_gen = backbone.tokenizer(batch_data[\"raw_input_text\"][i], return_tensors=\"pt\", add_special_tokens=True, truncation=True, max_length=config.max_seq_length - 50).input_ids.to(config.device)\n",
    "                            task_cls_names = current_task_class_names_map; max_cls_len = 10\n",
    "                            if task_cls_names and isinstance(task_cls_names, dict) and task_cls_names.values(): max_cls_len = max(len(backbone.tokenizer.encode(name, add_special_tokens=False)) for name in task_cls_names.values()) if task_cls_names else 10\n",
    "                            max_g_toks = max(1, min(max_cls_len + 5, config.max_seq_length - prompt_ids_gen.shape[1] -1))\n",
    "                            gen_ids = current_peft_model.generate(input_ids=prompt_ids_gen,attention_mask=torch.ones_like(prompt_ids_gen),max_new_tokens=max_g_toks,pad_token_id=backbone.tokenizer.pad_token_id,eos_token_id=backbone.tokenizer.eos_token_id,do_sample=False, temperature=0.7, top_p=0.9)\n",
    "                            gen_txt_cls = backbone.tokenizer.decode(gen_ids[0][prompt_ids_gen.shape[1]:], skip_special_tokens=True).strip()\n",
    "                            if gen_txt_cls.lower() == str(batch_data[\"raw_output_text\"][i]).lower(): batch_correct_preds += 1\n",
    "                    acc_metric_batch_avg = batch_correct_preds / ids_curr.size(0) if ids_curr.size(0) > 0 else 0.0\n",
    "                    epoch_losses.append(loss_val); epoch_accuracies.append(acc_metric_batch_avg); epoch_grad_norms.append(grad_n);\n",
    "                    if config.use_wandb and 'wandb' in globals() and wandb.run:\n",
    "                        wandb.log({\n",
    "                            f\"step_metrics/{raw_current_task_id.replace('/', '_')}/loss\": loss_val,\n",
    "                            \"accuracy\": acc_metric_batch_avg,\n",
    "                            \"grad_norm\": grad_n,\n",
    "                            f\"step_replay/{raw_current_task_id.replace('/', '_')}/sup_n\": sup_rep_n,\n",
    "                            \"sup_loss\": sup_rep_loss_val,\n",
    "                            \"feat_n\": feat_rep_n,\n",
    "                            \"feat_loss\": feat_rep_loss_val\n",
    "                        }, step=int(global_step_for_wandb))\n",
    "\n",
    "\n",
    "                # ---> START EARLY STOPPING LOGIC <---\n",
    "                if config.use_lora_early_stopping and actual_epochs_run_for_task >= config.min_lora_epochs_before_early_stop:\n",
    "                    logger.info(f\"    ES Check: Epoch {actual_epochs_run_for_task}, evaluating on val set for {raw_current_task_id} with LoRA {active_lora_to_train_tagged}\")\n",
    "                    current_peft_model.eval()\n",
    "\n",
    "                    es_corr, es_tot = 0, 0\n",
    "                    if val_examples_raw:\n",
    "                        for es_val_ex_item in val_examples_raw:\n",
    "                            es_raw_input_text_val = es_val_ex_item['input']\n",
    "                            es_raw_target_text_val = es_val_ex_item['output']\n",
    "                            with torch.no_grad():\n",
    "                                es_val_p_ids = backbone.tokenizer(es_raw_input_text_val, return_tensors=\"pt\", add_special_tokens=True, truncation=True, max_length=config.max_seq_length - 50).input_ids.to(config.device)\n",
    "                                es_val_max_cls_len = 10\n",
    "                                if current_task_class_names_map and isinstance(current_task_class_names_map, dict) and current_task_class_names_map.values():\n",
    "                                    es_val_max_cls_len = max(len(backbone.tokenizer.encode(name, add_special_tokens=False)) for name in current_task_class_names_map.values()) if current_task_class_names_map else 10\n",
    "                                es_val_max_toks = max(1, min(es_val_max_cls_len + 5, config.max_seq_length - es_val_p_ids.shape[1] -1))\n",
    "                                es_val_gen_ids = current_peft_model.generate(\n",
    "                                    input_ids=es_val_p_ids, attention_mask=torch.ones_like(es_val_p_ids),\n",
    "                                    max_new_tokens=es_val_max_toks, pad_token_id=backbone.tokenizer.pad_token_id,\n",
    "                                    eos_token_id=backbone.tokenizer.eos_token_id, do_sample=False, temperature=0.7, top_p=0.9\n",
    "                                )\n",
    "                                es_val_gen_txt = backbone.tokenizer.decode(es_val_gen_ids[0][es_val_p_ids.shape[1]:], skip_special_tokens=True).strip()\n",
    "                                if es_val_gen_txt.lower() == str(es_raw_target_text_val).lower():\n",
    "                                    es_corr += 1\n",
    "                            es_tot += 1\n",
    "                        current_val_metric = (es_corr / es_tot) if es_tot > 0 else 0.0\n",
    "                        logger.info(f\"    ES Check: Val Acc for {raw_current_task_id} on LoRA {active_lora_to_train_tagged}: {current_val_metric:.4f} (Best: {best_val_metric_for_early_stop:.4f})\")\n",
    "                        if config.use_wandb and 'wandb' in globals() and wandb.run:\n",
    "                            wandb.log({f\"early_stopping_val_metrics/{raw_current_task_id.replace('/', '_')}/val_accuracy_epoch_{actual_epochs_run_for_task}\": current_val_metric}, step=int(cumulative_global_steps + actual_epochs_run_for_task * len(current_task_train_dataloader) -1))\n",
    "\n",
    "                        if config.lora_early_stopping_metric == \"accuracy\":\n",
    "                            if current_val_metric > best_val_metric_for_early_stop + config.lora_early_stopping_delta:\n",
    "                                best_val_metric_for_early_stop = current_val_metric\n",
    "                                early_stop_patience_counter = 0\n",
    "                                logger.info(f\"    ES Check: New best val_acc: {best_val_metric_for_early_stop:.4f}. Patience reset.\")\n",
    "                            else:\n",
    "                                early_stop_patience_counter += 1\n",
    "                                logger.info(f\"    ES Check: No improvement. Patience: {early_stop_patience_counter}/{config.lora_early_stopping_patience}\")\n",
    "\n",
    "                        if early_stop_patience_counter >= config.lora_early_stopping_patience:\n",
    "                            logger.info(f\"  EARLY STOPPING LoRA training for {raw_current_task_id} at epoch {actual_epochs_run_for_task} due to no improvement on validation set.\")\n",
    "                            stopped_early_this_task = True\n",
    "                    else:\n",
    "                        logger.warning(f\"    ES Check: No validation examples for {raw_current_task_id}. Cannot perform early stopping.\")\n",
    "                # ---> END EARLY STOPPING LOGIC <---\n",
    "\n",
    "                if task_samples_for_replay_model_training_this_task and (config.replay_alpha > 0 or config.feature_replay_alpha > 0) and replay_manager.replay_model is not None and not config.run_sft_baseline :\n",
    "                    if epoch_num == 0: replay_manager.add_task_samples(raw_current_task_id, task_samples_for_replay_model_training_this_task, train_replay_model_now=False)\n",
    "                    logger.info(f\"Epoch {actual_epochs_run_for_task} ended. Training replay model for task '{raw_current_task_id}' using {len(replay_manager.replay_buffer.get(raw_current_task_id, []))} samples.\")\n",
    "                    replay_manager.train_replay_model(raw_current_task_id, replay_manager.replay_buffer.get(raw_current_task_id, []))\n",
    "\n",
    "                _e_val = 3.0\n",
    "                if 'batch_data' in locals() and batch_data:\n",
    "                    e_ids, e_mask = batch_data[\"input_ids\"].to(config.device), batch_data[\"attention_mask\"].to(config.device)\n",
    "                    lbls_curr_entropy = batch_data[\"labels\"].to(config.device); non_pad_lbls_first = lbls_curr_entropy[0][lbls_curr_entropy[0] != -100]\n",
    "                    e_prompt_len = e_ids.shape[1]\n",
    "                    if non_pad_lbls_first.numel() > 0: lbl_starts_first = (lbls_curr_entropy[0] != -100).nonzero(as_tuple=True)[0]; e_prompt_len = lbl_starts_first[0].item() if lbl_starts_first.numel() > 0 else e_ids.shape[1]\n",
    "                    with torch.no_grad(): current_peft_model.eval(); out_e = current_peft_model(input_ids=e_ids, attention_mask=e_mask); logits_e = out_e.logits\n",
    "                    if not (torch.isnan(logits_e).any() or torch.isinf(logits_e).any()):\n",
    "                        logits_prob = logits_e;\n",
    "                        if e_prompt_len > 0 and e_prompt_len < logits_e.shape[1]: logits_prob = logits_e[:, e_prompt_len:, :]\n",
    "                        if logits_prob.numel() > 0 and logits_prob.shape[0]>0 and logits_prob.shape[1] > 0 :\n",
    "                            probs_e = F.softmax(logits_prob.float(), dim=-1);\n",
    "                            if not torch.isnan(probs_e).any(): log_probs_e = F.log_softmax(logits_prob.float(), dim=-1); tok_e = -(probs_e * log_probs_e).sum(dim=-1)\n",
    "                            if not (torch.isnan(tok_e).any() or torch.isinf(tok_e).any()) and tok_e.numel() > 0: _e_val = tok_e.mean().item()\n",
    "                entr_val = _e_val;\n",
    "                if np.isnan(entr_val) or np.isinf(entr_val): entr_val = 3.0\n",
    "                epoch_entropies.append(entr_val); avg_epoch_loss = np.mean(epoch_losses) if epoch_losses else 0.0\n",
    "                avg_epoch_acc = np.mean(epoch_accuracies) if epoch_accuracies else 0.0; avg_epoch_grad = np.mean(epoch_grad_norms) if epoch_grad_norms else 0.0\n",
    "                avg_epoch_entropy = np.mean(epoch_entropies) if epoch_entropies else 0.0\n",
    "                logger.info(f\"  LoRA Epoch {actual_epochs_run_for_task} Avg Metrics - Loss: {avg_epoch_loss:.4f}, Acc: {avg_epoch_acc:.4f}, GradN: {avg_epoch_grad:.4f}, Entropy (last batch): {avg_epoch_entropy:.4f}\")\n",
    "                epoch_avg_metrics_nm = {\"accuracy\": avg_epoch_acc, \"nll\": avg_epoch_loss, \"gradient_norm\": avg_epoch_grad, \"entropy\": avg_epoch_entropy}\n",
    "                gamma_nm_epoch = neuromod_manager.compute_gamma_gain(raw_current_task_id, epoch_avg_metrics_nm)\n",
    "                if not config.run_sft_baseline and config.consolidation_method == \"aflora\": consolidation_manager.consolidate(active_lora_to_train_tagged, current_peft_model, gamma_nm_epoch, data_loader=None)\n",
    "                if config.use_wandb and 'wandb' in globals() and wandb.run:\n",
    "                    wandb.log({\n",
    "                        f\"epoch_metrics/{raw_current_task_id.replace('/', '_')}/avg_loss\": avg_epoch_loss,\n",
    "                        f\"epoch_metrics/{raw_current_task_id.replace('/', '_')}/avg_accuracy\": avg_epoch_acc,\n",
    "                        f\"epoch_metrics/{raw_current_task_id.replace('/', '_')}/avg_grad_norm\": avg_epoch_grad,\n",
    "                        f\"epoch_metrics/{raw_current_task_id.replace('/', '_')}/avg_entropy\": avg_epoch_entropy,\n",
    "                        f\"epoch_neuromod/{raw_current_task_id.replace('/', '_')}/gamma_gain\": gamma_nm_epoch\n",
    "                    }, step=int(cumulative_global_steps + actual_epochs_run_for_task * len(current_task_train_dataloader) -1))\n",
    "\n",
    "                if stopped_early_this_task:\n",
    "                    break\n",
    "\n",
    "            if train_examples_raw and optimizer:\n",
    "                cumulative_global_steps += actual_epochs_run_for_task * len(current_task_train_dataloader)\n",
    "\n",
    "\n",
    "        logger.info(f\"--- Evaluating after Task {task_index+1}: {raw_current_task_id} (after {actual_epochs_run_for_task} LoRA epochs) ---\")\n",
    "        current_peft_model.eval()\n",
    "        for j_eval in range(task_index + 1):\n",
    "            eval_task_info = processed_task_info_for_eval[j_eval]; eval_task_id_raw = eval_task_info[\"id\"]\n",
    "            eval_val_ex = all_task_val_sets[eval_task_id_raw]; eval_cls_map = eval_task_info[\"class_names_map\"]\n",
    "            eval_lora_to_activate_tagged = eval_task_info[\"lora_trained_on_this_task\"]; can_eval = False\n",
    "            eval_active_model = None\n",
    "            if isinstance(peft_manager.model, PeftModel):\n",
    "                if eval_lora_to_activate_tagged in peft_manager.model.peft_config: peft_manager.activate_lora_modules(eval_lora_to_activate_tagged); eval_active_model = peft_manager.get_current_peft_model(); can_eval = True\n",
    "                else:\n",
    "                    logger.info(f\"Attempting to load LoRA '{eval_lora_to_activate_tagged}' for evaluation of task {eval_task_id_raw}.\")\n",
    "                    lm_eval = peft_manager.load_lora_module(eval_lora_to_activate_tagged, set_as_active=True)\n",
    "                    if lm_eval and eval_lora_to_activate_tagged in peft_manager.model.peft_config: eval_active_model = lm_eval; can_eval = True\n",
    "            if not can_eval or eval_active_model is None: logger.warning(f\"Cannot evaluate LoRA {eval_lora_to_activate_tagged} for task {eval_task_id_raw}. Not found or not loadable.\"); accuracy_matrix[(task_index, j_eval)] = 0.0; continue\n",
    "            eval_active_model.eval(); corr, tot = 0,0\n",
    "            if eval_val_ex:\n",
    "                for val_ex_idx, val_ex_item in enumerate(eval_val_ex):\n",
    "                    raw_input_text_val = val_ex_item['input']; raw_target_text_val = val_ex_item['output']\n",
    "                    with torch.no_grad():\n",
    "                        val_p_ids = backbone.tokenizer(raw_input_text_val, return_tensors=\"pt\", add_special_tokens=True, truncation=True, max_length=config.max_seq_length - 50).input_ids.to(config.device)\n",
    "                        val_max_cls_len = 10\n",
    "                        if eval_cls_map and isinstance(eval_cls_map, dict) and eval_cls_map.values(): val_max_cls_len = max(len(backbone.tokenizer.encode(name, add_special_tokens=False)) for name in eval_cls_map.values()) if eval_cls_map else 10\n",
    "                        val_max_toks = max(1, min(val_max_cls_len + 5, config.max_seq_length - val_p_ids.shape[1] -1))\n",
    "                        val_gen_ids = eval_active_model.generate(input_ids=val_p_ids,attention_mask=torch.ones_like(val_p_ids),max_new_tokens=val_max_toks,pad_token_id=backbone.tokenizer.pad_token_id,eos_token_id=backbone.tokenizer.eos_token_id,do_sample=False, temperature=0.7, top_p=0.9)\n",
    "                        val_gen_txt = backbone.tokenizer.decode(val_gen_ids[0][val_p_ids.shape[1]:], skip_special_tokens=True).strip()\n",
    "                        if val_gen_txt.lower() == str(raw_target_text_val).lower(): corr += 1\n",
    "                    tot += 1\n",
    "                task_acc_val = corr / tot if tot > 0 else 0.0; accuracy_matrix[(task_index, j_eval)] = task_acc_val\n",
    "                logger.info(f\"    Acc on Task_{j_eval} ({eval_task_id_raw}) (LoRA '{eval_lora_to_activate_tagged}') after Task_{task_index} ({raw_current_task_id}): {task_acc_val:.4f}\")\n",
    "                current_eval_step = cumulative_global_steps -1 if (train_examples_raw and optimizer) else cumulative_global_steps\n",
    "                if config.use_wandb and 'wandb' in globals() and wandb.run:\n",
    "                    wandb.log({f\"eval_acc/acc_task{j_eval}_after_task{task_index}_{eval_task_id_raw.replace('/', '_')}_{eval_lora_to_activate_tagged.replace('/', '_')}\": task_acc_val}, step=int(current_eval_step))\n",
    "            else: accuracy_matrix[(task_index, j_eval)] = 0.0\n",
    "        current_eval_step_cl = cumulative_global_steps -1 if (train_examples_raw and optimizer) else cumulative_global_steps\n",
    "        if accuracy_matrix and task_index >=0 :\n",
    "            avg_acc_k = sum(accuracy_matrix.get((task_index, j),0.0) for j in range(task_index + 1)) / (task_index + 1)\n",
    "            logger.info(f\"  AvgAcc after task {raw_current_task_id} (Task {task_index}): {avg_acc_k:.4f}\")\n",
    "            if config.use_wandb and 'wandb' in globals() and wandb.run:\n",
    "                wandb.log({f\"cl_metrics/avg_acc_upto_task_{task_index}_{raw_current_task_id.replace('/', '_')}\": avg_acc_k}, step=int(current_eval_step_cl))\n",
    "            if task_index > 0:\n",
    "                bwt_s, bwt_n = 0.0, 0\n",
    "                for j_bwt in range(task_index): acc_kj, acc_jj = accuracy_matrix.get((task_index, j_bwt), None), accuracy_matrix.get((j_bwt, j_bwt), None);\n",
    "                if acc_kj is not None and acc_jj is not None: bwt_s += (acc_kj - acc_jj); bwt_n +=1\n",
    "                bwt_curr = bwt_s / bwt_n if bwt_n > 0 else 0.0; logger.info(f\"  BWT after task {raw_current_task_id} (Task {task_index}): {bwt_curr:.4f}\")\n",
    "                if config.use_wandb and 'wandb' in globals() and wandb.run:\n",
    "                     wandb.log({f\"cl_metrics/bwt_upto_task_{task_index}_{raw_current_task_id.replace('/', '_')}\": bwt_curr}, step=int(current_eval_step_cl))\n",
    "\n",
    "        if not config.run_sft_baseline and config.consolidation_method == \"ewc\" and train_examples_raw and optimizer:\n",
    "            ewc_dl = None\n",
    "            if config.replay_method == \"none\" and train_examples_raw and config.ewc_data_loader_num_samples > 0:\n",
    "                logger.info(f\"EWC Main: Replay is off. Using {config.ewc_data_loader_num_samples} samples from current task '{raw_current_task_id}' for EWC Fisher calculation.\")\n",
    "                num_samples_for_ewc_fisher = min(len(train_examples_raw), config.ewc_data_loader_num_samples)\n",
    "                if num_samples_for_ewc_fisher > 0:\n",
    "                    ewc_fisher_samples_indices = np.random.choice(len(train_examples_raw), num_samples_for_ewc_fisher, replace=False)\n",
    "                    ewc_fisher_samples = [train_examples_raw[i] for i in ewc_fisher_samples_indices]\n",
    "                    ewc_ds = EWCDataset(ewc_fisher_samples, backbone.tokenizer, config.max_seq_length)\n",
    "                    ewc_dl = TorchDataLoader(ewc_ds, batch_size=config.ewc_batch_size, shuffle=True, num_workers=config.num_workers)\n",
    "                else: logger.warning(f\"EWC Main: Not enough samples in train_examples_raw for task '{raw_current_task_id}' to create EWC DataLoader.\")\n",
    "            elif raw_current_task_id in replay_manager.replay_buffer and replay_manager.replay_buffer[raw_current_task_id] and replay_manager.replay_model:\n",
    "                raw_ewc = replay_manager.get_raw_samples_for_replay(raw_current_task_id, config.ewc_data_loader_num_samples )\n",
    "                if raw_ewc: ewc_ds = EWCDataset(raw_ewc, backbone.tokenizer, config.max_seq_length); ewc_dl = TorchDataLoader(ewc_ds, batch_size=config.ewc_batch_size, shuffle=True, num_workers=config.num_workers)\n",
    "            if ewc_dl is not None or config.ewc_data_loader_num_samples == 0 :\n",
    "                final_gamma_for_task_consolidation = neuromod_manager.gamma_gain_history.get(raw_current_task_id, [0.5])[-1]\n",
    "                if not (isinstance(peft_manager.model, PeftModel) and active_lora_to_train_tagged in peft_manager.model.peft_config):\n",
    "                    logger.error(f\"EWC Consolidation: LoRA '{active_lora_to_train_tagged}' not in model config. Cannot consolidate.\")\n",
    "                else:\n",
    "                    peft_manager.activate_lora_modules(active_lora_to_train_tagged)\n",
    "                    current_peft_model_for_ewc = peft_manager.get_current_peft_model()\n",
    "                    consolidation_manager.consolidate(active_lora_to_train_tagged, current_peft_model_for_ewc, final_gamma_for_task_consolidation, data_loader=ewc_dl)\n",
    "            else: logger.warning(f\"EWC Consolidation for {active_lora_to_train_tagged} skipped as ewc_dl could not be prepared and ewc_data_loader_num_samples > 0.\")\n",
    "\n",
    "        if not config.run_sft_baseline and task_index >= 0 and config.router_training_epochs_per_call > 0:\n",
    "            router_training_feed_data = []\n",
    "            for i_task_processed in range(task_index + 1):\n",
    "                task_info_for_router = processed_task_info_for_eval[i_task_processed]\n",
    "                lora_id_for_this_task_in_current_run = task_info_for_router[\"lora_trained_on_this_task\"]\n",
    "                desc_for_router_train = task_info_for_router[\"task_description_for_router\"]\n",
    "                router_training_feed_data.append(((desc_for_router_train, None, None), lora_id_for_this_task_in_current_run ))\n",
    "            if router_training_feed_data:\n",
    "                logger.info(f\"Preparing to train router with {len(router_training_feed_data)} (desc_only) -> LoRA ID pairs.\")\n",
    "                peft_manager.train_router(router_training_feed_data)\n",
    "            else: logger.info(\"No data prepared for router training after this task.\")\n",
    "\n",
    "        current_plot_step = cumulative_global_steps -1 if (train_examples_raw and optimizer) else cumulative_global_steps\n",
    "        if train_examples_raw and optimizer:\n",
    "            neuromod_manager.save_metrics(raw_current_task_id)\n",
    "            plot_filename = f\"{config.experiment_tag}_{raw_current_task_id}_gamma_plot.png\".replace(\".\", \"_\") if config.experiment_tag else f\"{raw_current_task_id}_gamma_plot.png\".replace(\".\", \"_\")\n",
    "            plot_dir = os.path.join(config.output_dir, config.experiment_tag.replace(\".\", \"_\")) if config.experiment_tag else config.output_dir\n",
    "            os.makedirs(plot_dir, exist_ok=True); plot_path = os.path.join(plot_dir, plot_filename)\n",
    "            fig_nm = None\n",
    "            try:\n",
    "                fig_nm = neuromod_manager.plot_gamma_gain(raw_current_task_id, save_path=plot_path)\n",
    "                if fig_nm and config.use_wandb and 'wandb' in globals() and wandb.run:\n",
    "                    wandb.log({f\"plots/{config.experiment_tag.replace('.', '_') or 'generic'}/{raw_current_task_id.replace('/', '_')}/gamma_history\": wandb.Image(fig_nm)}, step=int(current_plot_step))\n",
    "            except Exception as e: logger.error(f\"Plot/log gamma_gain error for '{raw_current_task_id}': {e}\")\n",
    "            finally:\n",
    "                if fig_nm: plt.close(fig_nm)\n",
    "        if train_examples_raw and optimizer: peft_manager.save_lora_module(active_lora_to_train_tagged, current_peft_model)\n",
    "        if config.device == \"cuda\": torch.cuda.empty_cache(); gc.collect()\n",
    "\n",
    "    if not config.run_sft_baseline and peft_manager is not None and hasattr(peft_manager, 'save_router_state'):\n",
    "        logger.info(\"Attempting to save router state at the end of the experiment.\")\n",
    "        peft_manager.save_router_state()\n",
    "    logger.info(\"\\\\n--- Experiment Main Loop Completed ---\\\\n\")\n",
    "    if config.use_wandb and 'wandb' in globals() and wandb.run and accuracy_matrix:\n",
    "        final_k = len(tasks_to_process) -1\n",
    "        if final_k >=0 :\n",
    "            final_eval_tasks_count = sum(1 for j in range(final_k + 1) if (final_k, j) in accuracy_matrix)\n",
    "            if final_eval_tasks_count > 0:\n",
    "                final_avg = sum(accuracy_matrix.get((final_k, j), 0.0) for j in range(final_k + 1)) / final_eval_tasks_count\n",
    "                wandb.summary[\"final_average_accuracy\"] = final_avg; logger.info(f\"Final Avg Acc: {final_avg:.4f}\")\n",
    "                if final_k > 0:\n",
    "                    bwt_f_sum, bwt_f_n = 0.0, 0\n",
    "                    for j_f_bwt in range(final_k): acc_kf, acc_jj = accuracy_matrix.get((final_k, j_f_bwt), None), accuracy_matrix.get((j_f_bwt, j_f_bwt), None);\n",
    "                    if acc_kf is not None and acc_jj is not None: bwt_f_sum += (acc_kf - acc_jj); bwt_f_n +=1\n",
    "                    if bwt_f_n > 0: final_b = bwt_f_sum / bwt_f_n; wandb.summary[\"final_backward_transfer\"] = final_b; logger.info(f\"Final BWT: {final_b:.4f}\")\n",
    "                    else: wandb.summary[\"final_backward_transfer\"] = 0.0; logger.info(f\"Final BWT: 0.0000 (not enough prior tasks for calculation)\")\n",
    "    if config.run_sft_baseline and sft_originals:\n",
    "        logger.info(\"--- SFT BASELINE RUN COMPLETE - Restoring original config values ---\")\n",
    "        for key, val in sft_originals.items(): setattr(config, key, val)\n",
    "        consolidation_manager.config = config; replay_manager.config = config\n",
    "        if peft_manager.router is not None: peft_manager.router.config = config\n",
    "        if sft_originals.get('replay_method') != \"none\" and replay_manager.replay_model is None:\n",
    "            logger.info(f\"SFT Restore: Re-initializing replay model of type '{config.replay_method}'\")\n",
    "            current_base_model_for_replay = peft_manager.get_current_peft_model()\n",
    "            if config.replay_method == \"cmt\": replay_manager.replay_model = CMTReplay(config, current_base_model_for_replay, backbone.tokenizer)\n",
    "            elif config.replay_method == \"pcgr\": replay_manager.replay_model = PCGRReplay(config, current_base_model_for_replay, backbone.tokenizer)\n",
    "        logger.info(\"--- Config restored after SFT baseline. ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-05-20 16:15:14,592 - __main__ - INFO - HF_TOKEN is set: True\n",
      "2025-05-20 16:15:14,593 - __main__ - INFO - WANDB_API_KEY is set: True\n",
      "2025-05-20 16:15:14,594 - __main__ - INFO - WANDB_PROJECT: RESTART01\n",
      "2025-05-20 16:15:14,594 - __main__ - INFO - WANDB_ENTITY: doingmyownthing82-none\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "# Assuming logger is configured\n",
    "logger.info(f\"HF_TOKEN is set: {os.environ.get('HF_TOKEN') is not None}\")\n",
    "logger.info(f\"WANDB_API_KEY is set: {os.environ.get('WANDB_API_KEY') is not None}\")\n",
    "logger.info(f\"WANDB_PROJECT: {os.environ.get('WANDB_PROJECT')}\")\n",
    "logger.info(f\"WANDB_ENTITY: {os.environ.get('WANDB_ENTITY')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 488
    },
    "id": "J9fBTqjsTCcC",
    "outputId": "a3bb32a7-bcad-4441-eb5d-6df2940e7689",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-05-20 16:15:14,608 - __main__ - INFO - \\n==================================================\\nStarting MVE-DataScale-1.1: Larger Datasets Validation (BS=8)\\n==================================================\n",
      "INFO (AdaptiveLearnerConfig): Found WANDB_API_KEY in environment variables.\n",
      "INFO (AdaptiveLearnerConfig): Hugging Face token (HF_TOKEN/HUGGINGFACE_HUB_TOKEN) found in environment variables.\n",
      "INFO (AdaptiveLearnerConfig): Found WANDB_PROJECT in environment variables: RESTART01\n",
      "INFO (AdaptiveLearnerConfig): Found WANDB_ENTITY in environment variables: doingmyownthing82-none\n",
      "2025-05-20 16:15:14,610 - __main__ - INFO - --- CONFIGURING FOR: Variant DataScale-1.1: Hybrid Scaled (2K Train, BS8, LR5e-5, ES, seed123) ---\n",
      "2025-05-20 16:15:14,610 - __main__ - INFO - Experiment Tag: MVE_CL_DataScale_2Ktrain_BS8_LR5e-5_ES_seed123\n",
      "2025-05-20 16:15:14,611 - __main__ - INFO - Consolidation: ewc\n",
      "2025-05-20 16:15:14,611 - __main__ - INFO - EWC Lambda: 250.0, Bypass Gamma for Accum: False\n",
      "2025-05-20 16:15:14,611 - __main__ - INFO - EWC Fisher Samples: 50\n",
      "2025-05-20 16:15:14,612 - __main__ - INFO - EWC Batch Size: 10\n",
      "2025-05-20 16:15:14,612 - __main__ - INFO - Gamma Gain Lambda: 3.0\n",
      "2025-05-20 16:15:14,612 - __main__ - INFO - Replay Method: cmt, Alpha: 0.2, Feature Alpha: 0.1\n",
      "2025-05-20 16:15:14,613 - __main__ - INFO - Replay Internal BS: 4, Replay Backbone Encoding BS: 16\n",
      "2025-05-20 16:15:14,613 - __main__ - INFO - Num tasks sharing LoRA: 2\n",
      "2025-05-20 16:15:14,613 - __main__ - INFO - Total tasks in sequence: 5\n",
      "2025-05-20 16:15:14,614 - __main__ - INFO - LoRA Train Batch Size: 8, Max LoRA Train Epochs: 12\n",
      "2025-05-20 16:15:14,614 - __main__ - INFO - Learning Rate: 5e-05\n",
      "2025-05-20 16:15:14,614 - __main__ - INFO - Use Advanced Embeddings: False, k_for_prototype: 0\n",
      "2025-05-20 16:15:14,615 - __main__ - INFO - Use LoRA Early Stopping: True\n",
      "2025-05-20 16:15:14,615 - __main__ - INFO -   ES Patience: 3, ES Metric: accuracy\n",
      "2025-05-20 16:15:14,615 - __main__ - INFO -   ES Min Epochs: 1, ES Delta: 0.001\n",
      "2025-05-20 16:15:14,616 - __main__ - INFO - \\n==================== Starting Experiment Variant: Variant DataScale-1.1: Hybrid Scaled (2K Train, BS8, LR5e-5, ES, seed123) (MVE_CL_DataScale_2Ktrain_BS8_LR5e-5_ES_seed123) ====================\n",
      "2025-05-20 16:15:14,616 - __main__ - INFO - W&B: API key found in environment for Variant DataScale-1.1: Hybrid Scaled (2K Train, BS8, LR5e-5, ES, seed123).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mdoingmyownthing82\u001b[0m (\u001b[33mdoingmyownthing82-none\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Using a boolean value for 'reinit' is deprecated. Use 'return_previous' or 'finish_previous' instead.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.11"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/workspace/wandb/run-20250520_161515-4qk9tv8l</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/doingmyownthing82-none/adaptive-learner-cl-datascale/runs/4qk9tv8l' target=\"_blank\">20250520-161514_MVE_CL_DataScale_2Ktrain_BS8_LR5e-5_ES_seed123</a></strong> to <a href='https://wandb.ai/doingmyownthing82-none/adaptive-learner-cl-datascale' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/doingmyownthing82-none/adaptive-learner-cl-datascale' target=\"_blank\">https://wandb.ai/doingmyownthing82-none/adaptive-learner-cl-datascale</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/doingmyownthing82-none/adaptive-learner-cl-datascale/runs/4qk9tv8l' target=\"_blank\">https://wandb.ai/doingmyownthing82-none/adaptive-learner-cl-datascale/runs/4qk9tv8l</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-05-20 16:15:15,578 - __main__ - INFO - W&B run initiated for Variant DataScale-1.1: Hybrid Scaled (2K Train, BS8, LR5e-5, ES, seed123) to project 'adaptive-learner-cl-datascale': 20250520-161514_MVE_CL_DataScale_2Ktrain_BS8_LR5e-5_ES_seed123\n",
      "2025-05-20 16:15:15,578 - __main__ - INFO - --- Main function started. SFT Mode: False. Exp Tag: MVE_CL_DataScale_2Ktrain_BS8_LR5e-5_ES_seed123 ---\n",
      "Router type: linear, Confidence threshold: 1.1\n",
      "Load router from tag: None\n",
      "k_examples_for_prototype: 0, Use Advanced Embeddings: False\n",
      "Use LoRA Early Stopping: True\n",
      "2025-05-20 16:15:15,579 - __main__ - INFO - Initializing components...\n",
      "2025-05-20 16:15:15,579 - __main__ - INFO - Loading model: google/gemma-2b\n",
      "2025-05-20 16:15:15,580 - __main__ - INFO - Attempting to load model with Flash Attention 2.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-05-20 16:15:15,956 - accelerate.utils.modeling - INFO - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "40db31dafddc41e99038f45ab0d01bea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-05-20 16:15:18,974 - __main__ - INFO - Model loaded successfully with Flash Attention 2 and 8-bit quantization.\n",
      "2025-05-20 16:15:19,620 - __main__ - INFO - Model loaded with 2,506,172,416 parameters\n",
      "2025-05-20 16:15:19,622 - __main__ - INFO - Base model parameters frozen\n",
      "2025-05-20 16:15:20,246 - __main__ - INFO - Router common init: Embedding model dtype: torch.float16, Router internal param dtype: torch.float32\n",
      "2025-05-20 16:15:20,247 - __main__ - INFO - LinearRouter initialized (ADV. EMBEDDINGS OFF) for DESCRIPTION features ONLY. Embed_dim: 2048.\n",
      "2025-05-20 16:15:20,247 - __main__ - INFO - PEFTManager init: Router object instantiated. Initial learnable profiles in router: 0\n",
      "2025-05-20 16:15:20,248 - __main__ - INFO - PEFTManager init: No router state to load (load_router_state_from_tag is None). Router starts fresh.\n",
      "2025-05-20 16:15:20,248 - __main__ - INFO - PEFTManager: (_scan_disk_and_update_metadata_globally) Scanning disk (/workspace/MyAdaptiveLearnerProject/outputs/lora_modules) for LoRA adapter metadata.\n",
      "2025-05-20 16:15:20,317 - __main__ - INFO - PEFTManager: Disk scan finished. Found/updated metadata for 25 LoRAs in PEFTManager's general records.\n",
      "2025-05-20 16:15:20,318 - __main__ - INFO - PEFTManager: After init and disk scan, router has 0 learnable profile(s).\n",
      "2025-05-20 16:15:20,346 - __main__ - INFO - Loading benchmark task: glue (config: sst2) 2000 train, 200 val.\n",
      "2025-05-20 16:15:22,358 - __main__ - INFO - Loaded 2000 train, 200 val for glue (config: sst2). Classes: {0: 'negative', 1: 'positive'}\n",
      "2025-05-20 16:15:22,359 - __main__ - INFO - Loading benchmark task: glue (config: rte) 2000 train, 200 val.\n",
      "2025-05-20 16:15:24,061 - __main__ - INFO - Loaded 2000 train, 200 val for glue (config: rte). Classes: {0: 'entailment', 1: 'not_entailment'}\n",
      "2025-05-20 16:15:24,062 - __main__ - INFO - Loading benchmark task: glue (config: mrpc) 2000 train, 200 val.\n",
      "2025-05-20 16:15:27,428 - __main__ - INFO - Loaded 2000 train, 200 val for glue (config: mrpc). Classes: {0: 'not_equivalent', 1: 'equivalent'}\n",
      "2025-05-20 16:15:27,429 - __main__ - INFO - Loading benchmark task: glue (config: qnli) 2000 train, 200 val.\n",
      "2025-05-20 16:15:28,696 - __main__ - INFO - Loaded 2000 train, 200 val for glue (config: qnli). Classes: {0: 'entailment', 1: 'not_entailment'}\n",
      "2025-05-20 16:15:28,698 - __main__ - INFO - Loading benchmark task: glue (config: cola) 2000 train, 200 val.\n",
      "2025-05-20 16:15:36,506 - __main__ - INFO - Loaded 2000 train, 200 val for glue (config: cola). Classes: {0: 'unacceptable', 1: 'acceptable'}\n",
      "2025-05-20 16:15:36,507 - __main__ - INFO - \\n--- Starting Task 1/5: ds_sst2_glue_sst2 (Exp Tag: MVE_CL_DataScale_2Ktrain_BS8_LR5e-5_ES_seed123) ---\\n  Task Description for Router: 'Benchmark: glue (sst2) Task Description: Sentiment analysis of movie reviews (SST-2)....'\n",
      "2025-05-20 16:15:36,508 - __main__ - INFO -   Train: 2000 examples, Val: 200 examples, Classes: {0: 'negative', 1: 'positive'}, Max LoRA Epochs: 12\n",
      "2025-05-20 16:15:36,508 - __main__ - INFO - SharedLoRA: Task 0 ('ds_sst2_glue_sst2') is the first shared task. Creating shared LoRA: 'MVE_CL_DataScale_2Ktrain_BS8_LR5e-5_ES_seed123_SharedAdapter_sst2_rte'\n",
      "2025-05-20 16:15:36,568 - __main__ - INFO - LinearRouter._on_module_added: CREATED new learnable embedding for 'MVE_CL_DataScale_2Ktrain_BS8_LR5e-5_ES_seed123_SharedAdapter_sst2_rte' with dim 2048.\n",
      "2025-05-20 16:15:36,569 - __main__ - INFO - LinearRouter optimizer (re)set. LR: 1.0e-04 with 1 embedding parameters.\n",
      "2025-05-20 16:15:36,573 - __main__ - INFO - PEFTManager: Created LoRA 'MVE_CL_DataScale_2Ktrain_BS8_LR5e-5_ES_seed123_SharedAdapter_sst2_rte' and called router.add_module.\n",
      "2025-05-20 16:15:36,574 - __main__ - INFO -   Task ds_sst2_glue_sst2 - LoRA Training Epoch 1/12 for LoRA: MVE_CL_DataScale_2Ktrain_BS8_LR5e-5_ES_seed123_SharedAdapter_sst2_rte\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 1 LoRA:MVE_CL_DataScale_2Kt Task:ds_sst2_glue_ss:   0%|          | 0/250 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.7` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-05-20 16:21:13,000 - __main__ - INFO -     ES Check: Epoch 1, evaluating on val set for ds_sst2_glue_sst2 with LoRA MVE_CL_DataScale_2Ktrain_BS8_LR5e-5_ES_seed123_SharedAdapter_sst2_rte\n",
      "2025-05-20 16:21:32,958 - __main__ - INFO -     ES Check: Val Acc for ds_sst2_glue_sst2 on LoRA MVE_CL_DataScale_2Ktrain_BS8_LR5e-5_ES_seed123_SharedAdapter_sst2_rte: 0.9600 (Best: -inf)\n",
      "2025-05-20 16:21:32,960 - __main__ - INFO -     ES Check: New best val_acc: 0.9600. Patience reset.\n",
      "2025-05-20 16:21:32,963 - __main__ - INFO - Epoch 1 ended. Training replay model for task 'ds_sst2_glue_sst2' using 500 samples.\n",
      "2025-05-20 16:21:32,963 - __main__ - INFO - CMTReplay: Training AE for task 'ds_sst2_glue_sst2' on 500 samples from buffer.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "CMT AE Ep1 Task:ds_sst2_glue_sst2:   0%|          | 0/125 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-05-20 16:21:35,479 - __main__ - INFO - CMT Initial WEIGHT DEBUG (Task ds_sst2_glue_sst2): Weights AFTER first AE optim step...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "CMT AE Ep2 Task:ds_sst2_glue_sst2:   0%|          | 0/125 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "CMT AE Ep3 Task:ds_sst2_glue_sst2:   0%|          | 0/125 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-05-20 16:21:36,411 - __main__ - INFO - CMTReplay AE training complete for task 'ds_sst2_glue_sst2' (500 buffer samples). AvgLoss: 1.2446, AvgUniformity: 0.5781, LR: 5.00e-05\n",
      "2025-05-20 16:21:36,590 - __main__ - INFO -   LoRA Epoch 1 Avg Metrics - Loss: 2.1594, Acc: 0.6960, GradN: 5.4113, Entropy (last batch): 4.2004\n",
      "2025-05-20 16:21:36,591 - __main__ - INFO - Task ds_sst2_glue_sst2 NM - Metrics: {'accuracy': 0.696, 'nll': 2.159, 'gradient_norm': 5.411, 'entropy': 4.2}, Î³-Gain: 1.7527\n",
      "2025-05-20 16:21:36,592 - __main__ - INFO -   Task ds_sst2_glue_sst2 - LoRA Training Epoch 2/12 for LoRA: MVE_CL_DataScale_2Ktrain_BS8_LR5e-5_ES_seed123_SharedAdapter_sst2_rte\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 2 LoRA:MVE_CL_DataScale_2Kt Task:ds_sst2_glue_ss:   0%|          | 0/250 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-05-20 16:26:02,806 - __main__ - INFO -     ES Check: Epoch 2, evaluating on val set for ds_sst2_glue_sst2 with LoRA MVE_CL_DataScale_2Ktrain_BS8_LR5e-5_ES_seed123_SharedAdapter_sst2_rte\n",
      "2025-05-20 16:26:22,444 - __main__ - INFO -     ES Check: Val Acc for ds_sst2_glue_sst2 on LoRA MVE_CL_DataScale_2Ktrain_BS8_LR5e-5_ES_seed123_SharedAdapter_sst2_rte: 0.9250 (Best: 0.9600)\n",
      "2025-05-20 16:26:22,447 - __main__ - INFO -     ES Check: No improvement. Patience: 1/3\n",
      "2025-05-20 16:26:22,448 - __main__ - INFO - Epoch 2 ended. Training replay model for task 'ds_sst2_glue_sst2' using 500 samples.\n",
      "2025-05-20 16:26:22,449 - __main__ - INFO - CMTReplay: Training AE for task 'ds_sst2_glue_sst2' on 500 samples from buffer.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "CMT AE Ep1 Task:ds_sst2_glue_sst2:   0%|          | 0/125 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "CMT AE Ep2 Task:ds_sst2_glue_sst2:   0%|          | 0/125 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "CMT AE Ep3 Task:ds_sst2_glue_sst2:   0%|          | 0/125 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-05-20 16:26:25,904 - __main__ - INFO - CMTReplay AE training complete for task 'ds_sst2_glue_sst2' (500 buffer samples). AvgLoss: 0.3265, AvgUniformity: 0.0729, LR: 5.00e-05\n",
      "2025-05-20 16:26:26,093 - __main__ - INFO -   LoRA Epoch 2 Avg Metrics - Loss: 0.0868, Acc: 0.9410, GradN: 2.5409, Entropy (last batch): 4.0865\n",
      "2025-05-20 16:26:26,095 - __main__ - INFO - Task ds_sst2_glue_sst2 NM - Metrics: {'accuracy': 0.941, 'nll': 0.087, 'gradient_norm': 2.541, 'entropy': 4.086}, Î³-Gain: 0.9000\n",
      "2025-05-20 16:26:26,095 - __main__ - INFO -   Task ds_sst2_glue_sst2 - LoRA Training Epoch 3/12 for LoRA: MVE_CL_DataScale_2Ktrain_BS8_LR5e-5_ES_seed123_SharedAdapter_sst2_rte\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 3 LoRA:MVE_CL_DataScale_2Kt Task:ds_sst2_glue_ss:   0%|          | 0/250 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-05-20 16:30:52,739 - __main__ - INFO -     ES Check: Epoch 3, evaluating on val set for ds_sst2_glue_sst2 with LoRA MVE_CL_DataScale_2Ktrain_BS8_LR5e-5_ES_seed123_SharedAdapter_sst2_rte\n",
      "2025-05-20 16:31:12,416 - __main__ - INFO -     ES Check: Val Acc for ds_sst2_glue_sst2 on LoRA MVE_CL_DataScale_2Ktrain_BS8_LR5e-5_ES_seed123_SharedAdapter_sst2_rte: 0.9450 (Best: 0.9600)\n",
      "2025-05-20 16:31:12,418 - __main__ - INFO -     ES Check: No improvement. Patience: 2/3\n",
      "2025-05-20 16:31:12,419 - __main__ - INFO - Epoch 3 ended. Training replay model for task 'ds_sst2_glue_sst2' using 500 samples.\n",
      "2025-05-20 16:31:12,419 - __main__ - INFO - CMTReplay: Training AE for task 'ds_sst2_glue_sst2' on 500 samples from buffer.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "CMT AE Ep1 Task:ds_sst2_glue_sst2:   0%|          | 0/125 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "CMT AE Ep2 Task:ds_sst2_glue_sst2:   0%|          | 0/125 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "CMT AE Ep3 Task:ds_sst2_glue_sst2:   0%|          | 0/125 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-05-20 16:31:15,813 - __main__ - INFO - CMTReplay AE training complete for task 'ds_sst2_glue_sst2' (500 buffer samples). AvgLoss: 0.1273, AvgUniformity: -0.1050, LR: 5.00e-05\n",
      "2025-05-20 16:31:15,990 - __main__ - INFO -   LoRA Epoch 3 Avg Metrics - Loss: 0.0541, Acc: 0.9700, GradN: 1.9948, Entropy (last batch): 4.3539\n",
      "2025-05-20 16:31:15,991 - __main__ - INFO - Task ds_sst2_glue_sst2 NM - Metrics: {'accuracy': 0.97, 'nll': 0.054, 'gradient_norm': 1.995, 'entropy': 4.354}, Î³-Gain: 1.5000\n",
      "2025-05-20 16:31:15,992 - __main__ - INFO -   Task ds_sst2_glue_sst2 - LoRA Training Epoch 4/12 for LoRA: MVE_CL_DataScale_2Ktrain_BS8_LR5e-5_ES_seed123_SharedAdapter_sst2_rte\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 4 LoRA:MVE_CL_DataScale_2Kt Task:ds_sst2_glue_ss:   0%|          | 0/250 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-05-20 16:35:42,275 - __main__ - INFO -     ES Check: Epoch 4, evaluating on val set for ds_sst2_glue_sst2 with LoRA MVE_CL_DataScale_2Ktrain_BS8_LR5e-5_ES_seed123_SharedAdapter_sst2_rte\n",
      "2025-05-20 16:36:02,345 - __main__ - INFO -     ES Check: Val Acc for ds_sst2_glue_sst2 on LoRA MVE_CL_DataScale_2Ktrain_BS8_LR5e-5_ES_seed123_SharedAdapter_sst2_rte: 0.9350 (Best: 0.9600)\n",
      "2025-05-20 16:36:02,347 - __main__ - INFO -     ES Check: No improvement. Patience: 3/3\n",
      "2025-05-20 16:36:02,348 - __main__ - INFO -   EARLY STOPPING LoRA training for ds_sst2_glue_sst2 at epoch 4 due to no improvement on validation set.\n",
      "2025-05-20 16:36:02,348 - __main__ - INFO - Epoch 4 ended. Training replay model for task 'ds_sst2_glue_sst2' using 500 samples.\n",
      "2025-05-20 16:36:02,348 - __main__ - INFO - CMTReplay: Training AE for task 'ds_sst2_glue_sst2' on 500 samples from buffer.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "CMT AE Ep1 Task:ds_sst2_glue_sst2:   0%|          | 0/125 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "CMT AE Ep2 Task:ds_sst2_glue_sst2:   0%|          | 0/125 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "CMT AE Ep3 Task:ds_sst2_glue_sst2:   0%|          | 0/125 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-05-20 16:36:05,714 - __main__ - INFO - CMTReplay AE training complete for task 'ds_sst2_glue_sst2' (500 buffer samples). AvgLoss: 0.0825, AvgUniformity: -0.1930, LR: 5.00e-05\n",
      "2025-05-20 16:36:05,899 - __main__ - INFO -   LoRA Epoch 4 Avg Metrics - Loss: 0.0364, Acc: 0.9840, GradN: 1.8153, Entropy (last batch): 4.4790\n",
      "2025-05-20 16:36:05,900 - __main__ - INFO - Task ds_sst2_glue_sst2 NM - Metrics: {'accuracy': 0.984, 'nll': 0.036, 'gradient_norm': 1.815, 'entropy': 4.479}, Î³-Gain: 1.5000\n",
      "2025-05-20 16:36:05,900 - __main__ - INFO - --- Evaluating after Task 1: ds_sst2_glue_sst2 (after 4 LoRA epochs) ---\n",
      "2025-05-20 16:36:25,781 - __main__ - INFO -     Acc on Task_0 (ds_sst2_glue_sst2) (LoRA 'MVE_CL_DataScale_2Ktrain_BS8_LR5e-5_ES_seed123_SharedAdapter_sst2_rte') after Task_0 (ds_sst2_glue_sst2): 0.9350\n",
      "2025-05-20 16:36:25,783 - __main__ - INFO -   AvgAcc after task ds_sst2_glue_sst2 (Task 0): 0.9350\n",
      "2025-05-20 16:36:25,786 - __main__ - INFO - Applying EWC for MVE_CL_DataScale_2Ktrain_BS8_LR5e-5_ES_seed123_SharedAdapter_sst2_rte with actual gamma_gain_param_val 1.5000\n",
      "2025-05-20 16:36:27,564 - __main__ - INFO - Computed Fisher matrix over 50 samples (target was 50).\n",
      "2025-05-20 16:36:27,574 - __main__ - INFO - EWC penalty gradient added for 144 parameters in module MVE_CL_DataScale_2Ktrain_BS8_LR5e-5_ES_seed123_SharedAdapter_sst2_rte. Bypass gamma for Fisher accum: False\n",
      "2025-05-20 16:36:27,582 - __main__ - INFO - Saved metrics and Î³-Gain history for task ds_sst2_glue_sst2\n",
      "2025-05-20 16:36:28,288 - __main__ - INFO - Saved LoRA adapter 'MVE_CL_DataScale_2Ktrain_BS8_LR5e-5_ES_seed123_SharedAdapter_sst2_rte' (likely into a nested subdir within /workspace/MyAdaptiveLearnerProject/outputs/lora_modules/MVE_CL_DataScale_2Ktrain_BS8_LR5e-5_ES_seed123_SharedAdapter_sst2_rte)\n",
      "2025-05-20 16:36:28,693 - __main__ - INFO - \\n--- Starting Task 2/5: ds_rte_glue_rte (Exp Tag: MVE_CL_DataScale_2Ktrain_BS8_LR5e-5_ES_seed123) ---\\n  Task Description for Router: 'Benchmark: glue (rte) Task Description: Recognizing Textual Entailment (RTE)....'\n",
      "2025-05-20 16:36:28,694 - __main__ - INFO -   Train: 2000 examples, Val: 200 examples, Classes: {0: 'entailment', 1: 'not_entailment'}, Max LoRA Epochs: 12\n",
      "2025-05-20 16:36:28,695 - __main__ - INFO - SharedLoRA: Task 1 ('ds_rte_glue_rte') reuses shared LoRA: 'MVE_CL_DataScale_2Ktrain_BS8_LR5e-5_ES_seed123_SharedAdapter_sst2_rte'\n",
      "2025-05-20 16:36:28,699 - __main__ - INFO -   Task ds_rte_glue_rte - LoRA Training Epoch 1/12 for LoRA: MVE_CL_DataScale_2Ktrain_BS8_LR5e-5_ES_seed123_SharedAdapter_sst2_rte\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 1 LoRA:MVE_CL_DataScale_2Kt Task:ds_rte_glue_rte:   0%|          | 0/250 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-05-20 16:45:26,861 - __main__ - INFO -     ES Check: Epoch 1, evaluating on val set for ds_rte_glue_rte with LoRA MVE_CL_DataScale_2Ktrain_BS8_LR5e-5_ES_seed123_SharedAdapter_sst2_rte\n",
      "2025-05-20 16:46:07,801 - __main__ - INFO -     ES Check: Val Acc for ds_rte_glue_rte on LoRA MVE_CL_DataScale_2Ktrain_BS8_LR5e-5_ES_seed123_SharedAdapter_sst2_rte: 0.7350 (Best: -inf)\n",
      "2025-05-20 16:46:07,805 - __main__ - INFO -     ES Check: New best val_acc: 0.7350. Patience reset.\n",
      "2025-05-20 16:46:07,809 - __main__ - INFO - Epoch 1 ended. Training replay model for task 'ds_rte_glue_rte' using 500 samples.\n",
      "2025-05-20 16:46:07,809 - __main__ - INFO - CMTReplay: Training AE for task 'ds_rte_glue_rte' on 500 samples from buffer.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "CMT AE Ep1 Task:ds_rte_glue_rte:   0%|          | 0/125 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "CMT AE Ep2 Task:ds_rte_glue_rte:   0%|          | 0/125 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "CMT AE Ep3 Task:ds_rte_glue_rte:   0%|          | 0/125 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-05-20 16:46:13,026 - __main__ - INFO - CMTReplay AE training complete for task 'ds_rte_glue_rte' (500 buffer samples). AvgLoss: 0.1391, AvgUniformity: -0.0613, LR: 5.00e-05\n",
      "2025-05-20 16:46:13,208 - __main__ - INFO -   LoRA Epoch 1 Avg Metrics - Loss: 0.7518, Acc: 0.5405, GradN: 7.7144, Entropy (last batch): 4.1832\n",
      "2025-05-20 16:46:13,210 - __main__ - INFO - Task ds_rte_glue_rte NM - Metrics: {'accuracy': 0.54, 'nll': 0.752, 'gradient_norm': 7.714, 'entropy': 4.183}, Î³-Gain: 1.8797\n",
      "2025-05-20 16:46:13,211 - __main__ - INFO -   Task ds_rte_glue_rte - LoRA Training Epoch 2/12 for LoRA: MVE_CL_DataScale_2Ktrain_BS8_LR5e-5_ES_seed123_SharedAdapter_sst2_rte\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 2 LoRA:MVE_CL_DataScale_2Kt Task:ds_rte_glue_rte:   0%|          | 0/250 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-05-20 16:56:11,998 - __main__ - INFO -     ES Check: Epoch 2, evaluating on val set for ds_rte_glue_rte with LoRA MVE_CL_DataScale_2Ktrain_BS8_LR5e-5_ES_seed123_SharedAdapter_sst2_rte\n",
      "2025-05-20 16:56:56,092 - __main__ - INFO -     ES Check: Val Acc for ds_rte_glue_rte on LoRA MVE_CL_DataScale_2Ktrain_BS8_LR5e-5_ES_seed123_SharedAdapter_sst2_rte: 0.8000 (Best: 0.7350)\n",
      "2025-05-20 16:56:56,096 - __main__ - INFO -     ES Check: New best val_acc: 0.8000. Patience reset.\n",
      "2025-05-20 16:56:56,097 - __main__ - INFO - Epoch 2 ended. Training replay model for task 'ds_rte_glue_rte' using 500 samples.\n",
      "2025-05-20 16:56:56,097 - __main__ - INFO - CMTReplay: Training AE for task 'ds_rte_glue_rte' on 500 samples from buffer.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "CMT AE Ep1 Task:ds_rte_glue_rte:   0%|          | 0/125 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "CMT AE Ep2 Task:ds_rte_glue_rte:   0%|          | 0/125 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "CMT AE Ep3 Task:ds_rte_glue_rte:   0%|          | 0/125 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-05-20 16:57:01,198 - __main__ - INFO - CMTReplay AE training complete for task 'ds_rte_glue_rte' (500 buffer samples). AvgLoss: 0.0771, AvgUniformity: -0.2145, LR: 5.00e-05\n",
      "2025-05-20 16:57:01,383 - __main__ - INFO -   LoRA Epoch 2 Avg Metrics - Loss: 0.1189, Acc: 0.7770, GradN: 5.0425, Entropy (last batch): 4.1657\n",
      "2025-05-20 16:57:01,385 - __main__ - INFO - Task ds_rte_glue_rte NM - Metrics: {'accuracy': 0.777, 'nll': 0.119, 'gradient_norm': 5.043, 'entropy': 4.166}, Î³-Gain: 0.9000\n",
      "2025-05-20 16:57:01,386 - __main__ - INFO -   Task ds_rte_glue_rte - LoRA Training Epoch 3/12 for LoRA: MVE_CL_DataScale_2Ktrain_BS8_LR5e-5_ES_seed123_SharedAdapter_sst2_rte\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 3 LoRA:MVE_CL_DataScale_2Kt Task:ds_rte_glue_rte:   0%|          | 0/250 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-05-20 17:06:51,102 - __main__ - INFO -     ES Check: Epoch 3, evaluating on val set for ds_rte_glue_rte with LoRA MVE_CL_DataScale_2Ktrain_BS8_LR5e-5_ES_seed123_SharedAdapter_sst2_rte\n",
      "2025-05-20 17:07:33,485 - __main__ - INFO -     ES Check: Val Acc for ds_rte_glue_rte on LoRA MVE_CL_DataScale_2Ktrain_BS8_LR5e-5_ES_seed123_SharedAdapter_sst2_rte: 0.8150 (Best: 0.8000)\n",
      "2025-05-20 17:07:33,491 - __main__ - INFO -     ES Check: New best val_acc: 0.8150. Patience reset.\n",
      "2025-05-20 17:07:33,492 - __main__ - INFO - Epoch 3 ended. Training replay model for task 'ds_rte_glue_rte' using 500 samples.\n",
      "2025-05-20 17:07:33,492 - __main__ - INFO - CMTReplay: Training AE for task 'ds_rte_glue_rte' on 500 samples from buffer.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "CMT AE Ep1 Task:ds_rte_glue_rte:   0%|          | 0/125 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "CMT AE Ep2 Task:ds_rte_glue_rte:   0%|          | 0/125 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "CMT AE Ep3 Task:ds_rte_glue_rte:   0%|          | 0/125 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-05-20 17:07:38,612 - __main__ - INFO - CMTReplay AE training complete for task 'ds_rte_glue_rte' (500 buffer samples). AvgLoss: 0.0550, AvgUniformity: -0.2462, LR: 5.00e-05\n",
      "2025-05-20 17:07:38,794 - __main__ - INFO -   LoRA Epoch 3 Avg Metrics - Loss: 0.0891, Acc: 0.8360, GradN: 4.2963, Entropy (last batch): 4.1856\n",
      "2025-05-20 17:07:38,796 - __main__ - INFO - Task ds_rte_glue_rte NM - Metrics: {'accuracy': 0.836, 'nll': 0.089, 'gradient_norm': 4.296, 'entropy': 4.186}, Î³-Gain: 1.5000\n",
      "2025-05-20 17:07:38,797 - __main__ - INFO -   Task ds_rte_glue_rte - LoRA Training Epoch 4/12 for LoRA: MVE_CL_DataScale_2Ktrain_BS8_LR5e-5_ES_seed123_SharedAdapter_sst2_rte\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 4 LoRA:MVE_CL_DataScale_2Kt Task:ds_rte_glue_rte:   0%|          | 0/250 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-05-20 17:17:26,504 - __main__ - INFO -     ES Check: Epoch 4, evaluating on val set for ds_rte_glue_rte with LoRA MVE_CL_DataScale_2Ktrain_BS8_LR5e-5_ES_seed123_SharedAdapter_sst2_rte\n",
      "2025-05-20 17:18:10,048 - __main__ - INFO -     ES Check: Val Acc for ds_rte_glue_rte on LoRA MVE_CL_DataScale_2Ktrain_BS8_LR5e-5_ES_seed123_SharedAdapter_sst2_rte: 0.8150 (Best: 0.8150)\n",
      "2025-05-20 17:18:10,052 - __main__ - INFO -     ES Check: No improvement. Patience: 1/3\n",
      "2025-05-20 17:18:10,053 - __main__ - INFO - Epoch 4 ended. Training replay model for task 'ds_rte_glue_rte' using 500 samples.\n",
      "2025-05-20 17:18:10,054 - __main__ - INFO - CMTReplay: Training AE for task 'ds_rte_glue_rte' on 500 samples from buffer.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "CMT AE Ep1 Task:ds_rte_glue_rte:   0%|          | 0/125 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "CMT AE Ep2 Task:ds_rte_glue_rte:   0%|          | 0/125 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "CMT AE Ep3 Task:ds_rte_glue_rte:   0%|          | 0/125 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-05-20 17:18:15,236 - __main__ - INFO - CMTReplay AE training complete for task 'ds_rte_glue_rte' (500 buffer samples). AvgLoss: 0.0436, AvgUniformity: -0.2558, LR: 5.00e-05\n",
      "2025-05-20 17:18:15,423 - __main__ - INFO -   LoRA Epoch 4 Avg Metrics - Loss: 0.0698, Acc: 0.8825, GradN: 4.1488, Entropy (last batch): 4.0701\n",
      "2025-05-20 17:18:15,424 - __main__ - INFO - Task ds_rte_glue_rte NM - Metrics: {'accuracy': 0.882, 'nll': 0.07, 'gradient_norm': 4.149, 'entropy': 4.07}, Î³-Gain: 0.9000\n",
      "2025-05-20 17:18:15,425 - __main__ - INFO -   Task ds_rte_glue_rte - LoRA Training Epoch 5/12 for LoRA: MVE_CL_DataScale_2Ktrain_BS8_LR5e-5_ES_seed123_SharedAdapter_sst2_rte\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 5 LoRA:MVE_CL_DataScale_2Kt Task:ds_rte_glue_rte:   0%|          | 0/250 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-05-20 17:28:03,344 - __main__ - INFO -     ES Check: Epoch 5, evaluating on val set for ds_rte_glue_rte with LoRA MVE_CL_DataScale_2Ktrain_BS8_LR5e-5_ES_seed123_SharedAdapter_sst2_rte\n",
      "2025-05-20 17:28:46,514 - __main__ - INFO -     ES Check: Val Acc for ds_rte_glue_rte on LoRA MVE_CL_DataScale_2Ktrain_BS8_LR5e-5_ES_seed123_SharedAdapter_sst2_rte: 0.8200 (Best: 0.8150)\n",
      "2025-05-20 17:28:46,518 - __main__ - INFO -     ES Check: New best val_acc: 0.8200. Patience reset.\n",
      "2025-05-20 17:28:46,519 - __main__ - INFO - Epoch 5 ended. Training replay model for task 'ds_rte_glue_rte' using 500 samples.\n",
      "2025-05-20 17:28:46,519 - __main__ - INFO - CMTReplay: Training AE for task 'ds_rte_glue_rte' on 500 samples from buffer.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "CMT AE Ep1 Task:ds_rte_glue_rte:   0%|          | 0/125 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "CMT AE Ep2 Task:ds_rte_glue_rte:   0%|          | 0/125 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "CMT AE Ep3 Task:ds_rte_glue_rte:   0%|          | 0/125 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-05-20 17:28:51,665 - __main__ - INFO - CMTReplay AE training complete for task 'ds_rte_glue_rte' (500 buffer samples). AvgLoss: 0.0356, AvgUniformity: -0.2726, LR: 5.00e-05\n",
      "2025-05-20 17:28:51,854 - __main__ - INFO -   LoRA Epoch 5 Avg Metrics - Loss: 0.0549, Acc: 0.9220, GradN: 3.9727, Entropy (last batch): 4.2395\n",
      "2025-05-20 17:28:51,855 - __main__ - INFO - Task ds_rte_glue_rte NM - Metrics: {'accuracy': 0.922, 'nll': 0.055, 'gradient_norm': 3.973, 'entropy': 4.24}, Î³-Gain: 1.5000\n",
      "2025-05-20 17:28:51,856 - __main__ - INFO -   Task ds_rte_glue_rte - LoRA Training Epoch 6/12 for LoRA: MVE_CL_DataScale_2Ktrain_BS8_LR5e-5_ES_seed123_SharedAdapter_sst2_rte\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 6 LoRA:MVE_CL_DataScale_2Kt Task:ds_rte_glue_rte:   0%|          | 0/250 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-05-20 17:38:44,388 - __main__ - INFO -     ES Check: Epoch 6, evaluating on val set for ds_rte_glue_rte with LoRA MVE_CL_DataScale_2Ktrain_BS8_LR5e-5_ES_seed123_SharedAdapter_sst2_rte\n",
      "2025-05-20 17:39:28,174 - __main__ - INFO -     ES Check: Val Acc for ds_rte_glue_rte on LoRA MVE_CL_DataScale_2Ktrain_BS8_LR5e-5_ES_seed123_SharedAdapter_sst2_rte: 0.8300 (Best: 0.8200)\n",
      "2025-05-20 17:39:28,176 - __main__ - INFO -     ES Check: New best val_acc: 0.8300. Patience reset.\n",
      "2025-05-20 17:39:28,177 - __main__ - INFO - Epoch 6 ended. Training replay model for task 'ds_rte_glue_rte' using 500 samples.\n",
      "2025-05-20 17:39:28,177 - __main__ - INFO - CMTReplay: Training AE for task 'ds_rte_glue_rte' on 500 samples from buffer.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "CMT AE Ep1 Task:ds_rte_glue_rte:   0%|          | 0/125 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "CMT AE Ep2 Task:ds_rte_glue_rte:   0%|          | 0/125 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "CMT AE Ep3 Task:ds_rte_glue_rte:   0%|          | 0/125 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-05-20 17:39:33,191 - __main__ - INFO - CMTReplay AE training complete for task 'ds_rte_glue_rte' (500 buffer samples). AvgLoss: 0.0327, AvgUniformity: -0.2726, LR: 5.00e-05\n",
      "2025-05-20 17:39:33,376 - __main__ - INFO -   LoRA Epoch 6 Avg Metrics - Loss: 0.0365, Acc: 0.9595, GradN: 3.4885, Entropy (last batch): 4.0377\n",
      "2025-05-20 17:39:33,377 - __main__ - INFO - Task ds_rte_glue_rte NM - Metrics: {'accuracy': 0.96, 'nll': 0.037, 'gradient_norm': 3.489, 'entropy': 4.038}, Î³-Gain: 0.9000\n",
      "2025-05-20 17:39:33,378 - __main__ - INFO -   Task ds_rte_glue_rte - LoRA Training Epoch 7/12 for LoRA: MVE_CL_DataScale_2Ktrain_BS8_LR5e-5_ES_seed123_SharedAdapter_sst2_rte\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 7 LoRA:MVE_CL_DataScale_2Kt Task:ds_rte_glue_rte:   0%|          | 0/250 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-05-20 17:49:26,150 - __main__ - INFO -     ES Check: Epoch 7, evaluating on val set for ds_rte_glue_rte with LoRA MVE_CL_DataScale_2Ktrain_BS8_LR5e-5_ES_seed123_SharedAdapter_sst2_rte\n",
      "2025-05-20 17:50:07,809 - __main__ - INFO -     ES Check: Val Acc for ds_rte_glue_rte on LoRA MVE_CL_DataScale_2Ktrain_BS8_LR5e-5_ES_seed123_SharedAdapter_sst2_rte: 0.8050 (Best: 0.8300)\n",
      "2025-05-20 17:50:07,811 - __main__ - INFO -     ES Check: No improvement. Patience: 1/3\n",
      "2025-05-20 17:50:07,812 - __main__ - INFO - Epoch 7 ended. Training replay model for task 'ds_rte_glue_rte' using 500 samples.\n",
      "2025-05-20 17:50:07,812 - __main__ - INFO - CMTReplay: Training AE for task 'ds_rte_glue_rte' on 500 samples from buffer.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "CMT AE Ep1 Task:ds_rte_glue_rte:   0%|          | 0/125 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "CMT AE Ep2 Task:ds_rte_glue_rte:   0%|          | 0/125 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "CMT AE Ep3 Task:ds_rte_glue_rte:   0%|          | 0/125 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-05-20 17:50:12,890 - __main__ - INFO - CMTReplay AE training complete for task 'ds_rte_glue_rte' (500 buffer samples). AvgLoss: 0.0256, AvgUniformity: -0.2762, LR: 5.00e-05\n",
      "2025-05-20 17:50:13,073 - __main__ - INFO -   LoRA Epoch 7 Avg Metrics - Loss: 0.0266, Acc: 0.9785, GradN: 3.3659, Entropy (last batch): 4.1816\n",
      "2025-05-20 17:50:13,075 - __main__ - INFO - Task ds_rte_glue_rte NM - Metrics: {'accuracy': 0.978, 'nll': 0.027, 'gradient_norm': 3.366, 'entropy': 4.182}, Î³-Gain: 1.3277\n",
      "2025-05-20 17:50:13,076 - __main__ - INFO -   Task ds_rte_glue_rte - LoRA Training Epoch 8/12 for LoRA: MVE_CL_DataScale_2Ktrain_BS8_LR5e-5_ES_seed123_SharedAdapter_sst2_rte\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 8 LoRA:MVE_CL_DataScale_2Kt Task:ds_rte_glue_rte:   0%|          | 0/250 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-05-20 18:00:11,665 - __main__ - INFO -     ES Check: Epoch 8, evaluating on val set for ds_rte_glue_rte with LoRA MVE_CL_DataScale_2Ktrain_BS8_LR5e-5_ES_seed123_SharedAdapter_sst2_rte\n",
      "2025-05-20 18:00:55,423 - __main__ - INFO -     ES Check: Val Acc for ds_rte_glue_rte on LoRA MVE_CL_DataScale_2Ktrain_BS8_LR5e-5_ES_seed123_SharedAdapter_sst2_rte: 0.7850 (Best: 0.8300)\n",
      "2025-05-20 18:00:55,426 - __main__ - INFO -     ES Check: No improvement. Patience: 2/3\n",
      "2025-05-20 18:00:55,427 - __main__ - INFO - Epoch 8 ended. Training replay model for task 'ds_rte_glue_rte' using 500 samples.\n",
      "2025-05-20 18:00:55,428 - __main__ - INFO - CMTReplay: Training AE for task 'ds_rte_glue_rte' on 500 samples from buffer.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "CMT AE Ep1 Task:ds_rte_glue_rte:   0%|          | 0/125 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "CMT AE Ep2 Task:ds_rte_glue_rte:   0%|          | 0/125 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "CMT AE Ep3 Task:ds_rte_glue_rte:   0%|          | 0/125 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-05-20 18:01:00,351 - __main__ - INFO - CMTReplay AE training complete for task 'ds_rte_glue_rte' (500 buffer samples). AvgLoss: 0.0231, AvgUniformity: -0.2858, LR: 5.00e-05\n",
      "2025-05-20 18:01:00,540 - __main__ - INFO -   LoRA Epoch 8 Avg Metrics - Loss: 0.0109, Acc: 0.9960, GradN: 2.0784, Entropy (last batch): 3.8011\n",
      "2025-05-20 18:01:00,542 - __main__ - INFO - Task ds_rte_glue_rte NM - Metrics: {'accuracy': 0.996, 'nll': 0.011, 'gradient_norm': 2.078, 'entropy': 3.801}, Î³-Gain: 0.9000\n",
      "2025-05-20 18:01:00,543 - __main__ - INFO -   Task ds_rte_glue_rte - LoRA Training Epoch 9/12 for LoRA: MVE_CL_DataScale_2Ktrain_BS8_LR5e-5_ES_seed123_SharedAdapter_sst2_rte\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa2fed7a46354e3fb3264e60e06dcb8b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 9 LoRA:MVE_CL_DataScale_2Kt Task:ds_rte_glue_rte:   0%|          | 0/250 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Cell 17: MVE-DataScale-1.1: Testing with Larger Datasets (Adjusted Batch Size)\n",
    "\n",
    "import os\n",
    "import time\n",
    "import torch\n",
    "import gc\n",
    "import logging\n",
    "import numpy as np\n",
    "\n",
    "# --- Task Definitions ---\n",
    "\n",
    "# --- Task Definitions for MVE-DataScale-1.1 (5-task sequence, larger datasets) ---\n",
    "# Base descriptions and fields remain the same, only sample counts change.\n",
    "\n",
    "sst2_task_def_datascale = {\n",
    "    \"name\": \"glue\", \"config\": \"sst2\", \"text_field\": \"sentence\", \"label_field\": \"label\",\n",
    "    \"class_names_map\": {0: \"negative\", 1: \"positive\"}, \"id_prefix\": \"ds_sst2_\",\n",
    "    \"num_train_samples\": 2000, \"num_val_samples\": 200, # Increased\n",
    "    \"description_for_router\": \"Sentiment analysis of movie reviews (SST-2).\"\n",
    "}\n",
    "rte_task_def_datascale = {\n",
    "    \"name\": \"glue\", \"config\": \"rte\", \"text_field\": [\"sentence1\", \"sentence2\"], \"label_field\": \"label\",\n",
    "    \"class_names_map\": {0: \"entailment\", 1: \"not_entailment\"}, \"id_prefix\": \"ds_rte_\",\n",
    "    \"num_train_samples\": 2000, \"num_val_samples\": 200, # Increased (will use max available for RTE)\n",
    "    \"description_for_router\": \"Recognizing Textual Entailment (RTE).\"\n",
    "}\n",
    "mrpc_task_def_datascale = {\n",
    "    \"name\": \"glue\", \"config\": \"mrpc\", \"text_field\": [\"sentence1\", \"sentence2\"], \"label_field\": \"label\",\n",
    "    \"class_names_map\": {0: \"not_equivalent\", 1: \"equivalent\"}, \"id_prefix\": \"ds_mrpc_\",\n",
    "    \"num_train_samples\": 2000, \"num_val_samples\": 200, # Increased (will use max available for MRPC)\n",
    "    \"description_for_router\": \"Paraphrase identification (MRPC).\"\n",
    "}\n",
    "qnli_task_def_datascale = {\n",
    "    \"name\": \"glue\", \"config\": \"qnli\", \"text_field\": [\"question\", \"sentence\"], \"label_field\": \"label\",\n",
    "    \"class_names_map\": {0: \"entailment\", 1: \"not_entailment\"}, \"id_prefix\": \"ds_qnli_\",\n",
    "    \"num_train_samples\": 2000, \"num_val_samples\": 200, # Increased\n",
    "    \"description_for_router\": \"Question answering entailment (QNLI).\"\n",
    "}\n",
    "cola_task_def_datascale = {\n",
    "    \"name\": \"glue\", \"config\": \"cola\", \"text_field\": \"sentence\", \"label_field\": \"label\",\n",
    "    \"class_names_map\": {0: \"unacceptable\", 1: \"acceptable\"}, \"id_prefix\": \"ds_cola_\",\n",
    "    \"num_train_samples\": 2000, \"num_val_samples\": 200, # Increased\n",
    "    \"description_for_router\": \"Corpus of Linguistic Acceptability (CoLA) - grammaticality.\"\n",
    "}\n",
    "\n",
    "benchmark_tasks_for_datascale_exp = [ # 5-task list with new sample sizes\n",
    "    sst2_task_def_datascale,\n",
    "    rte_task_def_datascale,\n",
    "    mrpc_task_def_datascale,\n",
    "    qnli_task_def_datascale,\n",
    "    cola_task_def_datascale\n",
    "]\n",
    "# --- End of Task Definitions ---\n",
    "\n",
    "\n",
    "# --- Base Configuration Function (can be reused or adapted if needed) ---\n",
    "def get_base_config_for_mve(experiment_tag_suffix: str, task_list: List[Dict[str, Any]]) -> 'AdaptiveLearnerConfig':\n",
    "    config = AdaptiveLearnerConfig() # Gets defaults, including early stopping params\n",
    "    config.benchmark_tasks_to_run = task_list\n",
    "    clean_suffix = experiment_tag_suffix.replace(\".\", \"_\")\n",
    "    config.experiment_tag = f\"MVE_CL_{clean_suffix}\" # Generic prefix, specific MVEs will refine\n",
    "    config.run_sft_baseline = False; config.router_type = \"linear\"\n",
    "    config.router_confidence_threshold = 1.1 ; config.use_advanced_embeddings = False\n",
    "    config.k_examples_for_prototype = 0 ; config.load_router_state_from_tag = None\n",
    "    config.router_training_epochs_per_call = 0\n",
    "    config.num_tasks_to_share_lora = 2 # Default from SL-F series\n",
    "    config.num_workers = 2; config.use_wandb = True\n",
    "    # Default CL strategy from SL-F_Rep1 / SL-F_Rep1_ES\n",
    "    config.consolidation_method = \"ewc\"; config.ewc_lambda = 250.0\n",
    "    config.ewc_fixed_lambda_bypass_gamma = False\n",
    "    config.ewc_data_loader_num_samples = 50; config.gamma_gain_lambda = 3.0\n",
    "    config.replay_method = \"cmt\"; config.replay_alpha = 0.2; config.feature_replay_alpha = 0.1\n",
    "    config.batch_size = 4; config.num_lora_train_epochs = 12 # Max epochs\n",
    "    config.ewc_batch_size = 10\n",
    "    config.replay_model_internal_batch_size = 4; config.replay_backbone_encoding_batch_size = 16\n",
    "    config.learning_rate = 5e-5 # Default LR\n",
    "    return config\n",
    "\n",
    "# --- Config Function for MVE-DataScale-1.1 (Adjusted) ---\n",
    "def get_config_variant_datascale_1_1_bs8() -> 'AdaptiveLearnerConfig':\n",
    "    # Start with a base that includes early stopping and SL-F_Rep1 CL settings\n",
    "    config = get_base_config_for_mve(\"DataScale_2Ktrain_BS8_LR5e-5_ES_seed123\", benchmark_tasks_for_datascale_exp)\n",
    "    \n",
    "    config.wandb_project = \"adaptive-learner-cl-datascale\" # New W&B project\n",
    "\n",
    "    # Data scaling specific changes\n",
    "    # Task list is already set by benchmark_tasks_for_datascale_exp\n",
    "    \n",
    "    # Training hyperparameters\n",
    "    config.batch_size = 8  # Adjusted batch size\n",
    "    config.learning_rate = 5e-5 # Kept original LR for BS=8\n",
    "\n",
    "    config.use_lora_early_stopping = True # Ensure Early Stopping is enabled\n",
    "    \n",
    "    # Inherit other CL parameters from get_base_config_for_mve which are based on SL-F_Rep1\n",
    "    config.num_tasks_to_share_lora = 2\n",
    "    config.consolidation_method = \"ewc\"\n",
    "    config.ewc_lambda = 250.0\n",
    "    config.ewc_fixed_lambda_bypass_gamma = False\n",
    "    config.ewc_data_loader_num_samples = 50\n",
    "    config.gamma_gain_lambda = 3.0\n",
    "    config.replay_method = \"cmt\"\n",
    "    config.replay_alpha = 0.2\n",
    "    config.feature_replay_alpha = 0.1\n",
    "    config.ewc_batch_size = 10 # This is for EWC Fisher calculation, can be different from LoRA train BS\n",
    "    config.replay_model_internal_batch_size = 4 # Keep from SL-F_Rep1\n",
    "    config.replay_backbone_encoding_batch_size = 16 # Keep from SL-F_Rep1\n",
    "    config.num_lora_train_epochs = 12 # This is max epochs, ES will control actual\n",
    "\n",
    "    return config\n",
    "\n",
    "\n",
    "# --- run_experiment_variant function (ensure it's the latest version from previous interactions) ---\n",
    "def run_experiment_variant(config_variant: 'AdaptiveLearnerConfig', variant_name_for_log: str):\n",
    "    logger.info(f\"--- CONFIGURING FOR: {variant_name_for_log} ---\")\n",
    "    logger.info(f\"Experiment Tag: {config_variant.experiment_tag}\")\n",
    "    logger.info(f\"Consolidation: {config_variant.consolidation_method}\")\n",
    "    if config_variant.consolidation_method == \"ewc\":\n",
    "        logger.info(f\"EWC Lambda: {config_variant.ewc_lambda}, Bypass Gamma for Accum: {config_variant.ewc_fixed_lambda_bypass_gamma}\")\n",
    "        logger.info(f\"EWC Fisher Samples: {config_variant.ewc_data_loader_num_samples}\")\n",
    "        logger.info(f\"EWC Batch Size: {config_variant.ewc_batch_size}\")\n",
    "    if hasattr(config_variant, 'gamma_gain_lambda') and not config_variant.ewc_fixed_lambda_bypass_gamma and config_variant.consolidation_method == \"ewc\":\n",
    "        logger.info(f\"Gamma Gain Lambda: {config_variant.gamma_gain_lambda}\")\n",
    "    logger.info(f\"Replay Method: {config_variant.replay_method}, Alpha: {config_variant.replay_alpha}, Feature Alpha: {config_variant.feature_replay_alpha}\")\n",
    "    if config_variant.replay_method != \"none\":\n",
    "        logger.info(f\"Replay Internal BS: {config_variant.replay_model_internal_batch_size}, Replay Backbone Encoding BS: {config_variant.replay_backbone_encoding_batch_size}\")\n",
    "    logger.info(f\"Num tasks sharing LoRA: {config_variant.num_tasks_to_share_lora}\")\n",
    "    logger.info(f\"Total tasks in sequence: {len(config_variant.benchmark_tasks_to_run)}\")\n",
    "    logger.info(f\"LoRA Train Batch Size: {config_variant.batch_size}, Max LoRA Train Epochs: {config_variant.num_lora_train_epochs}\")\n",
    "    logger.info(f\"Learning Rate: {config_variant.learning_rate}\")\n",
    "    logger.info(f\"Use Advanced Embeddings: {config_variant.use_advanced_embeddings}, k_for_prototype: {config_variant.k_examples_for_prototype}\")\n",
    "    logger.info(f\"Use LoRA Early Stopping: {config_variant.use_lora_early_stopping}\")\n",
    "    if config_variant.use_lora_early_stopping:\n",
    "        logger.info(f\"  ES Patience: {config_variant.lora_early_stopping_patience}, ES Metric: {config_variant.lora_early_stopping_metric}\")\n",
    "        logger.info(f\"  ES Min Epochs: {config_variant.min_lora_epochs_before_early_stop}, ES Delta: {config_variant.lora_early_stopping_delta}\")\n",
    "\n",
    "    logger.info(f\"\\\\n{'='*20} Starting Experiment Variant: {variant_name_for_log} ({config_variant.experiment_tag}) {'='*20}\")\n",
    "    wandb_module_available = False\n",
    "    if config_variant.use_wandb:\n",
    "        try:\n",
    "            import wandb\n",
    "            wandb_module_available = True\n",
    "        except ImportError:\n",
    "            logger.warning(\"wandb could not be imported. W&B features will be disabled for this variant.\")\n",
    "            config_variant.use_wandb = False\n",
    "\n",
    "    if config_variant.use_wandb and wandb_module_available:\n",
    "        if wandb.run is not None:\n",
    "            logger.info(f\"Finishing previous W&B run: {wandb.run.id if wandb.run else 'Unknown'} before starting {variant_name_for_log}.\")\n",
    "            wandb.finish()\n",
    "        effective_wandb_api_key = config_variant.wandb_api_key\n",
    "        if effective_wandb_api_key and config_variant.wandb_project:\n",
    "            wandb_run_name = f\"{time.strftime('%Y%m%d-%H%M%S')}_{config_variant.experiment_tag}\"\n",
    "            try:\n",
    "                if config_variant.wandb_api_key and not os.environ.get(\"WANDB_API_KEY\"):\n",
    "                    logger.info(f\"W&B: Attempting login with API key from config for {variant_name_for_log}.\")\n",
    "                    wandb.login(key=config_variant.wandb_api_key)\n",
    "                elif os.environ.get(\"WANDB_API_KEY\"):\n",
    "                    logger.info(f\"W&B: API key found in environment for {variant_name_for_log}.\")\n",
    "\n",
    "                wandb.init(project=config_variant.wandb_project,\n",
    "                           entity=config_variant.wandb_entity,\n",
    "                           config=config_variant.to_dict(),\n",
    "                           name=wandb_run_name,\n",
    "                           reinit=True)\n",
    "                logger.info(f\"W&B run initiated for {variant_name_for_log} to project '{config_variant.wandb_project}': {wandb.run.name if wandb.run else 'Failed'}\")\n",
    "            except Exception as e:\n",
    "                logger.error(f\"W&B initialization failed for {variant_name_for_log}: {e}\")\n",
    "        else:\n",
    "            logger.warning(f\"W&B API key or project not found/set in config for {variant_name_for_log}. W&B disabled for this variant.\")\n",
    "            config_variant.use_wandb = False\n",
    "    elif config_variant.use_wandb and not wandb_module_available:\n",
    "        pass\n",
    "    else:\n",
    "        logger.info(f\"W&B usage is disabled in config for {variant_name_for_log}.\")\n",
    "\n",
    "    global default_config\n",
    "    original_default_config_backup = default_config\n",
    "    default_config = config_variant\n",
    "\n",
    "    adaptive_learner_config_type_backup = None\n",
    "    if 'AdaptiveLearnerConfig' not in globals() and 'default_config' in globals():\n",
    "        adaptive_learner_config_type_backup = globals().get('AdaptiveLearnerConfig')\n",
    "        globals()['AdaptiveLearnerConfig'] = type(default_config)\n",
    "\n",
    "    try:\n",
    "        main(run_config=config_variant)\n",
    "    finally:\n",
    "        default_config = original_default_config_backup\n",
    "        if adaptive_learner_config_type_backup is not None:\n",
    "            globals()['AdaptiveLearnerConfig'] = adaptive_learner_config_type_backup\n",
    "        elif 'AdaptiveLearnerConfig' in globals() and type(default_config) != globals()['AdaptiveLearnerConfig'] :\n",
    "             pass\n",
    "\n",
    "    if config_variant.use_wandb and wandb_module_available and wandb.run is not None:\n",
    "        logger.info(f\"Finishing W&B run for {variant_name_for_log}: {wandb.run.id}\")\n",
    "        wandb.finish()\n",
    "\n",
    "    vars_to_delete = ['backbone', 'peft_manager', 'neuromod_manager', 'consolidation_manager', 'replay_manager']\n",
    "    for var_name in vars_to_delete:\n",
    "        if var_name in globals():\n",
    "            del globals()[var_name]\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    logger.info(f\"--- Variant {variant_name_for_log} ({config_variant.experiment_tag}) finished. Memory cleaned. ---\\\\n\")\n",
    "\n",
    "\n",
    "# --- Execution Plan ---\n",
    "\n",
    "# # Previous run (SL-F_Rep1_ES) - Commented out after completion\n",
    "# logger.info(\"\\\\n\" + \"=\"*50 + \"\\\\nStarting MVE-Efficiency-1.1: Early Stopping Validation\" + \"\\\\n\" + \"=\"*50)\n",
    "# config_sl_f_rep1_es = get_config_variant_sl_f_rep1_es() # Assuming this function was defined from previous step\n",
    "# run_experiment_variant(config_sl_f_rep1_es, \"Variant SL-F_Rep1_ES: Hybrid Scaled (50F, seed123) with Early Stopping\")\n",
    "# logger.info(\"\\\\n\" + \"=\"*50 + \"\\\\nFinished MVE-Efficiency-1.1: Early Stopping Validation\" + \"\\\\n\" + \"=\"*50)\n",
    "\n",
    "\n",
    "logger.info(\"\\\\n\" + \"=\"*50 + \"\\\\nStarting MVE-DataScale-1.1: Larger Datasets Validation (BS=8)\" + \"\\\\n\" + \"=\"*50)\n",
    "config_datascale_1_1_bs8 = get_config_variant_datascale_1_1_bs8()\n",
    "run_experiment_variant(config_datascale_1_1_bs8, \"Variant DataScale-1.1: Hybrid Scaled (2K Train, BS8, LR5e-5, ES, seed123)\")\n",
    "logger.info(\"\\\\n\" + \"=\"*50 + \"\\\\nFinished MVE-DataScale-1.1: Larger Datasets Validation (BS=8)\" + \"\\\\n\" + \"=\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sgeUSf50FClM",
    "outputId": "0b4cecb7-089b-4130-a452-4d8cc857f3e8",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import datasets\n",
    "print(datasets.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Temporary cell to inspect model layers for grad_sketch_layer_names\n",
    "if 'backbone' in globals() and hasattr(backbone, 'model'):\n",
    "    print(\"Listing all named modules in backbone.model:\")\n",
    "    for name, module in backbone.model.named_modules():\n",
    "        # We are interested in modules that likely have a 'weight' parameter,\n",
    "        # like Linear layers (often part of attention or MLPs)\n",
    "        if isinstance(module, torch.nn.Linear): # You can also check for other types if needed\n",
    "            print(f\"  Module Name: {name} (Type: {type(module)})\")\n",
    "    print(\"\\nListing all named parameters in backbone.model (for reference):\")\n",
    "    # for name, param in backbone.model.named_parameters():\n",
    "    #     print(f\"  Parameter Name: {name} (Shape: {param.shape})\")\n",
    "else:\n",
    "    print(\"Backbone model not found. Please initialize AdaptiveLearnerBackbone first.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Cell X: Cleanup Script for Experiment Outputs\n",
    "\n",
    "import os\n",
    "import shutil\n",
    "import logging \n",
    "\n",
    "# Configure a simple logger for this cell\n",
    "cleanup_logger = logging.getLogger(__name__ + \"_cleanup_script\")\n",
    "if not cleanup_logger.handlers:\n",
    "    handler = logging.StreamHandler()\n",
    "    formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "    handler.setFormatter(formatter)\n",
    "    cleanup_logger.addHandler(handler)\n",
    "    cleanup_logger.setLevel(logging.INFO)\n",
    "\n",
    "# --- !!! SET THIS TO THE TAG OF THE EXPERIMENT YOU WANT TO CLEAN UP !!! ---\n",
    "# This should be the tag of the run you just completed (SL-E Hybrid with original replay alphas).\n",
    "experiment_tag_to_clean = \"MVE_CL_SharedLoRA_SL_F_Rep1_ES_Hybrid_50F_seed123\"  # <<< FOR CLEANING UP AFTER SL-E\n",
    "# --- !!! ---\n",
    "\n",
    "# Define the base output path\n",
    "if 'RUNPODS_PROJECT_BASE_PATH' not in globals():\n",
    "    RUNPODS_PROJECT_BASE_PATH = \"/workspace/MyAdaptiveLearnerProject\" \n",
    "    cleanup_logger.warning(f\"RUNPODS_PROJECT_BASE_PATH not found in globals, using fallback: {RUNPODS_PROJECT_BASE_PATH}\")\n",
    "\n",
    "base_output_dir = os.path.join(RUNPODS_PROJECT_BASE_PATH, \"outputs\")\n",
    "lora_modules_dir = os.path.join(base_output_dir, \"lora_modules\")\n",
    "router_states_dir = os.path.join(base_output_dir, \"router_states\")\n",
    "gamma_metrics_base_dir = os.path.join(base_output_dir, \"gamma_metrics\") \n",
    "replay_base_dir = os.path.join(base_output_dir, \"replay\")\n",
    "\n",
    "\n",
    "cleanup_logger.info(f\"--- Starting Cleanup for Experiment Tag Prefix: {experiment_tag_to_clean} ---\")\n",
    "cleanup_logger.info(f\"Base output directory: {base_output_dir}\")\n",
    "\n",
    "# 1. Clean LoRA Module Directories\n",
    "if os.path.exists(lora_modules_dir):\n",
    "    cleaned_lora_count = 0\n",
    "    for item_name in os.listdir(lora_modules_dir):\n",
    "        if item_name.startswith(experiment_tag_to_clean): \n",
    "            item_path = os.path.join(lora_modules_dir, item_name)\n",
    "            if os.path.isdir(item_path):\n",
    "                try:\n",
    "                    shutil.rmtree(item_path)\n",
    "                    cleanup_logger.info(f\"  Deleted LoRA module directory: {item_path}\")\n",
    "                    cleaned_lora_count += 1\n",
    "                except Exception as e:\n",
    "                    cleanup_logger.error(f\"  Error deleting LoRA directory {item_path}: {e}\")\n",
    "    if cleaned_lora_count == 0:\n",
    "        cleanup_logger.info(f\"  No LoRA module directories found starting with '{experiment_tag_to_clean}' in {lora_modules_dir}\")\n",
    "else:\n",
    "    cleanup_logger.info(f\"LoRA modules directory not found: {lora_modules_dir}\")\n",
    "\n",
    "# 2. Clean Router State File\n",
    "router_state_filename = f\"router_state_{experiment_tag_to_clean}.pth\"\n",
    "router_state_filepath = os.path.join(router_states_dir, router_state_filename)\n",
    "if os.path.exists(router_state_filepath):\n",
    "    try:\n",
    "        os.remove(router_state_filepath)\n",
    "        cleanup_logger.info(f\"  Deleted router state file: {router_state_filepath}\")\n",
    "    except Exception as e:\n",
    "        cleanup_logger.error(f\"  Error deleting router state file {router_state_filepath}: {e}\")\n",
    "else:\n",
    "    cleanup_logger.info(f\"  Router state file not found: {router_state_filepath}\")\n",
    "\n",
    "# 3. Clean Experiment Tag Directory (contains plots, etc.)\n",
    "experiment_specific_output_dir = os.path.join(base_output_dir, experiment_tag_to_clean)\n",
    "if os.path.exists(experiment_specific_output_dir) and os.path.isdir(experiment_specific_output_dir):\n",
    "    try:\n",
    "        shutil.rmtree(experiment_specific_output_dir)\n",
    "        cleanup_logger.info(f\"  Deleted experiment-specific output directory: {experiment_specific_output_dir}\")\n",
    "    except Exception as e:\n",
    "        cleanup_logger.error(f\"  Error deleting experiment-specific output directory {experiment_specific_output_dir}: {e}\")\n",
    "else:\n",
    "    cleanup_logger.info(f\"  Experiment-specific output directory not found: {experiment_specific_output_dir}\")\n",
    "\n",
    "# 4. Clean Replay Model Directory (if any from this experiment tag)\n",
    "replay_experiment_dir = os.path.join(replay_base_dir, experiment_tag_to_clean) \n",
    "if os.path.exists(replay_experiment_dir) and os.path.isdir(replay_experiment_dir):\n",
    "    try:\n",
    "        shutil.rmtree(replay_experiment_dir)\n",
    "        cleanup_logger.info(f\"  Deleted replay experiment directory: {replay_experiment_dir}\")\n",
    "    except Exception as e:\n",
    "        cleanup_logger.error(f\"  Error deleting replay experiment directory {replay_experiment_dir}: {e}\")\n",
    "else:\n",
    "    cleanup_logger.info(f\"  Replay experiment directory not found: {replay_experiment_dir}\")\n",
    "\n",
    "cleanup_logger.info(f\"--- Cleanup for Experiment Tag: {experiment_tag_to_clean} Complete ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "0403f38cf63c4c09b0a0d9a2f70d1682": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "24e4784f197244118e4dec632be7f3d4": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "272b610d24cb4670b0b0ab45ad91e061": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "34c58fdb3adf4f6aa458b9e8dd471e8b": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "4576445817a448d5aeb73ed67b1af951": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_fc65d7dc2c1f47d5aa30f8608c69b02f",
       "IPY_MODEL_a108d2251bea436bbd1e1cef82e9750a",
       "IPY_MODEL_b797ae8a14d646bbb72a179f5d340dac"
      ],
      "layout": "IPY_MODEL_e708594a5c514595a1a5d7958d5041fa"
     }
    },
    "54700b7034064c00a182b28c69c52421": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "718cc1df548a4f86963f16c1e8690405": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "75b994b2cd4341589813d464c07b4329": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "82b7696e537c4f0bb32141a6b40426f4": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_34c58fdb3adf4f6aa458b9e8dd471e8b",
      "placeholder": "â",
      "style": "IPY_MODEL_e606938e85014167ae5a8613c9c773be",
      "value": "â2/2â[00:05&lt;00:00,ââ4.93s/it]"
     }
    },
    "92cac6efd7504a7b9e592583c064321a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_e3d65792a74e4d519ad9a169cb82639d",
      "placeholder": "â",
      "style": "IPY_MODEL_c98ee9abea7b4dafbd052d031579c909",
      "value": "Loadingâcheckpointâshards:â100%"
     }
    },
    "98be175717284e799f66caf8ad3f9a5f": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a108d2251bea436bbd1e1cef82e9750a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "danger",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_df6a41da7512451a83c2b8f8b3f2095a",
      "max": 250,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_0403f38cf63c4c09b0a0d9a2f70d1682",
      "value": 20
     }
    },
    "b797ae8a14d646bbb72a179f5d340dac": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_54700b7034064c00a182b28c69c52421",
      "placeholder": "â",
      "style": "IPY_MODEL_bbc9f520db3243358f02f998be259eef",
      "value": "â20/250â[00:21&lt;04:01,ââ1.05s/it]"
     }
    },
    "bbc9f520db3243358f02f998be259eef": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "c0e33f7f39624f32a5ad1d0e9c017d51": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_92cac6efd7504a7b9e592583c064321a",
       "IPY_MODEL_e88b9c533b4c423d9a92b187944396d7",
       "IPY_MODEL_82b7696e537c4f0bb32141a6b40426f4"
      ],
      "layout": "IPY_MODEL_272b610d24cb4670b0b0ab45ad91e061"
     }
    },
    "c98ee9abea7b4dafbd052d031579c909": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "df6a41da7512451a83c2b8f8b3f2095a": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e3d65792a74e4d519ad9a169cb82639d": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e606938e85014167ae5a8613c9c773be": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "e708594a5c514595a1a5d7958d5041fa": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e88b9c533b4c423d9a92b187944396d7": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_98be175717284e799f66caf8ad3f9a5f",
      "max": 2,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_24e4784f197244118e4dec632be7f3d4",
      "value": 2
     }
    },
    "fc65d7dc2c1f47d5aa30f8608c69b02f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_718cc1df548a4f86963f16c1e8690405",
      "placeholder": "â",
      "style": "IPY_MODEL_75b994b2cd4341589813d464c07b4329",
      "value": "Epochâ1âBatches:âââ8%"
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
